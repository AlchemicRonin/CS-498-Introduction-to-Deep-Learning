{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "MP5.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlchemicRonin/CS-498-Introduction-to-Deep-Learning/blob/TY/MP5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X29q4I9gPIDZ"
      },
      "source": [
        "# Deep Q-Learning "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vM9EZZ7uPIDZ"
      },
      "source": [
        "Install dependencies for AI gym to run properly (shouldn't take more than a minute). If running on google cloud or running locally, only need to run once. Colab may require installing everytime the vm shuts down."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VII1gnQZPIDZ",
        "outputId": "a24c8ec8-8a37-4868-8385-bed3d0203be3"
      },
      "source": [
        "!pip3 install gym pyvirtualdisplay\n",
        "!sudo apt-get install -y xvfb python-opengl ffmpeg"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (0.17.3)\n",
            "Collecting pyvirtualdisplay\n",
            "  Downloading https://files.pythonhosted.org/packages/d0/8a/643043cc70791367bee2d19eb20e00ed1a246ac48e5dbe57bbbcc8be40a9/PyVirtualDisplay-1.3.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym) (1.18.5)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.3.0)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.5.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym) (1.4.1)\n",
            "Collecting EasyProcess\n",
            "  Downloading https://files.pythonhosted.org/packages/48/3c/75573613641c90c6d094059ac28adb748560d99bd27ee6f80cce398f404e/EasyProcess-0.3-py2.py3-none-any.whl\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.16.0)\n",
            "Installing collected packages: EasyProcess, pyvirtualdisplay\n",
            "Successfully installed EasyProcess-0.3 pyvirtualdisplay-1.3.2\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:3.4.8-0ubuntu0.2).\n",
            "Suggested packages:\n",
            "  libgle3\n",
            "The following NEW packages will be installed:\n",
            "  python-opengl xvfb\n",
            "0 upgraded, 2 newly installed, 0 to remove and 14 not upgraded.\n",
            "Need to get 1,280 kB of archives.\n",
            "After this operation, 7,686 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 python-opengl all 3.1.0+dfsg-1 [496 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 xvfb amd64 2:1.19.6-1ubuntu4.8 [784 kB]\n",
            "Fetched 1,280 kB in 1s (951 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 2.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package python-opengl.\n",
            "(Reading database ... 144865 files and directories currently installed.)\n",
            "Preparing to unpack .../python-opengl_3.1.0+dfsg-1_all.deb ...\n",
            "Unpacking python-opengl (3.1.0+dfsg-1) ...\n",
            "Selecting previously unselected package xvfb.\n",
            "Preparing to unpack .../xvfb_2%3a1.19.6-1ubuntu4.8_amd64.deb ...\n",
            "Unpacking xvfb (2:1.19.6-1ubuntu4.8) ...\n",
            "Setting up python-opengl (3.1.0+dfsg-1) ...\n",
            "Setting up xvfb (2:1.19.6-1ubuntu4.8) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 599
        },
        "id": "K9jG93-FPIDZ",
        "outputId": "efd8abb0-d40c-4fe3-b7f3-63190aa04348"
      },
      "source": [
        "!pip3 install --upgrade setuptools\n",
        "!pip3 install ez_setup \n",
        "!pip3 install gym[atari] "
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting setuptools\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3d/f2/1489d3b6c72d68bf79cd0fba6b6c7497df4ebf7d40970e2d7eceb8d0ea9c/setuptools-51.0.0-py3-none-any.whl (785kB)\n",
            "\r\u001b[K     |▍                               | 10kB 23.4MB/s eta 0:00:01\r\u001b[K     |▉                               | 20kB 21.6MB/s eta 0:00:01\r\u001b[K     |█▎                              | 30kB 15.7MB/s eta 0:00:01\r\u001b[K     |█▊                              | 40kB 13.8MB/s eta 0:00:01\r\u001b[K     |██                              | 51kB 9.4MB/s eta 0:00:01\r\u001b[K     |██▌                             | 61kB 9.0MB/s eta 0:00:01\r\u001b[K     |███                             | 71kB 10.2MB/s eta 0:00:01\r\u001b[K     |███▍                            | 81kB 11.1MB/s eta 0:00:01\r\u001b[K     |███▊                            | 92kB 9.8MB/s eta 0:00:01\r\u001b[K     |████▏                           | 102kB 9.1MB/s eta 0:00:01\r\u001b[K     |████▋                           | 112kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████                           | 122kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 133kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 143kB 9.1MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 153kB 9.1MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 163kB 9.1MB/s eta 0:00:01\r\u001b[K     |███████                         | 174kB 9.1MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 184kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████                        | 194kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 204kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 215kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 225kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 235kB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████                      | 245kB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 256kB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 266kB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 276kB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 286kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████                    | 296kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 307kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 317kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 327kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 337kB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 348kB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 358kB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 368kB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 378kB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 389kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 399kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 409kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 419kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 430kB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 440kB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 450kB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 460kB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 471kB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 481kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 491kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 501kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 512kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 522kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 532kB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 542kB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 552kB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 563kB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 573kB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 583kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 593kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 604kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 614kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 624kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 634kB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 645kB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 655kB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 665kB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 675kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 686kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 696kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 706kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 716kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 727kB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 737kB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 747kB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 757kB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 768kB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 778kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 788kB 9.1MB/s \n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: setuptools\n",
            "  Found existing installation: setuptools 50.3.2\n",
            "    Uninstalling setuptools-50.3.2:\n",
            "      Successfully uninstalled setuptools-50.3.2\n",
            "Successfully installed setuptools-51.0.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pkg_resources"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Collecting ez_setup\n",
            "  Downloading https://files.pythonhosted.org/packages/ba/2c/743df41bd6b3298706dfe91b0c7ecdc47f2dc1a3104abeb6e9aa4a45fa5d/ez_setup-0.9.tar.gz\n",
            "Building wheels for collected packages: ez-setup\n",
            "  Building wheel for ez-setup (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ez-setup: filename=ez_setup-0.9-cp36-none-any.whl size=11016 sha256=51d0df91a3a97e25e4752a8afad72107a27954ed8cda0b65cad67bd4a91a16ae\n",
            "  Stored in directory: /root/.cache/pip/wheels/dc/e8/6b/3d5ff5a3efd7b5338d1e173ac981771e2628ceb2f7866d49ad\n",
            "Successfully built ez-setup\n",
            "Installing collected packages: ez-setup\n",
            "Successfully installed ez-setup-0.9\n",
            "Requirement already satisfied: gym[atari] in /usr/local/lib/python3.6/dist-packages (0.17.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.18.5)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.3.0)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.5.0)\n",
            "Requirement already satisfied: atari-py~=0.2.0; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (0.2.6)\n",
            "Requirement already satisfied: Pillow; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (7.0.0)\n",
            "Requirement already satisfied: opencv-python; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (4.1.2.30)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[atari]) (0.16.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from atari-py~=0.2.0; extra == \"atari\"->gym[atari]) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iXWqv5E9PTAL",
        "outputId": "d469cc13-f8c8-4bdc-f9ff-eb6c19d52b6d"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/',force_remount= True)\n",
        "import os\n",
        "os.chdir(\"/content/drive/My Drive/assignment5_materials\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qsqE-H8-PIDZ"
      },
      "source": [
        "For this assignment we will implement the Deep Q-Learning algorithm with Experience Replay as described in breakthrough paper __\"Playing Atari with Deep Reinforcement Learning\"__. We will train an agent to play the famous game of __Breakout__."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4oe-B_xQPIDZ"
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import sys\n",
        "import gym\n",
        "import torch\n",
        "import pylab\n",
        "import random\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "from datetime import datetime\n",
        "from copy import deepcopy\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from utils import find_max_lives, check_live, get_frame, get_init_state\n",
        "from model import DQN\n",
        "from config import *\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "# %load_ext autoreload\n",
        "# %autoreload 2"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jSuGpVIYPIDZ"
      },
      "source": [
        "## Understanding the environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9FWzFqb2PIDZ"
      },
      "source": [
        "In the following cell, we initialize our game of __Breakout__ and you can see how the environment looks like. For further documentation of the of the environment refer to https://gym.openai.com/envs. \n",
        "\n",
        "In breakout, we will use 3 actions \"fire\", \"left\", and \"right\". \"fire\" is only used to reset the game when a life is lost, \"left\" moves the agent left and \"right\" moves the agent right."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DVq1QZF7PIDZ"
      },
      "source": [
        "env = gym.make('BreakoutDeterministic-v4')\n",
        "state = env.reset()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xt_zcMQaPIDZ"
      },
      "source": [
        "number_lives = find_max_lives(env)\n",
        "state_size = env.observation_space.shape\n",
        "action_size = 3 #fire, left, and right"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xtAoIrToPIDZ"
      },
      "source": [
        "## Creating a DQN Agent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OntE5E03PIDZ"
      },
      "source": [
        "Here we create a DQN Agent. This agent is defined in the __agent.py__. The corresponding neural network is defined in the __model.py__. Once you've created a working DQN agent, use the code in agent.py to create a double DQN agent in __agent_double.py__. Set the flag \"double_dqn\" to True to train the double DQN agent.\n",
        "\n",
        "__Evaluation Reward__ : The average reward received in the past 100 episodes/games.\n",
        "\n",
        "__Frame__ : Number of frames processed in total.\n",
        "\n",
        "__Memory Size__ : The current size of the replay memory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_dx8EXyPIDZ"
      },
      "source": [
        "double_dqn = True # set to True if using double DQN agent\n",
        "\n",
        "if double_dqn:\n",
        "    from agent_double import Agent\n",
        "else:\n",
        "    from agent import Agent\n",
        "\n",
        "agent = Agent(action_size)\n",
        "evaluation_reward = deque(maxlen=evaluation_reward_length)\n",
        "frame = 0\n",
        "memory_size = 0"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Olr1uMLpPIDZ"
      },
      "source": [
        "### Main Training Loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTB7bL1yPIDZ"
      },
      "source": [
        "In this training loop, we do not render the screen because it slows down training signficantly. To watch the agent play the game, run the code in next section \"Visualize Agent Performance\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "E77LqI9CPIDZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7574a23a-4f0e-41a6-8173-a388e243595a"
      },
      "source": [
        "rewards, episodes = [], []\n",
        "best_eval_reward = 0\n",
        "for e in range(EPISODES):\n",
        "    done = False\n",
        "    score = 0\n",
        "\n",
        "    history = np.zeros([5, 84, 84], dtype=np.uint8)\n",
        "    step = 0\n",
        "    d = False\n",
        "    state = env.reset()\n",
        "    next_state = state\n",
        "    life = number_lives\n",
        "\n",
        "    get_init_state(history, state)\n",
        "\n",
        "    while not done:\n",
        "        step += 1\n",
        "        frame += 1\n",
        "\n",
        "        # Perform a fire action if ball is no longer on screen to continue onto next life\n",
        "        if step > 1 and len(np.unique(next_state[:189] == state[:189])) < 2:\n",
        "            action = 0\n",
        "        else:\n",
        "            action = agent.get_action(np.float32(history[:4, :, :]) / 255.)\n",
        "        state = next_state\n",
        "        next_state, reward, done, info = env.step(action + 1)\n",
        "        \n",
        "        frame_next_state = get_frame(next_state)\n",
        "        history[4, :, :] = frame_next_state\n",
        "        terminal_state = check_live(life, info['ale.lives'])\n",
        "\n",
        "        life = info['ale.lives']\n",
        "        r = np.clip(reward, -1, 1) \n",
        "        r = reward\n",
        "\n",
        "        # Store the transition in memory \n",
        "        agent.memory.push(deepcopy(frame_next_state), action, r, terminal_state)\n",
        "        # Start training after random sample generation\n",
        "        if(frame >= train_frame):\n",
        "            agent.train_policy_net(frame)\n",
        "            # Update the target network only for Double DQN only\n",
        "            if double_dqn and (frame % update_target_network_frequency)== 0:\n",
        "                agent.update_target_net()\n",
        "        score += reward\n",
        "        history[:4, :, :] = history[1:, :, :]\n",
        "            \n",
        "        if done:\n",
        "            evaluation_reward.append(score)\n",
        "            rewards.append(np.mean(evaluation_reward))\n",
        "            episodes.append(e)\n",
        "            pylab.plot(episodes, rewards, 'b')\n",
        "            pylab.xlabel('Episodes')\n",
        "            pylab.ylabel('Rewards') \n",
        "            pylab.title('Episodes vs Reward')\n",
        "            pylab.savefig(\"./save_graph/breakout_dqn.png\") # save graph for training visualization\n",
        "            \n",
        "            # every episode, plot the play time\n",
        "            print(\"episode:\", e, \"  score:\", score, \"  memory length:\",\n",
        "                  len(agent.memory), \"  epsilon:\", agent.epsilon, \"   steps:\", step,\n",
        "                  \"   lr:\", agent.optimizer.param_groups[0]['lr'], \"    evaluation reward:\", np.mean(evaluation_reward))\n",
        "\n",
        "            # if the mean of scores of last 100 episode is bigger than 5 save model\n",
        "            ### Change this save condition to whatever you prefer ###\n",
        "            if np.mean(evaluation_reward) > 5 and np.mean(evaluation_reward) > best_eval_reward:\n",
        "                torch.save(agent.policy_net, \"./save_model/breakout_dqn.pth\")\n",
        "                best_eval_reward = np.mean(evaluation_reward)\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "episode: 0   score: 1.0   memory length: 168   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.0\n",
            "episode: 1   score: 1.0   memory length: 337   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.0\n",
            "episode: 2   score: 2.0   memory length: 555   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.3333333333333333\n",
            "episode: 3   score: 1.0   memory length: 726   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.25\n",
            "episode: 4   score: 0.0   memory length: 849   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.0\n",
            "episode: 5   score: 0.0   memory length: 972   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 0.8333333333333334\n",
            "episode: 6   score: 1.0   memory length: 1124   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 0.8571428571428571\n",
            "episode: 7   score: 2.0   memory length: 1342   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.0\n",
            "episode: 8   score: 2.0   memory length: 1562   epsilon: 1.0    steps: 220    lr: 0.0001     evaluation reward: 1.1111111111111112\n",
            "episode: 9   score: 0.0   memory length: 1685   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.0\n",
            "episode: 10   score: 0.0   memory length: 1807   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 0.9090909090909091\n",
            "episode: 11   score: 0.0   memory length: 1929   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 0.8333333333333334\n",
            "episode: 12   score: 2.0   memory length: 2149   epsilon: 1.0    steps: 220    lr: 0.0001     evaluation reward: 0.9230769230769231\n",
            "episode: 13   score: 0.0   memory length: 2272   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 0.8571428571428571\n",
            "episode: 14   score: 0.0   memory length: 2394   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 0.8\n",
            "episode: 15   score: 3.0   memory length: 2638   epsilon: 1.0    steps: 244    lr: 0.0001     evaluation reward: 0.9375\n",
            "episode: 16   score: 3.0   memory length: 2885   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.0588235294117647\n",
            "episode: 17   score: 3.0   memory length: 3111   epsilon: 1.0    steps: 226    lr: 0.0001     evaluation reward: 1.1666666666666667\n",
            "episode: 18   score: 2.0   memory length: 3310   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.2105263157894737\n",
            "episode: 19   score: 2.0   memory length: 3529   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.25\n",
            "episode: 20   score: 3.0   memory length: 3774   epsilon: 1.0    steps: 245    lr: 0.0001     evaluation reward: 1.3333333333333333\n",
            "episode: 21   score: 2.0   memory length: 3972   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.3636363636363635\n",
            "episode: 22   score: 2.0   memory length: 4172   epsilon: 1.0    steps: 200    lr: 0.0001     evaluation reward: 1.391304347826087\n",
            "episode: 23   score: 0.0   memory length: 4295   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.3333333333333333\n",
            "episode: 24   score: 1.0   memory length: 4463   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.32\n",
            "episode: 25   score: 4.0   memory length: 4778   epsilon: 1.0    steps: 315    lr: 0.0001     evaluation reward: 1.4230769230769231\n",
            "episode: 26   score: 1.0   memory length: 4950   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.4074074074074074\n",
            "episode: 27   score: 2.0   memory length: 5148   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.4285714285714286\n",
            "episode: 28   score: 1.0   memory length: 5318   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.4137931034482758\n",
            "episode: 29   score: 0.0   memory length: 5441   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.3666666666666667\n",
            "episode: 30   score: 1.0   memory length: 5610   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.3548387096774193\n",
            "episode: 31   score: 0.0   memory length: 5732   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.3125\n",
            "episode: 32   score: 0.0   memory length: 5855   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.2727272727272727\n",
            "episode: 33   score: 1.0   memory length: 6024   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.2647058823529411\n",
            "episode: 34   score: 1.0   memory length: 6195   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.2571428571428571\n",
            "episode: 35   score: 2.0   memory length: 6395   epsilon: 1.0    steps: 200    lr: 0.0001     evaluation reward: 1.2777777777777777\n",
            "episode: 36   score: 2.0   memory length: 6578   epsilon: 1.0    steps: 183    lr: 0.0001     evaluation reward: 1.2972972972972974\n",
            "episode: 37   score: 2.0   memory length: 6794   epsilon: 1.0    steps: 216    lr: 0.0001     evaluation reward: 1.3157894736842106\n",
            "episode: 38   score: 0.0   memory length: 6917   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.2820512820512822\n",
            "episode: 39   score: 4.0   memory length: 7193   epsilon: 1.0    steps: 276    lr: 0.0001     evaluation reward: 1.35\n",
            "episode: 40   score: 1.0   memory length: 7344   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.3414634146341464\n",
            "episode: 41   score: 1.0   memory length: 7495   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.3333333333333333\n",
            "episode: 42   score: 1.0   memory length: 7664   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.3255813953488371\n",
            "episode: 43   score: 2.0   memory length: 7881   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.3409090909090908\n",
            "episode: 44   score: 0.0   memory length: 8003   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.3111111111111111\n",
            "episode: 45   score: 3.0   memory length: 8232   epsilon: 1.0    steps: 229    lr: 0.0001     evaluation reward: 1.3478260869565217\n",
            "episode: 46   score: 1.0   memory length: 8400   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.3404255319148937\n",
            "episode: 47   score: 2.0   memory length: 8600   epsilon: 1.0    steps: 200    lr: 0.0001     evaluation reward: 1.3541666666666667\n",
            "episode: 48   score: 3.0   memory length: 8867   epsilon: 1.0    steps: 267    lr: 0.0001     evaluation reward: 1.3877551020408163\n",
            "episode: 49   score: 4.0   memory length: 9164   epsilon: 1.0    steps: 297    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 50   score: 2.0   memory length: 9381   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.4509803921568627\n",
            "episode: 51   score: 2.0   memory length: 9602   epsilon: 1.0    steps: 221    lr: 0.0001     evaluation reward: 1.4615384615384615\n",
            "episode: 52   score: 2.0   memory length: 9820   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.471698113207547\n",
            "episode: 53   score: 0.0   memory length: 9943   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.4444444444444444\n",
            "episode: 54   score: 3.0   memory length: 10190   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.4727272727272727\n",
            "episode: 55   score: 2.0   memory length: 10407   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.4821428571428572\n",
            "episode: 56   score: 1.0   memory length: 10575   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.4736842105263157\n",
            "episode: 57   score: 0.0   memory length: 10698   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.4482758620689655\n",
            "episode: 58   score: 0.0   memory length: 10821   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.423728813559322\n",
            "episode: 59   score: 2.0   memory length: 11018   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.4333333333333333\n",
            "episode: 60   score: 2.0   memory length: 11236   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.4426229508196722\n",
            "episode: 61   score: 2.0   memory length: 11452   epsilon: 1.0    steps: 216    lr: 0.0001     evaluation reward: 1.4516129032258065\n",
            "episode: 62   score: 2.0   memory length: 11650   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.4603174603174602\n",
            "episode: 63   score: 0.0   memory length: 11773   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.4375\n",
            "episode: 64   score: 0.0   memory length: 11896   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.4153846153846155\n",
            "episode: 65   score: 0.0   memory length: 12019   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.393939393939394\n",
            "episode: 66   score: 4.0   memory length: 12297   epsilon: 1.0    steps: 278    lr: 0.0001     evaluation reward: 1.4328358208955223\n",
            "episode: 67   score: 2.0   memory length: 12494   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.4411764705882353\n",
            "episode: 68   score: 1.0   memory length: 12645   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.434782608695652\n",
            "episode: 69   score: 1.0   memory length: 12814   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.4285714285714286\n",
            "episode: 70   score: 2.0   memory length: 13031   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.4366197183098592\n",
            "episode: 71   score: 1.0   memory length: 13202   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.4305555555555556\n",
            "episode: 72   score: 1.0   memory length: 13373   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.4246575342465753\n",
            "episode: 73   score: 1.0   memory length: 13541   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.4189189189189189\n",
            "episode: 74   score: 0.0   memory length: 13664   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 75   score: 0.0   memory length: 13787   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.381578947368421\n",
            "episode: 76   score: 2.0   memory length: 14005   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.3896103896103895\n",
            "episode: 77   score: 4.0   memory length: 14303   epsilon: 1.0    steps: 298    lr: 0.0001     evaluation reward: 1.4230769230769231\n",
            "episode: 78   score: 0.0   memory length: 14425   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.4050632911392404\n",
            "episode: 79   score: 1.0   memory length: 14576   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 80   score: 2.0   memory length: 14777   epsilon: 1.0    steps: 201    lr: 0.0001     evaluation reward: 1.4074074074074074\n",
            "episode: 81   score: 2.0   memory length: 14975   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.4146341463414633\n",
            "episode: 82   score: 0.0   memory length: 15098   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.3975903614457832\n",
            "episode: 83   score: 1.0   memory length: 15266   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.3928571428571428\n",
            "episode: 84   score: 2.0   memory length: 15463   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 85   score: 2.0   memory length: 15678   epsilon: 1.0    steps: 215    lr: 0.0001     evaluation reward: 1.4069767441860466\n",
            "episode: 86   score: 0.0   memory length: 15801   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.3908045977011494\n",
            "episode: 87   score: 1.0   memory length: 15971   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.3863636363636365\n",
            "episode: 88   score: 0.0   memory length: 16094   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.3707865168539326\n",
            "episode: 89   score: 0.0   memory length: 16217   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.3555555555555556\n",
            "episode: 90   score: 1.0   memory length: 16370   epsilon: 1.0    steps: 153    lr: 0.0001     evaluation reward: 1.3516483516483517\n",
            "episode: 91   score: 1.0   memory length: 16542   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.3478260869565217\n",
            "episode: 92   score: 3.0   memory length: 16789   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.3655913978494623\n",
            "episode: 93   score: 0.0   memory length: 16912   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.351063829787234\n",
            "episode: 94   score: 3.0   memory length: 17159   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.368421052631579\n",
            "episode: 95   score: 1.0   memory length: 17331   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.3645833333333333\n",
            "episode: 96   score: 3.0   memory length: 17578   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.3814432989690721\n",
            "episode: 97   score: 2.0   memory length: 17776   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.3877551020408163\n",
            "episode: 98   score: 3.0   memory length: 18041   epsilon: 1.0    steps: 265    lr: 0.0001     evaluation reward: 1.404040404040404\n",
            "episode: 99   score: 3.0   memory length: 18287   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 100   score: 1.0   memory length: 18455   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 101   score: 2.0   memory length: 18670   epsilon: 1.0    steps: 215    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 102   score: 1.0   memory length: 18820   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 103   score: 1.0   memory length: 18989   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 104   score: 3.0   memory length: 19234   epsilon: 1.0    steps: 245    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 105   score: 0.0   memory length: 19356   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 106   score: 3.0   memory length: 19606   epsilon: 1.0    steps: 250    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 107   score: 1.0   memory length: 19775   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 108   score: 1.0   memory length: 19926   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 109   score: 1.0   memory length: 20076   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 110   score: 1.0   memory length: 20227   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 111   score: 0.0   memory length: 20349   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 112   score: 1.0   memory length: 20500   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 113   score: 0.0   memory length: 20622   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 114   score: 3.0   memory length: 20866   epsilon: 1.0    steps: 244    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 115   score: 0.0   memory length: 20988   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 116   score: 1.0   memory length: 21139   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 117   score: 2.0   memory length: 21356   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 118   score: 0.0   memory length: 21479   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 119   score: 0.0   memory length: 21601   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.39\n",
            "episode: 120   score: 1.0   memory length: 21769   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 121   score: 1.0   memory length: 21939   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 122   score: 2.0   memory length: 22156   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 123   score: 3.0   memory length: 22403   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.39\n",
            "episode: 124   score: 2.0   memory length: 22600   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 125   score: 1.0   memory length: 22751   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 126   score: 0.0   memory length: 22874   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 127   score: 3.0   memory length: 23124   epsilon: 1.0    steps: 250    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 128   score: 0.0   memory length: 23247   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 129   score: 0.0   memory length: 23370   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 130   score: 0.0   memory length: 23493   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.35\n",
            "episode: 131   score: 1.0   memory length: 23664   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 132   score: 2.0   memory length: 23843   epsilon: 1.0    steps: 179    lr: 0.0001     evaluation reward: 1.38\n",
            "episode: 133   score: 6.0   memory length: 24223   epsilon: 1.0    steps: 380    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 134   score: 2.0   memory length: 24439   epsilon: 1.0    steps: 216    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 135   score: 0.0   memory length: 24562   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 136   score: 3.0   memory length: 24788   epsilon: 1.0    steps: 226    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 137   score: 1.0   memory length: 24960   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 138   score: 1.0   memory length: 25129   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 139   score: 2.0   memory length: 25326   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 140   score: 1.0   memory length: 25477   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 141   score: 2.0   memory length: 25678   epsilon: 1.0    steps: 201    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 142   score: 1.0   memory length: 25850   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 143   score: 2.0   memory length: 26048   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 144   score: 0.0   memory length: 26170   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 145   score: 3.0   memory length: 26418   epsilon: 1.0    steps: 248    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 146   score: 5.0   memory length: 26725   epsilon: 1.0    steps: 307    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 147   score: 2.0   memory length: 26923   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 148   score: 0.0   memory length: 27046   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 149   score: 3.0   memory length: 27294   epsilon: 1.0    steps: 248    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 150   score: 2.0   memory length: 27512   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 151   score: 1.0   memory length: 27662   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 152   score: 2.0   memory length: 27878   epsilon: 1.0    steps: 216    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 153   score: 2.0   memory length: 28096   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 154   score: 1.0   memory length: 28266   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 155   score: 0.0   memory length: 28389   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.39\n",
            "episode: 156   score: 2.0   memory length: 28586   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 157   score: 0.0   memory length: 28709   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 158   score: 2.0   memory length: 28906   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 159   score: 2.0   memory length: 29103   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 160   score: 2.0   memory length: 29321   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 161   score: 0.0   memory length: 29444   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 162   score: 1.0   memory length: 29595   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.39\n",
            "episode: 163   score: 1.0   memory length: 29765   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 164   score: 0.0   memory length: 29887   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 165   score: 2.0   memory length: 30085   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 166   score: 4.0   memory length: 30360   epsilon: 1.0    steps: 275    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 167   score: 3.0   memory length: 30607   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 168   score: 0.0   memory length: 30730   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 169   score: 2.0   memory length: 30950   epsilon: 1.0    steps: 220    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 170   score: 2.0   memory length: 31168   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 171   score: 1.0   memory length: 31340   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 172   score: 0.0   memory length: 31462   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 173   score: 3.0   memory length: 31709   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 174   score: 3.0   memory length: 31956   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 175   score: 1.0   memory length: 32124   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 176   score: 1.0   memory length: 32275   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 177   score: 0.0   memory length: 32398   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 178   score: 1.0   memory length: 32567   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 179   score: 5.0   memory length: 32890   epsilon: 1.0    steps: 323    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 180   score: 0.0   memory length: 33012   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 181   score: 0.0   memory length: 33135   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 182   score: 0.0   memory length: 33257   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 183   score: 0.0   memory length: 33379   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 184   score: 2.0   memory length: 33579   epsilon: 1.0    steps: 200    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 185   score: 1.0   memory length: 33749   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 186   score: 4.0   memory length: 34041   epsilon: 1.0    steps: 292    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 187   score: 1.0   memory length: 34209   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 188   score: 1.0   memory length: 34360   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 189   score: 2.0   memory length: 34558   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 190   score: 4.0   memory length: 34853   epsilon: 1.0    steps: 295    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 191   score: 2.0   memory length: 35051   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 192   score: 1.0   memory length: 35222   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 193   score: 2.0   memory length: 35420   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 194   score: 1.0   memory length: 35592   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 195   score: 0.0   memory length: 35714   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 196   score: 0.0   memory length: 35837   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 197   score: 1.0   memory length: 35987   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 198   score: 0.0   memory length: 36110   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 199   score: 2.0   memory length: 36328   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 200   score: 1.0   memory length: 36500   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 201   score: 3.0   memory length: 36767   epsilon: 1.0    steps: 267    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 202   score: 2.0   memory length: 36985   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 203   score: 3.0   memory length: 37211   epsilon: 1.0    steps: 226    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 204   score: 0.0   memory length: 37334   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 205   score: 0.0   memory length: 37457   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 206   score: 3.0   memory length: 37724   epsilon: 1.0    steps: 267    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 207   score: 0.0   memory length: 37847   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 208   score: 1.0   memory length: 38016   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 209   score: 0.0   memory length: 38139   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 210   score: 1.0   memory length: 38309   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 211   score: 2.0   memory length: 38507   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 212   score: 0.0   memory length: 38629   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 213   score: 6.0   memory length: 38967   epsilon: 1.0    steps: 338    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 214   score: 0.0   memory length: 39090   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 215   score: 3.0   memory length: 39316   epsilon: 1.0    steps: 226    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 216   score: 0.0   memory length: 39438   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 217   score: 2.0   memory length: 39653   epsilon: 1.0    steps: 215    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 218   score: 3.0   memory length: 39880   epsilon: 1.0    steps: 227    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 219   score: 0.0   memory length: 40002   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 220   score: 2.0   memory length: 40200   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 221   score: 1.0   memory length: 40353   epsilon: 1.0    steps: 153    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 222   score: 0.0   memory length: 40476   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 223   score: 0.0   memory length: 40599   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 224   score: 5.0   memory length: 40892   epsilon: 1.0    steps: 293    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 225   score: 0.0   memory length: 41015   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 226   score: 3.0   memory length: 41280   epsilon: 1.0    steps: 265    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 227   score: 2.0   memory length: 41495   epsilon: 1.0    steps: 215    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 228   score: 3.0   memory length: 41741   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 229   score: 3.0   memory length: 41987   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 230   score: 2.0   memory length: 42185   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.58\n",
            "episode: 231   score: 2.0   memory length: 42403   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.59\n",
            "episode: 232   score: 5.0   memory length: 42708   epsilon: 1.0    steps: 305    lr: 0.0001     evaluation reward: 1.62\n",
            "episode: 233   score: 0.0   memory length: 42831   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 234   score: 1.0   memory length: 43001   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 235   score: 1.0   memory length: 43170   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 236   score: 2.0   memory length: 43368   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 237   score: 2.0   memory length: 43566   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 238   score: 1.0   memory length: 43736   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 239   score: 2.0   memory length: 43957   epsilon: 1.0    steps: 221    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 240   score: 0.0   memory length: 44080   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 241   score: 1.0   memory length: 44251   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 242   score: 4.0   memory length: 44544   epsilon: 1.0    steps: 293    lr: 0.0001     evaluation reward: 1.57\n",
            "episode: 243   score: 1.0   memory length: 44695   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 244   score: 3.0   memory length: 44920   epsilon: 1.0    steps: 225    lr: 0.0001     evaluation reward: 1.59\n",
            "episode: 245   score: 2.0   memory length: 45117   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.58\n",
            "episode: 246   score: 1.0   memory length: 45268   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 247   score: 0.0   memory length: 45391   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 248   score: 0.0   memory length: 45514   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 249   score: 0.0   memory length: 45637   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 250   score: 2.0   memory length: 45852   epsilon: 1.0    steps: 215    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 251   score: 4.0   memory length: 46129   epsilon: 1.0    steps: 277    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 252   score: 1.0   memory length: 46301   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 253   score: 1.0   memory length: 46471   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 254   score: 2.0   memory length: 46651   epsilon: 1.0    steps: 180    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 255   score: 0.0   memory length: 46774   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 256   score: 2.0   memory length: 46972   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 257   score: 0.0   memory length: 47095   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 258   score: 2.0   memory length: 47295   epsilon: 1.0    steps: 200    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 259   score: 0.0   memory length: 47418   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 260   score: 2.0   memory length: 47639   epsilon: 1.0    steps: 221    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 261   score: 1.0   memory length: 47811   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 262   score: 4.0   memory length: 48086   epsilon: 1.0    steps: 275    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 263   score: 1.0   memory length: 48256   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 264   score: 1.0   memory length: 48425   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 265   score: 2.0   memory length: 48626   epsilon: 1.0    steps: 201    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 266   score: 3.0   memory length: 48877   epsilon: 1.0    steps: 251    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 267   score: 2.0   memory length: 49075   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 268   score: 2.0   memory length: 49275   epsilon: 1.0    steps: 200    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 269   score: 2.0   memory length: 49491   epsilon: 1.0    steps: 216    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 270   score: 2.0   memory length: 49709   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 271   score: 4.0   memory length: 50023   epsilon: 1.0    steps: 314    lr: 0.0001     evaluation reward: 1.57\n",
            "episode: 272   score: 0.0   memory length: 50146   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.57\n",
            "episode: 273   score: 0.0   memory length: 50268   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 274   score: 0.0   memory length: 50390   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 275   score: 1.0   memory length: 50559   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 276   score: 1.0   memory length: 50728   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 277   score: 1.0   memory length: 50879   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 278   score: 1.0   memory length: 51030   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 279   score: 1.0   memory length: 51199   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 280   score: 1.0   memory length: 51350   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 281   score: 0.0   memory length: 51473   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 282   score: 1.0   memory length: 51624   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 283   score: 1.0   memory length: 51795   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 284   score: 2.0   memory length: 51995   epsilon: 1.0    steps: 200    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 285   score: 0.0   memory length: 52118   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 286   score: 1.0   memory length: 52287   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 287   score: 0.0   memory length: 52410   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 288   score: 3.0   memory length: 52656   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 289   score: 0.0   memory length: 52778   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 290   score: 2.0   memory length: 52976   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 291   score: 0.0   memory length: 53098   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 292   score: 1.0   memory length: 53269   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 293   score: 0.0   memory length: 53392   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 294   score: 4.0   memory length: 53707   epsilon: 1.0    steps: 315    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 295   score: 4.0   memory length: 54024   epsilon: 1.0    steps: 317    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 296   score: 2.0   memory length: 54243   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 297   score: 4.0   memory length: 54523   epsilon: 1.0    steps: 280    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 298   score: 4.0   memory length: 54820   epsilon: 1.0    steps: 297    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 299   score: 2.0   memory length: 55018   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 300   score: 3.0   memory length: 55244   epsilon: 1.0    steps: 226    lr: 0.0001     evaluation reward: 1.58\n",
            "episode: 301   score: 2.0   memory length: 55442   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.57\n",
            "episode: 302   score: 2.0   memory length: 55659   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.57\n",
            "episode: 303   score: 0.0   memory length: 55781   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 304   score: 2.0   memory length: 55998   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 305   score: 1.0   memory length: 56166   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.57\n",
            "episode: 306   score: 2.0   memory length: 56386   epsilon: 1.0    steps: 220    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 307   score: 0.0   memory length: 56509   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 308   score: 0.0   memory length: 56631   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 309   score: 2.0   memory length: 56829   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.57\n",
            "episode: 310   score: 2.0   memory length: 57047   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.58\n",
            "episode: 311   score: 2.0   memory length: 57244   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.58\n",
            "episode: 312   score: 0.0   memory length: 57367   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.58\n",
            "episode: 313   score: 3.0   memory length: 57634   epsilon: 1.0    steps: 267    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 314   score: 3.0   memory length: 57898   epsilon: 1.0    steps: 264    lr: 0.0001     evaluation reward: 1.58\n",
            "episode: 315   score: 2.0   memory length: 58116   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.57\n",
            "episode: 316   score: 0.0   memory length: 58239   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.57\n",
            "episode: 317   score: 0.0   memory length: 58361   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 318   score: 0.0   memory length: 58484   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 319   score: 0.0   memory length: 58607   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 320   score: 4.0   memory length: 58919   epsilon: 1.0    steps: 312    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 321   score: 0.0   memory length: 59041   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 322   score: 0.0   memory length: 59163   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 323   score: 2.0   memory length: 59361   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 324   score: 2.0   memory length: 59559   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 325   score: 1.0   memory length: 59710   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 326   score: 2.0   memory length: 59928   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 327   score: 0.0   memory length: 60051   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 328   score: 2.0   memory length: 60269   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 329   score: 0.0   memory length: 60392   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 330   score: 1.0   memory length: 60563   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 331   score: 1.0   memory length: 60733   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 332   score: 3.0   memory length: 60982   epsilon: 1.0    steps: 249    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 333   score: 0.0   memory length: 61105   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 334   score: 2.0   memory length: 61284   epsilon: 1.0    steps: 179    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 335   score: 1.0   memory length: 61435   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 336   score: 4.0   memory length: 61712   epsilon: 1.0    steps: 277    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 337   score: 0.0   memory length: 61834   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 338   score: 3.0   memory length: 62081   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 339   score: 2.0   memory length: 62296   epsilon: 1.0    steps: 215    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 340   score: 4.0   memory length: 62592   epsilon: 1.0    steps: 296    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 341   score: 1.0   memory length: 62762   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 342   score: 3.0   memory length: 63008   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 343   score: 0.0   memory length: 63131   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 344   score: 2.0   memory length: 63329   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 345   score: 0.0   memory length: 63451   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 346   score: 3.0   memory length: 63717   epsilon: 1.0    steps: 266    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 347   score: 1.0   memory length: 63868   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 348   score: 1.0   memory length: 64037   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 349   score: 0.0   memory length: 64160   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 350   score: 0.0   memory length: 64283   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 351   score: 1.0   memory length: 64434   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 352   score: 1.0   memory length: 64585   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 353   score: 1.0   memory length: 64753   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 354   score: 4.0   memory length: 65026   epsilon: 1.0    steps: 273    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 355   score: 1.0   memory length: 65197   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 356   score: 3.0   memory length: 65443   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 357   score: 0.0   memory length: 65566   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 358   score: 0.0   memory length: 65689   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 359   score: 3.0   memory length: 65919   epsilon: 1.0    steps: 230    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 360   score: 0.0   memory length: 66042   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 361   score: 1.0   memory length: 66211   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 362   score: 2.0   memory length: 66408   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 363   score: 0.0   memory length: 66531   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 364   score: 3.0   memory length: 66784   epsilon: 1.0    steps: 253    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 365   score: 0.0   memory length: 66907   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 366   score: 0.0   memory length: 67030   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 367   score: 2.0   memory length: 67228   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 368   score: 0.0   memory length: 67351   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.38\n",
            "episode: 369   score: 0.0   memory length: 67474   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 370   score: 0.0   memory length: 67596   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.34\n",
            "episode: 371   score: 3.0   memory length: 67863   epsilon: 1.0    steps: 267    lr: 0.0001     evaluation reward: 1.33\n",
            "episode: 372   score: 1.0   memory length: 68033   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.34\n",
            "episode: 373   score: 0.0   memory length: 68155   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.34\n",
            "episode: 374   score: 2.0   memory length: 68353   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 375   score: 0.0   memory length: 68476   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.35\n",
            "episode: 376   score: 0.0   memory length: 68599   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.34\n",
            "episode: 377   score: 0.0   memory length: 68721   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.33\n",
            "episode: 378   score: 2.0   memory length: 68919   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.34\n",
            "episode: 379   score: 1.0   memory length: 69087   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.34\n",
            "episode: 380   score: 3.0   memory length: 69333   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 381   score: 2.0   memory length: 69530   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.38\n",
            "episode: 382   score: 1.0   memory length: 69699   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.38\n",
            "episode: 383   score: 0.0   memory length: 69822   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 384   score: 4.0   memory length: 70080   epsilon: 1.0    steps: 258    lr: 0.0001     evaluation reward: 1.39\n",
            "episode: 385   score: 2.0   memory length: 70295   epsilon: 1.0    steps: 215    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 386   score: 1.0   memory length: 70445   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 387   score: 2.0   memory length: 70664   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 388   score: 1.0   memory length: 70833   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 389   score: 1.0   memory length: 71002   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 390   score: 1.0   memory length: 71153   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 391   score: 0.0   memory length: 71276   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 392   score: 1.0   memory length: 71427   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 393   score: 2.0   memory length: 71646   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 394   score: 0.0   memory length: 71768   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.39\n",
            "episode: 395   score: 3.0   memory length: 71999   epsilon: 1.0    steps: 231    lr: 0.0001     evaluation reward: 1.38\n",
            "episode: 396   score: 2.0   memory length: 72217   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.38\n",
            "episode: 397   score: 3.0   memory length: 72442   epsilon: 1.0    steps: 225    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 398   score: 3.0   memory length: 72707   epsilon: 1.0    steps: 265    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 399   score: 1.0   memory length: 72859   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.35\n",
            "episode: 400   score: 2.0   memory length: 73056   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.34\n",
            "episode: 401   score: 3.0   memory length: 73328   epsilon: 1.0    steps: 272    lr: 0.0001     evaluation reward: 1.35\n",
            "episode: 402   score: 2.0   memory length: 73546   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.35\n",
            "episode: 403   score: 0.0   memory length: 73669   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.35\n",
            "episode: 404   score: 3.0   memory length: 73895   epsilon: 1.0    steps: 226    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 405   score: 0.0   memory length: 74017   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.35\n",
            "episode: 406   score: 1.0   memory length: 74167   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.34\n",
            "episode: 407   score: 0.0   memory length: 74290   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.34\n",
            "episode: 408   score: 2.0   memory length: 74488   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 409   score: 1.0   memory length: 74638   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.35\n",
            "episode: 410   score: 2.0   memory length: 74820   epsilon: 1.0    steps: 182    lr: 0.0001     evaluation reward: 1.35\n",
            "episode: 411   score: 1.0   memory length: 74992   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.34\n",
            "episode: 412   score: 2.0   memory length: 75190   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 413   score: 1.0   memory length: 75359   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.34\n",
            "episode: 414   score: 1.0   memory length: 75528   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.32\n",
            "episode: 415   score: 3.0   memory length: 75774   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.33\n",
            "episode: 416   score: 0.0   memory length: 75897   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.33\n",
            "episode: 417   score: 0.0   memory length: 76020   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.33\n",
            "episode: 418   score: 3.0   memory length: 76248   epsilon: 1.0    steps: 228    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 419   score: 1.0   memory length: 76420   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 420   score: 2.0   memory length: 76638   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.35\n",
            "episode: 421   score: 2.0   memory length: 76857   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 422   score: 1.0   memory length: 77026   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.38\n",
            "episode: 423   score: 1.0   memory length: 77195   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 424   score: 0.0   memory length: 77318   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.35\n",
            "episode: 425   score: 2.0   memory length: 77534   epsilon: 1.0    steps: 216    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 426   score: 0.0   memory length: 77657   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.34\n",
            "episode: 427   score: 0.0   memory length: 77780   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.34\n",
            "episode: 428   score: 1.0   memory length: 77931   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.33\n",
            "episode: 429   score: 1.0   memory length: 78100   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.34\n",
            "episode: 430   score: 2.0   memory length: 78318   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.35\n",
            "episode: 431   score: 0.0   memory length: 78440   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.34\n",
            "episode: 432   score: 2.0   memory length: 78625   epsilon: 1.0    steps: 185    lr: 0.0001     evaluation reward: 1.33\n",
            "episode: 433   score: 6.0   memory length: 78979   epsilon: 1.0    steps: 354    lr: 0.0001     evaluation reward: 1.39\n",
            "episode: 434   score: 1.0   memory length: 79151   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.38\n",
            "episode: 435   score: 0.0   memory length: 79274   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 436   score: 0.0   memory length: 79397   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.33\n",
            "episode: 437   score: 0.0   memory length: 79519   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.33\n",
            "episode: 438   score: 1.0   memory length: 79689   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.31\n",
            "episode: 439   score: 1.0   memory length: 79860   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.3\n",
            "episode: 440   score: 0.0   memory length: 79983   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.26\n",
            "episode: 441   score: 0.0   memory length: 80106   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.25\n",
            "episode: 442   score: 0.0   memory length: 80229   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.22\n",
            "episode: 443   score: 0.0   memory length: 80351   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.22\n",
            "episode: 444   score: 1.0   memory length: 80520   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.21\n",
            "episode: 445   score: 3.0   memory length: 80747   epsilon: 1.0    steps: 227    lr: 0.0001     evaluation reward: 1.24\n",
            "episode: 446   score: 0.0   memory length: 80869   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.21\n",
            "episode: 447   score: 1.0   memory length: 81039   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.21\n",
            "episode: 448   score: 1.0   memory length: 81208   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.21\n",
            "episode: 449   score: 3.0   memory length: 81458   epsilon: 1.0    steps: 250    lr: 0.0001     evaluation reward: 1.24\n",
            "episode: 450   score: 3.0   memory length: 81725   epsilon: 1.0    steps: 267    lr: 0.0001     evaluation reward: 1.27\n",
            "episode: 451   score: 4.0   memory length: 82003   epsilon: 1.0    steps: 278    lr: 0.0001     evaluation reward: 1.3\n",
            "episode: 452   score: 1.0   memory length: 82153   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.3\n",
            "episode: 453   score: 2.0   memory length: 82351   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.31\n",
            "episode: 454   score: 2.0   memory length: 82566   epsilon: 1.0    steps: 215    lr: 0.0001     evaluation reward: 1.29\n",
            "episode: 455   score: 2.0   memory length: 82763   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.3\n",
            "episode: 456   score: 2.0   memory length: 82961   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.29\n",
            "episode: 457   score: 2.0   memory length: 83159   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.31\n",
            "episode: 458   score: 3.0   memory length: 83385   epsilon: 1.0    steps: 226    lr: 0.0001     evaluation reward: 1.34\n",
            "episode: 459   score: 0.0   memory length: 83507   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.31\n",
            "episode: 460   score: 2.0   memory length: 83705   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.33\n",
            "episode: 461   score: 0.0   memory length: 83827   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.32\n",
            "episode: 462   score: 3.0   memory length: 84095   epsilon: 1.0    steps: 268    lr: 0.0001     evaluation reward: 1.33\n",
            "episode: 463   score: 2.0   memory length: 84314   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.35\n",
            "episode: 464   score: 2.0   memory length: 84531   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.34\n",
            "episode: 465   score: 0.0   memory length: 84653   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.34\n",
            "episode: 466   score: 2.0   memory length: 84851   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 467   score: 4.0   memory length: 85166   epsilon: 1.0    steps: 315    lr: 0.0001     evaluation reward: 1.38\n",
            "episode: 468   score: 0.0   memory length: 85289   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.38\n",
            "episode: 469   score: 7.0   memory length: 85611   epsilon: 1.0    steps: 322    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 470   score: 0.0   memory length: 85734   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 471   score: 2.0   memory length: 85950   epsilon: 1.0    steps: 216    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 472   score: 3.0   memory length: 86178   epsilon: 1.0    steps: 228    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 473   score: 1.0   memory length: 86347   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 474   score: 3.0   memory length: 86576   epsilon: 1.0    steps: 229    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 475   score: 1.0   memory length: 86745   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 476   score: 3.0   memory length: 86995   epsilon: 1.0    steps: 250    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 477   score: 3.0   memory length: 87261   epsilon: 1.0    steps: 266    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 478   score: 0.0   memory length: 87384   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 479   score: 6.0   memory length: 87712   epsilon: 1.0    steps: 328    lr: 0.0001     evaluation reward: 1.58\n",
            "episode: 480   score: 0.0   memory length: 87835   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 481   score: 1.0   memory length: 87988   epsilon: 1.0    steps: 153    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 482   score: 2.0   memory length: 88185   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 483   score: 0.0   memory length: 88307   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 484   score: 0.0   memory length: 88430   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 485   score: 0.0   memory length: 88553   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 486   score: 2.0   memory length: 88734   epsilon: 1.0    steps: 181    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 487   score: 3.0   memory length: 88995   epsilon: 1.0    steps: 261    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 488   score: 1.0   memory length: 89146   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 489   score: 4.0   memory length: 89442   epsilon: 1.0    steps: 296    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 490   score: 1.0   memory length: 89611   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 491   score: 1.0   memory length: 89781   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 492   score: 2.0   memory length: 89979   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 493   score: 1.0   memory length: 90150   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 494   score: 3.0   memory length: 90376   epsilon: 1.0    steps: 226    lr: 0.0001     evaluation reward: 1.58\n",
            "episode: 495   score: 1.0   memory length: 90545   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 496   score: 1.0   memory length: 90716   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 497   score: 1.0   memory length: 90887   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 498   score: 0.0   memory length: 91010   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 499   score: 0.0   memory length: 91133   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 500   score: 1.0   memory length: 91284   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 501   score: 2.0   memory length: 91500   epsilon: 1.0    steps: 216    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 502   score: 0.0   memory length: 91623   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 503   score: 2.0   memory length: 91841   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 504   score: 0.0   memory length: 91963   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 505   score: 2.0   memory length: 92161   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 506   score: 2.0   memory length: 92378   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 507   score: 2.0   memory length: 92578   epsilon: 1.0    steps: 200    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 508   score: 0.0   memory length: 92700   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 509   score: 1.0   memory length: 92872   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 510   score: 0.0   memory length: 92994   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 511   score: 1.0   memory length: 93162   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 512   score: 2.0   memory length: 93363   epsilon: 1.0    steps: 201    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 513   score: 2.0   memory length: 93560   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 514   score: 2.0   memory length: 93760   epsilon: 1.0    steps: 200    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 515   score: 2.0   memory length: 93960   epsilon: 1.0    steps: 200    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 516   score: 1.0   memory length: 94110   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 517   score: 4.0   memory length: 94425   epsilon: 1.0    steps: 315    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 518   score: 2.0   memory length: 94644   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 519   score: 7.0   memory length: 94943   epsilon: 1.0    steps: 299    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 520   score: 2.0   memory length: 95141   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 521   score: 3.0   memory length: 95387   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.57\n",
            "episode: 522   score: 1.0   memory length: 95556   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.57\n",
            "episode: 523   score: 2.0   memory length: 95754   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.58\n",
            "episode: 524   score: 4.0   memory length: 96070   epsilon: 1.0    steps: 316    lr: 0.0001     evaluation reward: 1.62\n",
            "episode: 525   score: 1.0   memory length: 96239   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.61\n",
            "episode: 526   score: 0.0   memory length: 96362   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.61\n",
            "episode: 527   score: 2.0   memory length: 96560   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.63\n",
            "episode: 528   score: 0.0   memory length: 96683   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.62\n",
            "episode: 529   score: 1.0   memory length: 96834   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.62\n",
            "episode: 530   score: 2.0   memory length: 97054   epsilon: 1.0    steps: 220    lr: 0.0001     evaluation reward: 1.62\n",
            "episode: 531   score: 0.0   memory length: 97177   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.62\n",
            "episode: 532   score: 3.0   memory length: 97425   epsilon: 1.0    steps: 248    lr: 0.0001     evaluation reward: 1.63\n",
            "episode: 533   score: 0.0   memory length: 97548   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.57\n",
            "episode: 534   score: 0.0   memory length: 97671   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 535   score: 0.0   memory length: 97794   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 536   score: 1.0   memory length: 97944   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.57\n",
            "episode: 537   score: 0.0   memory length: 98067   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.57\n",
            "episode: 538   score: 1.0   memory length: 98218   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.57\n",
            "episode: 539   score: 0.0   memory length: 98340   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 540   score: 1.0   memory length: 98510   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.57\n",
            "episode: 541   score: 2.0   memory length: 98728   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.59\n",
            "episode: 542   score: 2.0   memory length: 98925   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.61\n",
            "episode: 543   score: 2.0   memory length: 99123   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.63\n",
            "episode: 544   score: 1.0   memory length: 99291   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.63\n",
            "episode: 545   score: 2.0   memory length: 99489   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.62\n",
            "episode: 546   score: 1.0   memory length: 99658   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.63\n",
            "episode: 547   score: 0.0   memory length: 99780   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.62\n",
            "episode: 548   score: 3.0   memory length: 99993   epsilon: 1.0    steps: 213    lr: 0.0001     evaluation reward: 1.64\n",
            "episode: 549   score: 2.0   memory length: 100193   epsilon: 0.9996158800000083    steps: 200    lr: 0.0001     evaluation reward: 1.63\n",
            "episode: 550   score: 1.0   memory length: 100362   epsilon: 0.9992812600000156    steps: 169    lr: 0.0001     evaluation reward: 1.61\n",
            "episode: 551   score: 2.0   memory length: 100547   epsilon: 0.9989149600000236    steps: 185    lr: 0.0001     evaluation reward: 1.59\n",
            "episode: 552   score: 3.0   memory length: 100812   epsilon: 0.998390260000035    steps: 265    lr: 0.0001     evaluation reward: 1.61\n",
            "episode: 553   score: 0.0   memory length: 100935   epsilon: 0.9981467200000402    steps: 123    lr: 0.0001     evaluation reward: 1.59\n",
            "episode: 554   score: 2.0   memory length: 101132   epsilon: 0.9977566600000487    steps: 197    lr: 0.0001     evaluation reward: 1.59\n",
            "episode: 555   score: 2.0   memory length: 101330   epsilon: 0.9973646200000572    steps: 198    lr: 0.0001     evaluation reward: 1.59\n",
            "episode: 556   score: 1.0   memory length: 101500   epsilon: 0.9970280200000645    steps: 170    lr: 0.0001     evaluation reward: 1.58\n",
            "episode: 557   score: 0.0   memory length: 101623   epsilon: 0.9967844800000698    steps: 123    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 558   score: 3.0   memory length: 101890   epsilon: 0.9962558200000813    steps: 267    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 559   score: 0.0   memory length: 102013   epsilon: 0.9960122800000866    steps: 123    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 560   score: 2.0   memory length: 102210   epsilon: 0.995622220000095    steps: 197    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 561   score: 1.0   memory length: 102378   epsilon: 0.9952895800001023    steps: 168    lr: 0.0001     evaluation reward: 1.57\n",
            "episode: 562   score: 2.0   memory length: 102594   epsilon: 0.9948619000001115    steps: 216    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 563   score: 1.0   memory length: 102745   epsilon: 0.994562920000118    steps: 151    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 564   score: 3.0   memory length: 102971   epsilon: 0.9941154400001277    steps: 226    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 565   score: 0.0   memory length: 103094   epsilon: 0.993871900000133    steps: 123    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 566   score: 1.0   memory length: 103245   epsilon: 0.9935729200001395    steps: 151    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 567   score: 1.0   memory length: 103396   epsilon: 0.993273940000146    steps: 151    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 568   score: 3.0   memory length: 103641   epsilon: 0.9927888400001565    steps: 245    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 569   score: 1.0   memory length: 103809   epsilon: 0.9924562000001638    steps: 168    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 570   score: 2.0   memory length: 104006   epsilon: 0.9920661400001722    steps: 197    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 571   score: 0.0   memory length: 104128   epsilon: 0.9918245800001775    steps: 122    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 572   score: 1.0   memory length: 104279   epsilon: 0.991525600000184    steps: 151    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 573   score: 0.0   memory length: 104402   epsilon: 0.9912820600001893    steps: 123    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 574   score: 2.0   memory length: 104600   epsilon: 0.9908900200001978    steps: 198    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 575   score: 2.0   memory length: 104797   epsilon: 0.9904999600002062    steps: 197    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 576   score: 2.0   memory length: 104995   epsilon: 0.9901079200002147    steps: 198    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 577   score: 0.0   memory length: 105118   epsilon: 0.98986438000022    steps: 123    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 578   score: 2.0   memory length: 105315   epsilon: 0.9894743200002285    steps: 197    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 579   score: 0.0   memory length: 105438   epsilon: 0.9892307800002338    steps: 123    lr: 0.0001     evaluation reward: 1.38\n",
            "episode: 580   score: 0.0   memory length: 105561   epsilon: 0.9889872400002391    steps: 123    lr: 0.0001     evaluation reward: 1.38\n",
            "episode: 581   score: 1.0   memory length: 105729   epsilon: 0.9886546000002463    steps: 168    lr: 0.0001     evaluation reward: 1.38\n",
            "episode: 582   score: 0.0   memory length: 105852   epsilon: 0.9884110600002516    steps: 123    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 583   score: 1.0   memory length: 106003   epsilon: 0.9881120800002581    steps: 151    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 584   score: 0.0   memory length: 106125   epsilon: 0.9878705200002633    steps: 122    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 585   score: 3.0   memory length: 106371   epsilon: 0.9873834400002739    steps: 246    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 586   score: 2.0   memory length: 106551   epsilon: 0.9870270400002816    steps: 180    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 587   score: 2.0   memory length: 106769   epsilon: 0.986595400000291    steps: 218    lr: 0.0001     evaluation reward: 1.39\n",
            "episode: 588   score: 2.0   memory length: 106985   epsilon: 0.9861677200003003    steps: 216    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 589   score: 2.0   memory length: 107183   epsilon: 0.9857756800003088    steps: 198    lr: 0.0001     evaluation reward: 1.38\n",
            "episode: 590   score: 2.0   memory length: 107380   epsilon: 0.9853856200003173    steps: 197    lr: 0.0001     evaluation reward: 1.39\n",
            "episode: 591   score: 0.0   memory length: 107502   epsilon: 0.9851440600003225    steps: 122    lr: 0.0001     evaluation reward: 1.38\n",
            "episode: 592   score: 1.0   memory length: 107653   epsilon: 0.984845080000329    steps: 151    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 593   score: 0.0   memory length: 107776   epsilon: 0.9846015400003343    steps: 123    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 594   score: 2.0   memory length: 107974   epsilon: 0.9842095000003428    steps: 198    lr: 0.0001     evaluation reward: 1.35\n",
            "episode: 595   score: 2.0   memory length: 108172   epsilon: 0.9838174600003513    steps: 198    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 596   score: 1.0   memory length: 108343   epsilon: 0.9834788800003587    steps: 171    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 597   score: 0.0   memory length: 108466   epsilon: 0.9832353400003639    steps: 123    lr: 0.0001     evaluation reward: 1.35\n",
            "episode: 598   score: 6.0   memory length: 108881   epsilon: 0.9824136400003818    steps: 415    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 599   score: 4.0   memory length: 109155   epsilon: 0.9818711200003936    steps: 274    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 600   score: 3.0   memory length: 109404   epsilon: 0.9813781000004043    steps: 249    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 601   score: 0.0   memory length: 109527   epsilon: 0.9811345600004096    steps: 123    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 602   score: 3.0   memory length: 109771   epsilon: 0.98065144000042    steps: 244    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 603   score: 3.0   memory length: 110000   epsilon: 0.9801980200004299    steps: 229    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 604   score: 0.0   memory length: 110123   epsilon: 0.9799544800004352    steps: 123    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 605   score: 3.0   memory length: 110368   epsilon: 0.9794693800004457    steps: 245    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 606   score: 2.0   memory length: 110566   epsilon: 0.9790773400004542    steps: 198    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 607   score: 2.0   memory length: 110784   epsilon: 0.9786457000004636    steps: 218    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 608   score: 2.0   memory length: 111002   epsilon: 0.978214060000473    steps: 218    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 609   score: 2.0   memory length: 111218   epsilon: 0.9777863800004822    steps: 216    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 610   score: 0.0   memory length: 111341   epsilon: 0.9775428400004875    steps: 123    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 611   score: 1.0   memory length: 111513   epsilon: 0.9772022800004949    steps: 172    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 612   score: 0.0   memory length: 111636   epsilon: 0.9769587400005002    steps: 123    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 613   score: 5.0   memory length: 111974   epsilon: 0.9762895000005147    steps: 338    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 614   score: 2.0   memory length: 112192   epsilon: 0.9758578600005241    steps: 218    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 615   score: 3.0   memory length: 112424   epsilon: 0.9753985000005341    steps: 232    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 616   score: 2.0   memory length: 112642   epsilon: 0.9749668600005434    steps: 218    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 617   score: 0.0   memory length: 112764   epsilon: 0.9747253000005487    steps: 122    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 618   score: 1.0   memory length: 112917   epsilon: 0.9744223600005553    steps: 153    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 619   score: 1.0   memory length: 113086   epsilon: 0.9740877400005625    steps: 169    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 620   score: 3.0   memory length: 113332   epsilon: 0.9736006600005731    steps: 246    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 621   score: 0.0   memory length: 113455   epsilon: 0.9733571200005784    steps: 123    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 622   score: 2.0   memory length: 113653   epsilon: 0.9729650800005869    steps: 198    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 623   score: 1.0   memory length: 113804   epsilon: 0.9726661000005934    steps: 151    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 624   score: 2.0   memory length: 114020   epsilon: 0.9722384200006027    steps: 216    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 625   score: 3.0   memory length: 114247   epsilon: 0.9717889600006124    steps: 227    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 626   score: 0.0   memory length: 114370   epsilon: 0.9715454200006177    steps: 123    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 627   score: 0.0   memory length: 114493   epsilon: 0.971301880000623    steps: 123    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 628   score: 3.0   memory length: 114738   epsilon: 0.9708167800006335    steps: 245    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 629   score: 1.0   memory length: 114907   epsilon: 0.9704821600006408    steps: 169    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 630   score: 2.0   memory length: 115086   epsilon: 0.9701277400006485    steps: 179    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 631   score: 10.0   memory length: 115523   epsilon: 0.9692624800006673    steps: 437    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 632   score: 1.0   memory length: 115674   epsilon: 0.9689635000006738    steps: 151    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 633   score: 1.0   memory length: 115844   epsilon: 0.9686269000006811    steps: 170    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 634   score: 1.0   memory length: 116013   epsilon: 0.9682922800006883    steps: 169    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 635   score: 1.0   memory length: 116182   epsilon: 0.9679576600006956    steps: 169    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 636   score: 0.0   memory length: 116305   epsilon: 0.9677141200007009    steps: 123    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 637   score: 1.0   memory length: 116477   epsilon: 0.9673735600007083    steps: 172    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 638   score: 7.0   memory length: 116833   epsilon: 0.9666686800007236    steps: 356    lr: 0.0001     evaluation reward: 1.61\n",
            "episode: 639   score: 0.0   memory length: 116956   epsilon: 0.9664251400007289    steps: 123    lr: 0.0001     evaluation reward: 1.61\n",
            "episode: 640   score: 4.0   memory length: 117232   epsilon: 0.9658786600007407    steps: 276    lr: 0.0001     evaluation reward: 1.64\n",
            "episode: 641   score: 0.0   memory length: 117355   epsilon: 0.965635120000746    steps: 123    lr: 0.0001     evaluation reward: 1.62\n",
            "episode: 642   score: 3.0   memory length: 117604   epsilon: 0.9651421000007567    steps: 249    lr: 0.0001     evaluation reward: 1.63\n",
            "episode: 643   score: 1.0   memory length: 117754   epsilon: 0.9648451000007632    steps: 150    lr: 0.0001     evaluation reward: 1.62\n",
            "episode: 644   score: 0.0   memory length: 117877   epsilon: 0.9646015600007685    steps: 123    lr: 0.0001     evaluation reward: 1.61\n",
            "episode: 645   score: 3.0   memory length: 118103   epsilon: 0.9641540800007782    steps: 226    lr: 0.0001     evaluation reward: 1.62\n",
            "episode: 646   score: 1.0   memory length: 118273   epsilon: 0.9638174800007855    steps: 170    lr: 0.0001     evaluation reward: 1.62\n",
            "episode: 647   score: 0.0   memory length: 118395   epsilon: 0.9635759200007907    steps: 122    lr: 0.0001     evaluation reward: 1.62\n",
            "episode: 648   score: 2.0   memory length: 118614   epsilon: 0.9631423000008001    steps: 219    lr: 0.0001     evaluation reward: 1.61\n",
            "episode: 649   score: 2.0   memory length: 118829   epsilon: 0.9627166000008094    steps: 215    lr: 0.0001     evaluation reward: 1.61\n",
            "episode: 650   score: 0.0   memory length: 118951   epsilon: 0.9624750400008146    steps: 122    lr: 0.0001     evaluation reward: 1.6\n",
            "episode: 651   score: 0.0   memory length: 119074   epsilon: 0.9622315000008199    steps: 123    lr: 0.0001     evaluation reward: 1.58\n",
            "episode: 652   score: 0.0   memory length: 119197   epsilon: 0.9619879600008252    steps: 123    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 653   score: 2.0   memory length: 119394   epsilon: 0.9615979000008337    steps: 197    lr: 0.0001     evaluation reward: 1.57\n",
            "episode: 654   score: 0.0   memory length: 119516   epsilon: 0.9613563400008389    steps: 122    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 655   score: 2.0   memory length: 119714   epsilon: 0.9609643000008474    steps: 198    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 656   score: 3.0   memory length: 119979   epsilon: 0.9604396000008588    steps: 265    lr: 0.0001     evaluation reward: 1.57\n",
            "episode: 657   score: 3.0   memory length: 120245   epsilon: 0.9599129200008703    steps: 266    lr: 0.0001     evaluation reward: 1.6\n",
            "episode: 658   score: 0.0   memory length: 120368   epsilon: 0.9596693800008755    steps: 123    lr: 0.0001     evaluation reward: 1.57\n",
            "episode: 659   score: 3.0   memory length: 120612   epsilon: 0.959186260000886    steps: 244    lr: 0.0001     evaluation reward: 1.6\n",
            "episode: 660   score: 0.0   memory length: 120735   epsilon: 0.9589427200008913    steps: 123    lr: 0.0001     evaluation reward: 1.58\n",
            "episode: 661   score: 0.0   memory length: 120858   epsilon: 0.9586991800008966    steps: 123    lr: 0.0001     evaluation reward: 1.57\n",
            "episode: 662   score: 2.0   memory length: 121074   epsilon: 0.9582715000009059    steps: 216    lr: 0.0001     evaluation reward: 1.57\n",
            "episode: 663   score: 0.0   memory length: 121197   epsilon: 0.9580279600009112    steps: 123    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 664   score: 2.0   memory length: 121395   epsilon: 0.9576359200009197    steps: 198    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 665   score: 2.0   memory length: 121596   epsilon: 0.9572379400009283    steps: 201    lr: 0.0001     evaluation reward: 1.57\n",
            "episode: 666   score: 1.0   memory length: 121765   epsilon: 0.9569033200009356    steps: 169    lr: 0.0001     evaluation reward: 1.57\n",
            "episode: 667   score: 0.0   memory length: 121888   epsilon: 0.9566597800009409    steps: 123    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 668   score: 7.0   memory length: 122263   epsilon: 0.955917280000957    steps: 375    lr: 0.0001     evaluation reward: 1.6\n",
            "episode: 669   score: 1.0   memory length: 122434   epsilon: 0.9555787000009643    steps: 171    lr: 0.0001     evaluation reward: 1.6\n",
            "episode: 670   score: 2.0   memory length: 122653   epsilon: 0.9551450800009738    steps: 219    lr: 0.0001     evaluation reward: 1.6\n",
            "episode: 671   score: 1.0   memory length: 122822   epsilon: 0.954810460000981    steps: 169    lr: 0.0001     evaluation reward: 1.61\n",
            "episode: 672   score: 0.0   memory length: 122945   epsilon: 0.9545669200009863    steps: 123    lr: 0.0001     evaluation reward: 1.6\n",
            "episode: 673   score: 3.0   memory length: 123170   epsilon: 0.954121420000996    steps: 225    lr: 0.0001     evaluation reward: 1.63\n",
            "episode: 674   score: 1.0   memory length: 123342   epsilon: 0.9537808600010034    steps: 172    lr: 0.0001     evaluation reward: 1.62\n",
            "episode: 675   score: 0.0   memory length: 123465   epsilon: 0.9535373200010087    steps: 123    lr: 0.0001     evaluation reward: 1.6\n",
            "episode: 676   score: 1.0   memory length: 123616   epsilon: 0.9532383400010151    steps: 151    lr: 0.0001     evaluation reward: 1.59\n",
            "episode: 677   score: 4.0   memory length: 123884   epsilon: 0.9527077000010267    steps: 268    lr: 0.0001     evaluation reward: 1.63\n",
            "episode: 678   score: 0.0   memory length: 124007   epsilon: 0.952464160001032    steps: 123    lr: 0.0001     evaluation reward: 1.61\n",
            "episode: 679   score: 2.0   memory length: 124208   epsilon: 0.9520661800010406    steps: 201    lr: 0.0001     evaluation reward: 1.63\n",
            "episode: 680   score: 3.0   memory length: 124474   epsilon: 0.951539500001052    steps: 266    lr: 0.0001     evaluation reward: 1.66\n",
            "episode: 681   score: 2.0   memory length: 124672   epsilon: 0.9511474600010605    steps: 198    lr: 0.0001     evaluation reward: 1.67\n",
            "episode: 682   score: 1.0   memory length: 124843   epsilon: 0.9508088800010679    steps: 171    lr: 0.0001     evaluation reward: 1.68\n",
            "episode: 683   score: 2.0   memory length: 125041   epsilon: 0.9504168400010764    steps: 198    lr: 0.0001     evaluation reward: 1.69\n",
            "episode: 684   score: 4.0   memory length: 125298   epsilon: 0.9499079800010874    steps: 257    lr: 0.0001     evaluation reward: 1.73\n",
            "episode: 685   score: 2.0   memory length: 125496   epsilon: 0.949515940001096    steps: 198    lr: 0.0001     evaluation reward: 1.72\n",
            "episode: 686   score: 1.0   memory length: 125666   epsilon: 0.9491793400011033    steps: 170    lr: 0.0001     evaluation reward: 1.71\n",
            "episode: 687   score: 1.0   memory length: 125834   epsilon: 0.9488467000011105    steps: 168    lr: 0.0001     evaluation reward: 1.7\n",
            "episode: 688   score: 0.0   memory length: 125957   epsilon: 0.9486031600011158    steps: 123    lr: 0.0001     evaluation reward: 1.68\n",
            "episode: 689   score: 1.0   memory length: 126108   epsilon: 0.9483041800011223    steps: 151    lr: 0.0001     evaluation reward: 1.67\n",
            "episode: 690   score: 1.0   memory length: 126279   epsilon: 0.9479656000011296    steps: 171    lr: 0.0001     evaluation reward: 1.66\n",
            "episode: 691   score: 0.0   memory length: 126402   epsilon: 0.9477220600011349    steps: 123    lr: 0.0001     evaluation reward: 1.66\n",
            "episode: 692   score: 3.0   memory length: 126628   epsilon: 0.9472745800011446    steps: 226    lr: 0.0001     evaluation reward: 1.68\n",
            "episode: 693   score: 1.0   memory length: 126779   epsilon: 0.9469756000011511    steps: 151    lr: 0.0001     evaluation reward: 1.69\n",
            "episode: 694   score: 1.0   memory length: 126950   epsilon: 0.9466370200011585    steps: 171    lr: 0.0001     evaluation reward: 1.68\n",
            "episode: 695   score: 0.0   memory length: 127073   epsilon: 0.9463934800011637    steps: 123    lr: 0.0001     evaluation reward: 1.66\n",
            "episode: 696   score: 0.0   memory length: 127196   epsilon: 0.946149940001169    steps: 123    lr: 0.0001     evaluation reward: 1.65\n",
            "episode: 697   score: 1.0   memory length: 127347   epsilon: 0.9458509600011755    steps: 151    lr: 0.0001     evaluation reward: 1.66\n",
            "episode: 698   score: 2.0   memory length: 127545   epsilon: 0.945458920001184    steps: 198    lr: 0.0001     evaluation reward: 1.62\n",
            "episode: 699   score: 1.0   memory length: 127696   epsilon: 0.9451599400011905    steps: 151    lr: 0.0001     evaluation reward: 1.59\n",
            "episode: 700   score: 1.0   memory length: 127847   epsilon: 0.944860960001197    steps: 151    lr: 0.0001     evaluation reward: 1.57\n",
            "episode: 701   score: 2.0   memory length: 128064   epsilon: 0.9444313000012063    steps: 217    lr: 0.0001     evaluation reward: 1.59\n",
            "episode: 702   score: 0.0   memory length: 128187   epsilon: 0.9441877600012116    steps: 123    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 703   score: 1.0   memory length: 128338   epsilon: 0.9438887800012181    steps: 151    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 704   score: 0.0   memory length: 128461   epsilon: 0.9436452400012234    steps: 123    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 705   score: 2.0   memory length: 128659   epsilon: 0.9432532000012319    steps: 198    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 706   score: 1.0   memory length: 128828   epsilon: 0.9429185800012392    steps: 169    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 707   score: 0.0   memory length: 128951   epsilon: 0.9426750400012445    steps: 123    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 708   score: 3.0   memory length: 129199   epsilon: 0.9421840000012551    steps: 248    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 709   score: 2.0   memory length: 129397   epsilon: 0.9417919600012636    steps: 198    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 710   score: 1.0   memory length: 129569   epsilon: 0.941451400001271    steps: 172    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 711   score: 1.0   memory length: 129741   epsilon: 0.9411108400012784    steps: 172    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 712   score: 3.0   memory length: 129985   epsilon: 0.9406277200012889    steps: 244    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 713   score: 1.0   memory length: 130136   epsilon: 0.9403287400012954    steps: 151    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 714   score: 2.0   memory length: 130333   epsilon: 0.9399386800013039    steps: 197    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 715   score: 2.0   memory length: 130531   epsilon: 0.9395466400013124    steps: 198    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 716   score: 2.0   memory length: 130747   epsilon: 0.9391189600013217    steps: 216    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 717   score: 0.0   memory length: 130869   epsilon: 0.9388774000013269    steps: 122    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 718   score: 0.0   memory length: 130991   epsilon: 0.9386358400013322    steps: 122    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 719   score: 4.0   memory length: 131271   epsilon: 0.9380814400013442    steps: 280    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 720   score: 0.0   memory length: 131393   epsilon: 0.9378398800013494    steps: 122    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 721   score: 2.0   memory length: 131609   epsilon: 0.9374122000013587    steps: 216    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 722   score: 3.0   memory length: 131853   epsilon: 0.9369290800013692    steps: 244    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 723   score: 2.0   memory length: 132050   epsilon: 0.9365390200013777    steps: 197    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 724   score: 0.0   memory length: 132172   epsilon: 0.9362974600013829    steps: 122    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 725   score: 1.0   memory length: 132341   epsilon: 0.9359628400013902    steps: 169    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 726   score: 3.0   memory length: 132608   epsilon: 0.9354341800014017    steps: 267    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 727   score: 2.0   memory length: 132805   epsilon: 0.9350441200014101    steps: 197    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 728   score: 2.0   memory length: 133002   epsilon: 0.9346540600014186    steps: 197    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 729   score: 1.0   memory length: 133153   epsilon: 0.9343550800014251    steps: 151    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 730   score: 0.0   memory length: 133276   epsilon: 0.9341115400014304    steps: 123    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 731   score: 0.0   memory length: 133399   epsilon: 0.9338680000014357    steps: 123    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 732   score: 3.0   memory length: 133650   epsilon: 0.9333710200014464    steps: 251    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 733   score: 3.0   memory length: 133898   epsilon: 0.9328799800014571    steps: 248    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 734   score: 0.0   memory length: 134021   epsilon: 0.9326364400014624    steps: 123    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 735   score: 2.0   memory length: 134219   epsilon: 0.9322444000014709    steps: 198    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 736   score: 0.0   memory length: 134342   epsilon: 0.9320008600014762    steps: 123    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 737   score: 2.0   memory length: 134540   epsilon: 0.9316088200014847    steps: 198    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 738   score: 1.0   memory length: 134690   epsilon: 0.9313118200014912    steps: 150    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 739   score: 0.0   memory length: 134813   epsilon: 0.9310682800014964    steps: 123    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 740   score: 1.0   memory length: 134964   epsilon: 0.9307693000015029    steps: 151    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 741   score: 1.0   memory length: 135133   epsilon: 0.9304346800015102    steps: 169    lr: 0.0001     evaluation reward: 1.38\n",
            "episode: 742   score: 2.0   memory length: 135330   epsilon: 0.9300446200015187    steps: 197    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 743   score: 1.0   memory length: 135500   epsilon: 0.929708020001526    steps: 170    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 744   score: 2.0   memory length: 135698   epsilon: 0.9293159800015345    steps: 198    lr: 0.0001     evaluation reward: 1.39\n",
            "episode: 745   score: 3.0   memory length: 135928   epsilon: 0.9288605800015444    steps: 230    lr: 0.0001     evaluation reward: 1.39\n",
            "episode: 746   score: 1.0   memory length: 136079   epsilon: 0.9285616000015509    steps: 151    lr: 0.0001     evaluation reward: 1.39\n",
            "episode: 747   score: 1.0   memory length: 136230   epsilon: 0.9282626200015573    steps: 151    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 748   score: 2.0   memory length: 136428   epsilon: 0.9278705800015659    steps: 198    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 749   score: 1.0   memory length: 136600   epsilon: 0.9275300200015733    steps: 172    lr: 0.0001     evaluation reward: 1.39\n",
            "episode: 750   score: 0.0   memory length: 136723   epsilon: 0.9272864800015785    steps: 123    lr: 0.0001     evaluation reward: 1.39\n",
            "episode: 751   score: 0.0   memory length: 136845   epsilon: 0.9270449200015838    steps: 122    lr: 0.0001     evaluation reward: 1.39\n",
            "episode: 752   score: 4.0   memory length: 137156   epsilon: 0.9264291400015972    steps: 311    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 753   score: 0.0   memory length: 137278   epsilon: 0.9261875800016024    steps: 122    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 754   score: 1.0   memory length: 137447   epsilon: 0.9258529600016097    steps: 169    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 755   score: 2.0   memory length: 137647   epsilon: 0.9254569600016183    steps: 200    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 756   score: 0.0   memory length: 137770   epsilon: 0.9252134200016235    steps: 123    lr: 0.0001     evaluation reward: 1.39\n",
            "episode: 757   score: 3.0   memory length: 138015   epsilon: 0.9247283200016341    steps: 245    lr: 0.0001     evaluation reward: 1.39\n",
            "episode: 758   score: 1.0   memory length: 138166   epsilon: 0.9244293400016406    steps: 151    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 759   score: 1.0   memory length: 138334   epsilon: 0.9240967000016478    steps: 168    lr: 0.0001     evaluation reward: 1.38\n",
            "episode: 760   score: 1.0   memory length: 138504   epsilon: 0.9237601000016551    steps: 170    lr: 0.0001     evaluation reward: 1.39\n",
            "episode: 761   score: 4.0   memory length: 138800   epsilon: 0.9231740200016678    steps: 296    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 762   score: 2.0   memory length: 139016   epsilon: 0.9227463400016771    steps: 216    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 763   score: 5.0   memory length: 139339   epsilon: 0.922106800001691    steps: 323    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 764   score: 3.0   memory length: 139566   epsilon: 0.9216573400017007    steps: 227    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 765   score: 2.0   memory length: 139764   epsilon: 0.9212653000017093    steps: 198    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 766   score: 3.0   memory length: 140010   epsilon: 0.9207782200017198    steps: 246    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 767   score: 1.0   memory length: 140179   epsilon: 0.9204436000017271    steps: 169    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 768   score: 0.0   memory length: 140302   epsilon: 0.9202000600017324    steps: 123    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 769   score: 2.0   memory length: 140500   epsilon: 0.9198080200017409    steps: 198    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 770   score: 2.0   memory length: 140698   epsilon: 0.9194159800017494    steps: 198    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 771   score: 0.0   memory length: 140821   epsilon: 0.9191724400017547    steps: 123    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 772   score: 0.0   memory length: 140943   epsilon: 0.9189308800017599    steps: 122    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 773   score: 1.0   memory length: 141114   epsilon: 0.9185923000017673    steps: 171    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 774   score: 0.0   memory length: 141237   epsilon: 0.9183487600017726    steps: 123    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 775   score: 3.0   memory length: 141503   epsilon: 0.917822080001784    steps: 266    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 776   score: 2.0   memory length: 141718   epsilon: 0.9173963800017932    steps: 215    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 777   score: 2.0   memory length: 141898   epsilon: 0.917039980001801    steps: 180    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 778   score: 3.0   memory length: 142144   epsilon: 0.9165529000018116    steps: 246    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 779   score: 1.0   memory length: 142295   epsilon: 0.916253920001818    steps: 151    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 780   score: 3.0   memory length: 142543   epsilon: 0.9157628800018287    steps: 248    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 781   score: 2.0   memory length: 142759   epsilon: 0.915335200001838    steps: 216    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 782   score: 1.0   memory length: 142910   epsilon: 0.9150362200018445    steps: 151    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 783   score: 0.0   memory length: 143033   epsilon: 0.9147926800018498    steps: 123    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 784   score: 0.0   memory length: 143156   epsilon: 0.914549140001855    steps: 123    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 785   score: 0.0   memory length: 143278   epsilon: 0.9143075800018603    steps: 122    lr: 0.0001     evaluation reward: 1.38\n",
            "episode: 786   score: 0.0   memory length: 143401   epsilon: 0.9140640400018656    steps: 123    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 787   score: 1.0   memory length: 143552   epsilon: 0.9137650600018721    steps: 151    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 788   score: 1.0   memory length: 143702   epsilon: 0.9134680600018785    steps: 150    lr: 0.0001     evaluation reward: 1.38\n",
            "episode: 789   score: 1.0   memory length: 143853   epsilon: 0.913169080001885    steps: 151    lr: 0.0001     evaluation reward: 1.38\n",
            "episode: 790   score: 0.0   memory length: 143976   epsilon: 0.9129255400018903    steps: 123    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 791   score: 0.0   memory length: 144099   epsilon: 0.9126820000018956    steps: 123    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 792   score: 4.0   memory length: 144397   epsilon: 0.9120919600019084    steps: 298    lr: 0.0001     evaluation reward: 1.38\n",
            "episode: 793   score: 1.0   memory length: 144566   epsilon: 0.9117573400019157    steps: 169    lr: 0.0001     evaluation reward: 1.38\n",
            "episode: 794   score: 1.0   memory length: 144718   epsilon: 0.9114563800019222    steps: 152    lr: 0.0001     evaluation reward: 1.38\n",
            "episode: 795   score: 0.0   memory length: 144840   epsilon: 0.9112148200019274    steps: 122    lr: 0.0001     evaluation reward: 1.38\n",
            "episode: 796   score: 1.0   memory length: 145011   epsilon: 0.9108762400019348    steps: 171    lr: 0.0001     evaluation reward: 1.39\n",
            "episode: 797   score: 0.0   memory length: 145134   epsilon: 0.9106327000019401    steps: 123    lr: 0.0001     evaluation reward: 1.38\n",
            "episode: 798   score: 2.0   memory length: 145315   epsilon: 0.9102743200019479    steps: 181    lr: 0.0001     evaluation reward: 1.38\n",
            "episode: 799   score: 2.0   memory length: 145513   epsilon: 0.9098822800019564    steps: 198    lr: 0.0001     evaluation reward: 1.39\n",
            "episode: 800   score: 4.0   memory length: 145773   epsilon: 0.9093674800019675    steps: 260    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 801   score: 1.0   memory length: 145943   epsilon: 0.9090308800019749    steps: 170    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 802   score: 0.0   memory length: 146065   epsilon: 0.9087893200019801    steps: 122    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 803   score: 1.0   memory length: 146234   epsilon: 0.9084547000019874    steps: 169    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 804   score: 0.0   memory length: 146357   epsilon: 0.9082111600019926    steps: 123    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 805   score: 3.0   memory length: 146604   epsilon: 0.9077221000020033    steps: 247    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 806   score: 2.0   memory length: 146824   epsilon: 0.9072865000020127    steps: 220    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 807   score: 0.0   memory length: 146946   epsilon: 0.907044940002018    steps: 122    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 808   score: 0.0   memory length: 147069   epsilon: 0.9068014000020233    steps: 123    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 809   score: 2.0   memory length: 147253   epsilon: 0.9064370800020312    steps: 184    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 810   score: 0.0   memory length: 147376   epsilon: 0.9061935400020364    steps: 123    lr: 0.0001     evaluation reward: 1.39\n",
            "episode: 811   score: 3.0   memory length: 147602   epsilon: 0.9057460600020462    steps: 226    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 812   score: 1.0   memory length: 147771   epsilon: 0.9054114400020534    steps: 169    lr: 0.0001     evaluation reward: 1.39\n",
            "episode: 813   score: 0.0   memory length: 147894   epsilon: 0.9051679000020587    steps: 123    lr: 0.0001     evaluation reward: 1.38\n",
            "episode: 814   score: 4.0   memory length: 148172   epsilon: 0.9046174600020707    steps: 278    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 815   score: 2.0   memory length: 148370   epsilon: 0.9042254200020792    steps: 198    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 816   score: 3.0   memory length: 148634   epsilon: 0.9037027000020905    steps: 264    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 817   score: 6.0   memory length: 148959   epsilon: 0.9030592000021045    steps: 325    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 818   score: 3.0   memory length: 149207   epsilon: 0.9025681600021151    steps: 248    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 819   score: 1.0   memory length: 149357   epsilon: 0.9022711600021216    steps: 150    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 820   score: 3.0   memory length: 149619   epsilon: 0.9017524000021329    steps: 262    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 821   score: 1.0   memory length: 149788   epsilon: 0.9014177800021401    steps: 169    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 822   score: 2.0   memory length: 149985   epsilon: 0.9010277200021486    steps: 197    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 823   score: 0.0   memory length: 150107   epsilon: 0.9007861600021538    steps: 122    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 824   score: 3.0   memory length: 150374   epsilon: 0.9002575000021653    steps: 267    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 825   score: 1.0   memory length: 150543   epsilon: 0.8999228800021726    steps: 169    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 826   score: 1.0   memory length: 150712   epsilon: 0.8995882600021798    steps: 169    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 827   score: 5.0   memory length: 151042   epsilon: 0.898934860002194    steps: 330    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 828   score: 2.0   memory length: 151259   epsilon: 0.8985052000022034    steps: 217    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 829   score: 2.0   memory length: 151457   epsilon: 0.8981131600022119    steps: 198    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 830   score: 1.0   memory length: 151628   epsilon: 0.8977745800022192    steps: 171    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 831   score: 0.0   memory length: 151751   epsilon: 0.8975310400022245    steps: 123    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 832   score: 4.0   memory length: 152065   epsilon: 0.896909320002238    steps: 314    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 833   score: 1.0   memory length: 152234   epsilon: 0.8965747000022453    steps: 169    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 834   score: 0.0   memory length: 152356   epsilon: 0.8963331400022505    steps: 122    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 835   score: 0.0   memory length: 152478   epsilon: 0.8960915800022557    steps: 122    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 836   score: 3.0   memory length: 152723   epsilon: 0.8956064800022663    steps: 245    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 837   score: 2.0   memory length: 152921   epsilon: 0.8952144400022748    steps: 198    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 838   score: 2.0   memory length: 153137   epsilon: 0.8947867600022841    steps: 216    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 839   score: 4.0   memory length: 153412   epsilon: 0.8942422600022959    steps: 275    lr: 0.0001     evaluation reward: 1.57\n",
            "episode: 840   score: 1.0   memory length: 153562   epsilon: 0.8939452600023023    steps: 150    lr: 0.0001     evaluation reward: 1.57\n",
            "episode: 841   score: 2.0   memory length: 153761   epsilon: 0.8935512400023109    steps: 199    lr: 0.0001     evaluation reward: 1.58\n",
            "episode: 842   score: 1.0   memory length: 153932   epsilon: 0.8932126600023182    steps: 171    lr: 0.0001     evaluation reward: 1.57\n",
            "episode: 843   score: 3.0   memory length: 154161   epsilon: 0.8927592400023281    steps: 229    lr: 0.0001     evaluation reward: 1.59\n",
            "episode: 844   score: 2.0   memory length: 154359   epsilon: 0.8923672000023366    steps: 198    lr: 0.0001     evaluation reward: 1.59\n",
            "episode: 845   score: 3.0   memory length: 154603   epsilon: 0.8918840800023471    steps: 244    lr: 0.0001     evaluation reward: 1.59\n",
            "episode: 846   score: 4.0   memory length: 154890   epsilon: 0.8913158200023594    steps: 287    lr: 0.0001     evaluation reward: 1.62\n",
            "episode: 847   score: 6.0   memory length: 155283   epsilon: 0.8905376800023763    steps: 393    lr: 0.0001     evaluation reward: 1.67\n",
            "episode: 848   score: 2.0   memory length: 155501   epsilon: 0.8901060400023857    steps: 218    lr: 0.0001     evaluation reward: 1.67\n",
            "episode: 849   score: 0.0   memory length: 155624   epsilon: 0.889862500002391    steps: 123    lr: 0.0001     evaluation reward: 1.66\n",
            "episode: 850   score: 5.0   memory length: 155988   epsilon: 0.8891417800024066    steps: 364    lr: 0.0001     evaluation reward: 1.71\n",
            "episode: 851   score: 3.0   memory length: 156201   epsilon: 0.8887200400024158    steps: 213    lr: 0.0001     evaluation reward: 1.74\n",
            "episode: 852   score: 2.0   memory length: 156401   epsilon: 0.8883240400024244    steps: 200    lr: 0.0001     evaluation reward: 1.72\n",
            "episode: 853   score: 0.0   memory length: 156524   epsilon: 0.8880805000024297    steps: 123    lr: 0.0001     evaluation reward: 1.72\n",
            "episode: 854   score: 4.0   memory length: 156802   epsilon: 0.8875300600024416    steps: 278    lr: 0.0001     evaluation reward: 1.75\n",
            "episode: 855   score: 4.0   memory length: 157099   epsilon: 0.8869420000024544    steps: 297    lr: 0.0001     evaluation reward: 1.77\n",
            "episode: 856   score: 1.0   memory length: 157250   epsilon: 0.8866430200024609    steps: 151    lr: 0.0001     evaluation reward: 1.78\n",
            "episode: 857   score: 0.0   memory length: 157372   epsilon: 0.8864014600024661    steps: 122    lr: 0.0001     evaluation reward: 1.75\n",
            "episode: 858   score: 0.0   memory length: 157494   epsilon: 0.8861599000024714    steps: 122    lr: 0.0001     evaluation reward: 1.74\n",
            "episode: 859   score: 4.0   memory length: 157773   epsilon: 0.8856074800024833    steps: 279    lr: 0.0001     evaluation reward: 1.77\n",
            "episode: 860   score: 2.0   memory length: 157972   epsilon: 0.8852134600024919    steps: 199    lr: 0.0001     evaluation reward: 1.78\n",
            "episode: 861   score: 2.0   memory length: 158191   epsilon: 0.8847798400025013    steps: 219    lr: 0.0001     evaluation reward: 1.76\n",
            "episode: 862   score: 0.0   memory length: 158314   epsilon: 0.8845363000025066    steps: 123    lr: 0.0001     evaluation reward: 1.74\n",
            "episode: 863   score: 0.0   memory length: 158436   epsilon: 0.8842947400025118    steps: 122    lr: 0.0001     evaluation reward: 1.69\n",
            "episode: 864   score: 3.0   memory length: 158683   epsilon: 0.8838056800025225    steps: 247    lr: 0.0001     evaluation reward: 1.69\n",
            "episode: 865   score: 4.0   memory length: 158997   epsilon: 0.883183960002536    steps: 314    lr: 0.0001     evaluation reward: 1.71\n",
            "episode: 866   score: 0.0   memory length: 159120   epsilon: 0.8829404200025412    steps: 123    lr: 0.0001     evaluation reward: 1.68\n",
            "episode: 867   score: 1.0   memory length: 159290   epsilon: 0.8826038200025486    steps: 170    lr: 0.0001     evaluation reward: 1.68\n",
            "episode: 868   score: 0.0   memory length: 159413   epsilon: 0.8823602800025538    steps: 123    lr: 0.0001     evaluation reward: 1.68\n",
            "episode: 869   score: 3.0   memory length: 159638   epsilon: 0.8819147800025635    steps: 225    lr: 0.0001     evaluation reward: 1.69\n",
            "episode: 870   score: 4.0   memory length: 159935   epsilon: 0.8813267200025763    steps: 297    lr: 0.0001     evaluation reward: 1.71\n",
            "episode: 871   score: 1.0   memory length: 160086   epsilon: 0.8810277400025828    steps: 151    lr: 0.0001     evaluation reward: 1.72\n",
            "episode: 872   score: 2.0   memory length: 160301   epsilon: 0.880602040002592    steps: 215    lr: 0.0001     evaluation reward: 1.74\n",
            "episode: 873   score: 3.0   memory length: 160571   epsilon: 0.8800674400026036    steps: 270    lr: 0.0001     evaluation reward: 1.76\n",
            "episode: 874   score: 5.0   memory length: 160886   epsilon: 0.8794437400026172    steps: 315    lr: 0.0001     evaluation reward: 1.81\n",
            "episode: 875   score: 3.0   memory length: 161155   epsilon: 0.8789111200026287    steps: 269    lr: 0.0001     evaluation reward: 1.81\n",
            "episode: 876   score: 1.0   memory length: 161325   epsilon: 0.878574520002636    steps: 170    lr: 0.0001     evaluation reward: 1.8\n",
            "episode: 877   score: 1.0   memory length: 161476   epsilon: 0.8782755400026425    steps: 151    lr: 0.0001     evaluation reward: 1.79\n",
            "episode: 878   score: 1.0   memory length: 161627   epsilon: 0.877976560002649    steps: 151    lr: 0.0001     evaluation reward: 1.77\n",
            "episode: 879   score: 2.0   memory length: 161824   epsilon: 0.8775865000026575    steps: 197    lr: 0.0001     evaluation reward: 1.78\n",
            "episode: 880   score: 2.0   memory length: 162024   epsilon: 0.8771905000026661    steps: 200    lr: 0.0001     evaluation reward: 1.77\n",
            "episode: 881   score: 4.0   memory length: 162299   epsilon: 0.8766460000026779    steps: 275    lr: 0.0001     evaluation reward: 1.79\n",
            "episode: 882   score: 2.0   memory length: 162517   epsilon: 0.8762143600026873    steps: 218    lr: 0.0001     evaluation reward: 1.8\n",
            "episode: 883   score: 2.0   memory length: 162698   epsilon: 0.875855980002695    steps: 181    lr: 0.0001     evaluation reward: 1.82\n",
            "episode: 884   score: 1.0   memory length: 162866   epsilon: 0.8755233400027023    steps: 168    lr: 0.0001     evaluation reward: 1.83\n",
            "episode: 885   score: 3.0   memory length: 163110   epsilon: 0.8750402200027128    steps: 244    lr: 0.0001     evaluation reward: 1.86\n",
            "episode: 886   score: 0.0   memory length: 163233   epsilon: 0.874796680002718    steps: 123    lr: 0.0001     evaluation reward: 1.86\n",
            "episode: 887   score: 2.0   memory length: 163431   epsilon: 0.8744046400027266    steps: 198    lr: 0.0001     evaluation reward: 1.87\n",
            "episode: 888   score: 1.0   memory length: 163581   epsilon: 0.874107640002733    steps: 150    lr: 0.0001     evaluation reward: 1.87\n",
            "episode: 889   score: 3.0   memory length: 163828   epsilon: 0.8736185800027436    steps: 247    lr: 0.0001     evaluation reward: 1.89\n",
            "episode: 890   score: 2.0   memory length: 164007   epsilon: 0.8732641600027513    steps: 179    lr: 0.0001     evaluation reward: 1.91\n",
            "episode: 891   score: 2.0   memory length: 164205   epsilon: 0.8728721200027598    steps: 198    lr: 0.0001     evaluation reward: 1.93\n",
            "episode: 892   score: 3.0   memory length: 164436   epsilon: 0.8724147400027698    steps: 231    lr: 0.0001     evaluation reward: 1.92\n",
            "episode: 893   score: 2.0   memory length: 164638   epsilon: 0.8720147800027784    steps: 202    lr: 0.0001     evaluation reward: 1.93\n",
            "episode: 894   score: 1.0   memory length: 164809   epsilon: 0.8716762000027858    steps: 171    lr: 0.0001     evaluation reward: 1.93\n",
            "episode: 895   score: 4.0   memory length: 165078   epsilon: 0.8711435800027973    steps: 269    lr: 0.0001     evaluation reward: 1.97\n",
            "episode: 896   score: 3.0   memory length: 165344   epsilon: 0.8706169000028088    steps: 266    lr: 0.0001     evaluation reward: 1.99\n",
            "episode: 897   score: 1.0   memory length: 165494   epsilon: 0.8703199000028152    steps: 150    lr: 0.0001     evaluation reward: 2.0\n",
            "episode: 898   score: 3.0   memory length: 165764   epsilon: 0.8697853000028268    steps: 270    lr: 0.0001     evaluation reward: 2.01\n",
            "episode: 899   score: 3.0   memory length: 165990   epsilon: 0.8693378200028365    steps: 226    lr: 0.0001     evaluation reward: 2.02\n",
            "episode: 900   score: 5.0   memory length: 166318   epsilon: 0.8686883800028506    steps: 328    lr: 0.0001     evaluation reward: 2.03\n",
            "episode: 901   score: 0.0   memory length: 166440   epsilon: 0.8684468200028559    steps: 122    lr: 0.0001     evaluation reward: 2.02\n",
            "episode: 902   score: 4.0   memory length: 166718   epsilon: 0.8678963800028678    steps: 278    lr: 0.0001     evaluation reward: 2.06\n",
            "episode: 903   score: 0.0   memory length: 166840   epsilon: 0.8676548200028731    steps: 122    lr: 0.0001     evaluation reward: 2.05\n",
            "episode: 904   score: 4.0   memory length: 167113   epsilon: 0.8671142800028848    steps: 273    lr: 0.0001     evaluation reward: 2.09\n",
            "episode: 905   score: 1.0   memory length: 167283   epsilon: 0.8667776800028921    steps: 170    lr: 0.0001     evaluation reward: 2.07\n",
            "episode: 906   score: 1.0   memory length: 167434   epsilon: 0.8664787000028986    steps: 151    lr: 0.0001     evaluation reward: 2.06\n",
            "episode: 907   score: 3.0   memory length: 167681   epsilon: 0.8659896400029092    steps: 247    lr: 0.0001     evaluation reward: 2.09\n",
            "episode: 908   score: 1.0   memory length: 167832   epsilon: 0.8656906600029157    steps: 151    lr: 0.0001     evaluation reward: 2.1\n",
            "episode: 909   score: 3.0   memory length: 168060   epsilon: 0.8652392200029255    steps: 228    lr: 0.0001     evaluation reward: 2.11\n",
            "episode: 910   score: 4.0   memory length: 168358   epsilon: 0.8646491800029383    steps: 298    lr: 0.0001     evaluation reward: 2.15\n",
            "episode: 911   score: 2.0   memory length: 168576   epsilon: 0.8642175400029477    steps: 218    lr: 0.0001     evaluation reward: 2.14\n",
            "episode: 912   score: 2.0   memory length: 168791   epsilon: 0.863791840002957    steps: 215    lr: 0.0001     evaluation reward: 2.15\n",
            "episode: 913   score: 3.0   memory length: 169039   epsilon: 0.8633008000029676    steps: 248    lr: 0.0001     evaluation reward: 2.18\n",
            "episode: 914   score: 1.0   memory length: 169190   epsilon: 0.8630018200029741    steps: 151    lr: 0.0001     evaluation reward: 2.15\n",
            "episode: 915   score: 2.0   memory length: 169387   epsilon: 0.8626117600029826    steps: 197    lr: 0.0001     evaluation reward: 2.15\n",
            "episode: 916   score: 3.0   memory length: 169613   epsilon: 0.8621642800029923    steps: 226    lr: 0.0001     evaluation reward: 2.15\n",
            "episode: 917   score: 0.0   memory length: 169736   epsilon: 0.8619207400029976    steps: 123    lr: 0.0001     evaluation reward: 2.09\n",
            "episode: 918   score: 2.0   memory length: 169954   epsilon: 0.8614891000030069    steps: 218    lr: 0.0001     evaluation reward: 2.08\n",
            "episode: 919   score: 5.0   memory length: 170262   epsilon: 0.8608792600030202    steps: 308    lr: 0.0001     evaluation reward: 2.12\n",
            "episode: 920   score: 4.0   memory length: 170556   epsilon: 0.8602971400030328    steps: 294    lr: 0.0001     evaluation reward: 2.13\n",
            "episode: 921   score: 3.0   memory length: 170802   epsilon: 0.8598100600030434    steps: 246    lr: 0.0001     evaluation reward: 2.15\n",
            "episode: 922   score: 1.0   memory length: 170953   epsilon: 0.8595110800030499    steps: 151    lr: 0.0001     evaluation reward: 2.14\n",
            "episode: 923   score: 2.0   memory length: 171155   epsilon: 0.8591111200030586    steps: 202    lr: 0.0001     evaluation reward: 2.16\n",
            "episode: 924   score: 1.0   memory length: 171327   epsilon: 0.858770560003066    steps: 172    lr: 0.0001     evaluation reward: 2.14\n",
            "episode: 925   score: 3.0   memory length: 171556   epsilon: 0.8583171400030758    steps: 229    lr: 0.0001     evaluation reward: 2.16\n",
            "episode: 926   score: 3.0   memory length: 171802   epsilon: 0.8578300600030864    steps: 246    lr: 0.0001     evaluation reward: 2.18\n",
            "episode: 927   score: 1.0   memory length: 171973   epsilon: 0.8574914800030937    steps: 171    lr: 0.0001     evaluation reward: 2.14\n",
            "episode: 928   score: 1.0   memory length: 172144   epsilon: 0.8571529000031011    steps: 171    lr: 0.0001     evaluation reward: 2.13\n",
            "episode: 929   score: 1.0   memory length: 172314   epsilon: 0.8568163000031084    steps: 170    lr: 0.0001     evaluation reward: 2.12\n",
            "episode: 930   score: 0.0   memory length: 172437   epsilon: 0.8565727600031137    steps: 123    lr: 0.0001     evaluation reward: 2.11\n",
            "episode: 931   score: 3.0   memory length: 172663   epsilon: 0.8561252800031234    steps: 226    lr: 0.0001     evaluation reward: 2.14\n",
            "episode: 932   score: 3.0   memory length: 172907   epsilon: 0.8556421600031339    steps: 244    lr: 0.0001     evaluation reward: 2.13\n",
            "episode: 933   score: 4.0   memory length: 173186   epsilon: 0.8550897400031459    steps: 279    lr: 0.0001     evaluation reward: 2.16\n",
            "episode: 934   score: 2.0   memory length: 173384   epsilon: 0.8546977000031544    steps: 198    lr: 0.0001     evaluation reward: 2.18\n",
            "episode: 935   score: 4.0   memory length: 173695   epsilon: 0.8540819200031677    steps: 311    lr: 0.0001     evaluation reward: 2.22\n",
            "episode: 936   score: 3.0   memory length: 173942   epsilon: 0.8535928600031784    steps: 247    lr: 0.0001     evaluation reward: 2.22\n",
            "episode: 937   score: 6.0   memory length: 174297   epsilon: 0.8528899600031936    steps: 355    lr: 0.0001     evaluation reward: 2.26\n",
            "episode: 938   score: 1.0   memory length: 174465   epsilon: 0.8525573200032008    steps: 168    lr: 0.0001     evaluation reward: 2.25\n",
            "episode: 939   score: 0.0   memory length: 174588   epsilon: 0.8523137800032061    steps: 123    lr: 0.0001     evaluation reward: 2.21\n",
            "episode: 940   score: 2.0   memory length: 174811   epsilon: 0.8518722400032157    steps: 223    lr: 0.0001     evaluation reward: 2.22\n",
            "episode: 941   score: 4.0   memory length: 175107   epsilon: 0.8512861600032284    steps: 296    lr: 0.0001     evaluation reward: 2.24\n",
            "episode: 942   score: 0.0   memory length: 175230   epsilon: 0.8510426200032337    steps: 123    lr: 0.0001     evaluation reward: 2.23\n",
            "episode: 943   score: 3.0   memory length: 175461   epsilon: 0.8505852400032436    steps: 231    lr: 0.0001     evaluation reward: 2.23\n",
            "episode: 944   score: 0.0   memory length: 175584   epsilon: 0.8503417000032489    steps: 123    lr: 0.0001     evaluation reward: 2.21\n",
            "episode: 945   score: 4.0   memory length: 175882   epsilon: 0.8497516600032617    steps: 298    lr: 0.0001     evaluation reward: 2.22\n",
            "episode: 946   score: 5.0   memory length: 176184   epsilon: 0.8491537000032747    steps: 302    lr: 0.0001     evaluation reward: 2.23\n",
            "episode: 947   score: 4.0   memory length: 176500   epsilon: 0.8485280200032883    steps: 316    lr: 0.0001     evaluation reward: 2.21\n",
            "episode: 948   score: 5.0   memory length: 176815   epsilon: 0.8479043200033018    steps: 315    lr: 0.0001     evaluation reward: 2.24\n",
            "episode: 949   score: 2.0   memory length: 176996   epsilon: 0.8475459400033096    steps: 181    lr: 0.0001     evaluation reward: 2.26\n",
            "episode: 950   score: 5.0   memory length: 177320   epsilon: 0.8469044200033236    steps: 324    lr: 0.0001     evaluation reward: 2.26\n",
            "episode: 951   score: 0.0   memory length: 177443   epsilon: 0.8466608800033288    steps: 123    lr: 0.0001     evaluation reward: 2.23\n",
            "episode: 952   score: 2.0   memory length: 177641   epsilon: 0.8462688400033374    steps: 198    lr: 0.0001     evaluation reward: 2.23\n",
            "episode: 953   score: 2.0   memory length: 177859   epsilon: 0.8458372000033467    steps: 218    lr: 0.0001     evaluation reward: 2.25\n",
            "episode: 954   score: 3.0   memory length: 178110   epsilon: 0.8453402200033575    steps: 251    lr: 0.0001     evaluation reward: 2.24\n",
            "episode: 955   score: 3.0   memory length: 178358   epsilon: 0.8448491800033682    steps: 248    lr: 0.0001     evaluation reward: 2.23\n",
            "episode: 956   score: 3.0   memory length: 178586   epsilon: 0.844397740003378    steps: 228    lr: 0.0001     evaluation reward: 2.25\n",
            "episode: 957   score: 2.0   memory length: 178784   epsilon: 0.8440057000033865    steps: 198    lr: 0.0001     evaluation reward: 2.27\n",
            "episode: 958   score: 1.0   memory length: 178955   epsilon: 0.8436671200033938    steps: 171    lr: 0.0001     evaluation reward: 2.28\n",
            "episode: 959   score: 2.0   memory length: 179156   epsilon: 0.8432691400034025    steps: 201    lr: 0.0001     evaluation reward: 2.26\n",
            "episode: 960   score: 0.0   memory length: 179278   epsilon: 0.8430275800034077    steps: 122    lr: 0.0001     evaluation reward: 2.24\n",
            "episode: 961   score: 3.0   memory length: 179526   epsilon: 0.8425365400034184    steps: 248    lr: 0.0001     evaluation reward: 2.25\n",
            "episode: 962   score: 6.0   memory length: 179778   epsilon: 0.8420375800034292    steps: 252    lr: 0.0001     evaluation reward: 2.31\n",
            "episode: 963   score: 1.0   memory length: 179930   epsilon: 0.8417366200034357    steps: 152    lr: 0.0001     evaluation reward: 2.32\n",
            "episode: 964   score: 1.0   memory length: 180102   epsilon: 0.8413960600034431    steps: 172    lr: 0.0001     evaluation reward: 2.3\n",
            "episode: 965   score: 3.0   memory length: 180353   epsilon: 0.8408990800034539    steps: 251    lr: 0.0001     evaluation reward: 2.29\n",
            "episode: 966   score: 2.0   memory length: 180551   epsilon: 0.8405070400034624    steps: 198    lr: 0.0001     evaluation reward: 2.31\n",
            "episode: 967   score: 0.0   memory length: 180674   epsilon: 0.8402635000034677    steps: 123    lr: 0.0001     evaluation reward: 2.3\n",
            "episode: 968   score: 1.0   memory length: 180843   epsilon: 0.839928880003475    steps: 169    lr: 0.0001     evaluation reward: 2.31\n",
            "episode: 969   score: 3.0   memory length: 181092   epsilon: 0.8394358600034857    steps: 249    lr: 0.0001     evaluation reward: 2.31\n",
            "episode: 970   score: 4.0   memory length: 181367   epsilon: 0.8388913600034975    steps: 275    lr: 0.0001     evaluation reward: 2.31\n",
            "episode: 971   score: 1.0   memory length: 181535   epsilon: 0.8385587200035047    steps: 168    lr: 0.0001     evaluation reward: 2.31\n",
            "episode: 972   score: 4.0   memory length: 181812   epsilon: 0.8380102600035166    steps: 277    lr: 0.0001     evaluation reward: 2.33\n",
            "episode: 973   score: 0.0   memory length: 181935   epsilon: 0.8377667200035219    steps: 123    lr: 0.0001     evaluation reward: 2.3\n",
            "episode: 974   score: 1.0   memory length: 182105   epsilon: 0.8374301200035292    steps: 170    lr: 0.0001     evaluation reward: 2.26\n",
            "episode: 975   score: 0.0   memory length: 182228   epsilon: 0.8371865800035345    steps: 123    lr: 0.0001     evaluation reward: 2.23\n",
            "episode: 976   score: 0.0   memory length: 182350   epsilon: 0.8369450200035398    steps: 122    lr: 0.0001     evaluation reward: 2.22\n",
            "episode: 977   score: 2.0   memory length: 182548   epsilon: 0.8365529800035483    steps: 198    lr: 0.0001     evaluation reward: 2.23\n",
            "episode: 978   score: 3.0   memory length: 182795   epsilon: 0.8360639200035589    steps: 247    lr: 0.0001     evaluation reward: 2.25\n",
            "episode: 979   score: 3.0   memory length: 183025   epsilon: 0.8356085200035688    steps: 230    lr: 0.0001     evaluation reward: 2.26\n",
            "episode: 980   score: 2.0   memory length: 183223   epsilon: 0.8352164800035773    steps: 198    lr: 0.0001     evaluation reward: 2.26\n",
            "episode: 981   score: 6.0   memory length: 183569   epsilon: 0.8345314000035922    steps: 346    lr: 0.0001     evaluation reward: 2.28\n",
            "episode: 982   score: 0.0   memory length: 183691   epsilon: 0.8342898400035974    steps: 122    lr: 0.0001     evaluation reward: 2.26\n",
            "episode: 983   score: 1.0   memory length: 183842   epsilon: 0.8339908600036039    steps: 151    lr: 0.0001     evaluation reward: 2.25\n",
            "episode: 984   score: 4.0   memory length: 184097   epsilon: 0.8334859600036149    steps: 255    lr: 0.0001     evaluation reward: 2.28\n",
            "episode: 985   score: 3.0   memory length: 184344   epsilon: 0.8329969000036255    steps: 247    lr: 0.0001     evaluation reward: 2.28\n",
            "episode: 986   score: 3.0   memory length: 184590   epsilon: 0.832509820003636    steps: 246    lr: 0.0001     evaluation reward: 2.31\n",
            "episode: 987   score: 2.0   memory length: 184808   epsilon: 0.8320781800036454    steps: 218    lr: 0.0001     evaluation reward: 2.31\n",
            "episode: 988   score: 5.0   memory length: 185134   epsilon: 0.8314327000036594    steps: 326    lr: 0.0001     evaluation reward: 2.35\n",
            "episode: 989   score: 0.0   memory length: 185257   epsilon: 0.8311891600036647    steps: 123    lr: 0.0001     evaluation reward: 2.32\n",
            "episode: 990   score: 4.0   memory length: 185556   epsilon: 0.8305971400036776    steps: 299    lr: 0.0001     evaluation reward: 2.34\n",
            "episode: 991   score: 1.0   memory length: 185707   epsilon: 0.8302981600036841    steps: 151    lr: 0.0001     evaluation reward: 2.33\n",
            "episode: 992   score: 2.0   memory length: 185904   epsilon: 0.8299081000036925    steps: 197    lr: 0.0001     evaluation reward: 2.32\n",
            "episode: 993   score: 0.0   memory length: 186027   epsilon: 0.8296645600036978    steps: 123    lr: 0.0001     evaluation reward: 2.3\n",
            "episode: 994   score: 4.0   memory length: 186325   epsilon: 0.8290745200037106    steps: 298    lr: 0.0001     evaluation reward: 2.33\n",
            "episode: 995   score: 4.0   memory length: 186600   epsilon: 0.8285300200037224    steps: 275    lr: 0.0001     evaluation reward: 2.33\n",
            "episode: 996   score: 0.0   memory length: 186723   epsilon: 0.8282864800037277    steps: 123    lr: 0.0001     evaluation reward: 2.3\n",
            "episode: 997   score: 4.0   memory length: 187021   epsilon: 0.8276964400037405    steps: 298    lr: 0.0001     evaluation reward: 2.33\n",
            "episode: 998   score: 3.0   memory length: 187265   epsilon: 0.827213320003751    steps: 244    lr: 0.0001     evaluation reward: 2.33\n",
            "episode: 999   score: 2.0   memory length: 187482   epsilon: 0.8267836600037604    steps: 217    lr: 0.0001     evaluation reward: 2.32\n",
            "episode: 1000   score: 2.0   memory length: 187664   epsilon: 0.8264233000037682    steps: 182    lr: 0.0001     evaluation reward: 2.29\n",
            "episode: 1001   score: 1.0   memory length: 187815   epsilon: 0.8261243200037747    steps: 151    lr: 0.0001     evaluation reward: 2.3\n",
            "episode: 1002   score: 1.0   memory length: 187984   epsilon: 0.8257897000037819    steps: 169    lr: 0.0001     evaluation reward: 2.27\n",
            "episode: 1003   score: 10.0   memory length: 188374   epsilon: 0.8250175000037987    steps: 390    lr: 0.0001     evaluation reward: 2.37\n",
            "episode: 1004   score: 1.0   memory length: 188544   epsilon: 0.824680900003806    steps: 170    lr: 0.0001     evaluation reward: 2.34\n",
            "episode: 1005   score: 2.0   memory length: 188766   epsilon: 0.8242413400038155    steps: 222    lr: 0.0001     evaluation reward: 2.35\n",
            "episode: 1006   score: 9.0   memory length: 189253   epsilon: 0.8232770800038365    steps: 487    lr: 0.0001     evaluation reward: 2.43\n",
            "episode: 1007   score: 1.0   memory length: 189424   epsilon: 0.8229385000038438    steps: 171    lr: 0.0001     evaluation reward: 2.41\n",
            "episode: 1008   score: 1.0   memory length: 189593   epsilon: 0.8226038800038511    steps: 169    lr: 0.0001     evaluation reward: 2.41\n",
            "episode: 1009   score: 3.0   memory length: 189822   epsilon: 0.8221504600038609    steps: 229    lr: 0.0001     evaluation reward: 2.41\n",
            "episode: 1010   score: 2.0   memory length: 190020   epsilon: 0.8217584200038694    steps: 198    lr: 0.0001     evaluation reward: 2.39\n",
            "episode: 1011   score: 3.0   memory length: 190267   epsilon: 0.8212693600038801    steps: 247    lr: 0.0001     evaluation reward: 2.4\n",
            "episode: 1012   score: 4.0   memory length: 190525   epsilon: 0.8207585200038912    steps: 258    lr: 0.0001     evaluation reward: 2.42\n",
            "episode: 1013   score: 4.0   memory length: 190840   epsilon: 0.8201348200039047    steps: 315    lr: 0.0001     evaluation reward: 2.43\n",
            "episode: 1014   score: 5.0   memory length: 191145   epsilon: 0.8195309200039178    steps: 305    lr: 0.0001     evaluation reward: 2.47\n",
            "episode: 1015   score: 2.0   memory length: 191345   epsilon: 0.8191349200039264    steps: 200    lr: 0.0001     evaluation reward: 2.47\n",
            "episode: 1016   score: 3.0   memory length: 191571   epsilon: 0.8186874400039361    steps: 226    lr: 0.0001     evaluation reward: 2.47\n",
            "episode: 1017   score: 4.0   memory length: 191887   epsilon: 0.8180617600039497    steps: 316    lr: 0.0001     evaluation reward: 2.51\n",
            "episode: 1018   score: 1.0   memory length: 192056   epsilon: 0.817727140003957    steps: 169    lr: 0.0001     evaluation reward: 2.5\n",
            "episode: 1019   score: 4.0   memory length: 192358   epsilon: 0.81712918000397    steps: 302    lr: 0.0001     evaluation reward: 2.49\n",
            "episode: 1020   score: 11.0   memory length: 192858   epsilon: 0.8161391800039914    steps: 500    lr: 0.0001     evaluation reward: 2.56\n",
            "episode: 1021   score: 0.0   memory length: 192980   epsilon: 0.8158976200039967    steps: 122    lr: 0.0001     evaluation reward: 2.53\n",
            "episode: 1022   score: 4.0   memory length: 193278   epsilon: 0.8153075800040095    steps: 298    lr: 0.0001     evaluation reward: 2.56\n",
            "episode: 1023   score: 1.0   memory length: 193428   epsilon: 0.8150105800040159    steps: 150    lr: 0.0001     evaluation reward: 2.55\n",
            "episode: 1024   score: 1.0   memory length: 193599   epsilon: 0.8146720000040233    steps: 171    lr: 0.0001     evaluation reward: 2.55\n",
            "episode: 1025   score: 3.0   memory length: 193848   epsilon: 0.814178980004034    steps: 249    lr: 0.0001     evaluation reward: 2.55\n",
            "episode: 1026   score: 1.0   memory length: 194017   epsilon: 0.8138443600040413    steps: 169    lr: 0.0001     evaluation reward: 2.53\n",
            "episode: 1027   score: 1.0   memory length: 194186   epsilon: 0.8135097400040485    steps: 169    lr: 0.0001     evaluation reward: 2.53\n",
            "episode: 1028   score: 0.0   memory length: 194309   epsilon: 0.8132662000040538    steps: 123    lr: 0.0001     evaluation reward: 2.52\n",
            "episode: 1029   score: 2.0   memory length: 194507   epsilon: 0.8128741600040623    steps: 198    lr: 0.0001     evaluation reward: 2.53\n",
            "episode: 1030   score: 1.0   memory length: 194658   epsilon: 0.8125751800040688    steps: 151    lr: 0.0001     evaluation reward: 2.54\n",
            "episode: 1031   score: 2.0   memory length: 194855   epsilon: 0.8121851200040773    steps: 197    lr: 0.0001     evaluation reward: 2.53\n",
            "episode: 1032   score: 2.0   memory length: 195054   epsilon: 0.8117911000040858    steps: 199    lr: 0.0001     evaluation reward: 2.52\n",
            "episode: 1033   score: 2.0   memory length: 195272   epsilon: 0.8113594600040952    steps: 218    lr: 0.0001     evaluation reward: 2.5\n",
            "episode: 1034   score: 5.0   memory length: 195579   epsilon: 0.8107516000041084    steps: 307    lr: 0.0001     evaluation reward: 2.53\n",
            "episode: 1035   score: 1.0   memory length: 195748   epsilon: 0.8104169800041157    steps: 169    lr: 0.0001     evaluation reward: 2.5\n",
            "episode: 1036   score: 3.0   memory length: 195975   epsilon: 0.8099675200041254    steps: 227    lr: 0.0001     evaluation reward: 2.5\n",
            "episode: 1037   score: 5.0   memory length: 196285   epsilon: 0.8093537200041387    steps: 310    lr: 0.0001     evaluation reward: 2.49\n",
            "episode: 1038   score: 4.0   memory length: 196543   epsilon: 0.8088428800041498    steps: 258    lr: 0.0001     evaluation reward: 2.52\n",
            "episode: 1039   score: 3.0   memory length: 196812   epsilon: 0.8083102600041614    steps: 269    lr: 0.0001     evaluation reward: 2.55\n",
            "episode: 1040   score: 4.0   memory length: 197105   epsilon: 0.807730120004174    steps: 293    lr: 0.0001     evaluation reward: 2.57\n",
            "episode: 1041   score: 4.0   memory length: 197386   epsilon: 0.8071737400041861    steps: 281    lr: 0.0001     evaluation reward: 2.57\n",
            "episode: 1042   score: 3.0   memory length: 197614   epsilon: 0.8067223000041959    steps: 228    lr: 0.0001     evaluation reward: 2.6\n",
            "episode: 1043   score: 3.0   memory length: 197863   epsilon: 0.8062292800042066    steps: 249    lr: 0.0001     evaluation reward: 2.6\n",
            "episode: 1044   score: 1.0   memory length: 198014   epsilon: 0.8059303000042131    steps: 151    lr: 0.0001     evaluation reward: 2.61\n",
            "episode: 1045   score: 2.0   memory length: 198213   epsilon: 0.8055362800042216    steps: 199    lr: 0.0001     evaluation reward: 2.59\n",
            "episode: 1046   score: 2.0   memory length: 198412   epsilon: 0.8051422600042302    steps: 199    lr: 0.0001     evaluation reward: 2.56\n",
            "episode: 1047   score: 3.0   memory length: 198656   epsilon: 0.8046591400042407    steps: 244    lr: 0.0001     evaluation reward: 2.55\n",
            "episode: 1048   score: 1.0   memory length: 198827   epsilon: 0.804320560004248    steps: 171    lr: 0.0001     evaluation reward: 2.51\n",
            "episode: 1049   score: 1.0   memory length: 198978   epsilon: 0.8040215800042545    steps: 151    lr: 0.0001     evaluation reward: 2.5\n",
            "episode: 1050   score: 2.0   memory length: 199176   epsilon: 0.803629540004263    steps: 198    lr: 0.0001     evaluation reward: 2.47\n",
            "episode: 1051   score: 2.0   memory length: 199356   epsilon: 0.8032731400042707    steps: 180    lr: 0.0001     evaluation reward: 2.49\n",
            "episode: 1052   score: 1.0   memory length: 199525   epsilon: 0.802938520004278    steps: 169    lr: 0.0001     evaluation reward: 2.48\n",
            "episode: 1053   score: 2.0   memory length: 199709   epsilon: 0.8025742000042859    steps: 184    lr: 0.0001     evaluation reward: 2.48\n",
            "episode: 1054   score: 2.0   memory length: 199928   epsilon: 0.8021405800042953    steps: 219    lr: 0.0001     evaluation reward: 2.47\n",
            "episode: 1055   score: 3.0   memory length: 200156   epsilon: 0.8016891400043051    steps: 228    lr: 4e-05     evaluation reward: 2.47\n",
            "episode: 1056   score: 4.0   memory length: 200432   epsilon: 0.801142660004317    steps: 276    lr: 4e-05     evaluation reward: 2.48\n",
            "episode: 1057   score: 1.0   memory length: 200583   epsilon: 0.8008436800043235    steps: 151    lr: 4e-05     evaluation reward: 2.47\n",
            "episode: 1058   score: 4.0   memory length: 200843   epsilon: 0.8003288800043347    steps: 260    lr: 4e-05     evaluation reward: 2.5\n",
            "episode: 1059   score: 1.0   memory length: 200994   epsilon: 0.8000299000043412    steps: 151    lr: 4e-05     evaluation reward: 2.49\n",
            "episode: 1060   score: 1.0   memory length: 201165   epsilon: 0.7996913200043485    steps: 171    lr: 4e-05     evaluation reward: 2.5\n",
            "episode: 1061   score: 2.0   memory length: 201363   epsilon: 0.799299280004357    steps: 198    lr: 4e-05     evaluation reward: 2.49\n",
            "episode: 1062   score: 4.0   memory length: 201641   epsilon: 0.798748840004369    steps: 278    lr: 4e-05     evaluation reward: 2.47\n",
            "episode: 1063   score: 5.0   memory length: 201945   epsilon: 0.798146920004382    steps: 304    lr: 4e-05     evaluation reward: 2.51\n",
            "episode: 1064   score: 4.0   memory length: 202220   epsilon: 0.7976024200043939    steps: 275    lr: 4e-05     evaluation reward: 2.54\n",
            "episode: 1065   score: 6.0   memory length: 202539   epsilon: 0.7969708000044076    steps: 319    lr: 4e-05     evaluation reward: 2.57\n",
            "episode: 1066   score: 4.0   memory length: 202806   epsilon: 0.796442140004419    steps: 267    lr: 4e-05     evaluation reward: 2.59\n",
            "episode: 1067   score: 3.0   memory length: 203016   epsilon: 0.7960263400044281    steps: 210    lr: 4e-05     evaluation reward: 2.62\n",
            "episode: 1068   score: 4.0   memory length: 203292   epsilon: 0.7954798600044399    steps: 276    lr: 4e-05     evaluation reward: 2.65\n",
            "episode: 1069   score: 0.0   memory length: 203415   epsilon: 0.7952363200044452    steps: 123    lr: 4e-05     evaluation reward: 2.62\n",
            "episode: 1070   score: 3.0   memory length: 203625   epsilon: 0.7948205200044542    steps: 210    lr: 4e-05     evaluation reward: 2.61\n",
            "episode: 1071   score: 2.0   memory length: 203823   epsilon: 0.7944284800044628    steps: 198    lr: 4e-05     evaluation reward: 2.62\n",
            "episode: 1072   score: 4.0   memory length: 204119   epsilon: 0.7938424000044755    steps: 296    lr: 4e-05     evaluation reward: 2.62\n",
            "episode: 1073   score: 3.0   memory length: 204365   epsilon: 0.793355320004486    steps: 246    lr: 4e-05     evaluation reward: 2.65\n",
            "episode: 1074   score: 1.0   memory length: 204535   epsilon: 0.7930187200044934    steps: 170    lr: 4e-05     evaluation reward: 2.65\n",
            "episode: 1075   score: 1.0   memory length: 204705   epsilon: 0.7926821200045007    steps: 170    lr: 4e-05     evaluation reward: 2.66\n",
            "episode: 1076   score: 3.0   memory length: 204953   epsilon: 0.7921910800045113    steps: 248    lr: 4e-05     evaluation reward: 2.69\n",
            "episode: 1077   score: 4.0   memory length: 205250   epsilon: 0.7916030200045241    steps: 297    lr: 4e-05     evaluation reward: 2.71\n",
            "episode: 1078   score: 1.0   memory length: 205401   epsilon: 0.7913040400045306    steps: 151    lr: 4e-05     evaluation reward: 2.69\n",
            "episode: 1079   score: 2.0   memory length: 205599   epsilon: 0.7909120000045391    steps: 198    lr: 4e-05     evaluation reward: 2.68\n",
            "episode: 1080   score: 3.0   memory length: 205811   epsilon: 0.7904922400045482    steps: 212    lr: 4e-05     evaluation reward: 2.69\n",
            "episode: 1081   score: 4.0   memory length: 206106   epsilon: 0.7899081400045609    steps: 295    lr: 4e-05     evaluation reward: 2.67\n",
            "episode: 1082   score: 0.0   memory length: 206229   epsilon: 0.7896646000045662    steps: 123    lr: 4e-05     evaluation reward: 2.67\n",
            "episode: 1083   score: 5.0   memory length: 206565   epsilon: 0.7889993200045806    steps: 336    lr: 4e-05     evaluation reward: 2.71\n",
            "episode: 1084   score: 3.0   memory length: 206794   epsilon: 0.7885459000045905    steps: 229    lr: 4e-05     evaluation reward: 2.7\n",
            "episode: 1085   score: 3.0   memory length: 207007   epsilon: 0.7881241600045996    steps: 213    lr: 4e-05     evaluation reward: 2.7\n",
            "episode: 1086   score: 4.0   memory length: 207324   epsilon: 0.7874965000046132    steps: 317    lr: 4e-05     evaluation reward: 2.71\n",
            "episode: 1087   score: 1.0   memory length: 207492   epsilon: 0.7871638600046205    steps: 168    lr: 4e-05     evaluation reward: 2.7\n",
            "episode: 1088   score: 2.0   memory length: 207690   epsilon: 0.786771820004629    steps: 198    lr: 4e-05     evaluation reward: 2.67\n",
            "episode: 1089   score: 4.0   memory length: 207981   epsilon: 0.7861956400046415    steps: 291    lr: 4e-05     evaluation reward: 2.71\n",
            "episode: 1090   score: 4.0   memory length: 208256   epsilon: 0.7856511400046533    steps: 275    lr: 4e-05     evaluation reward: 2.71\n",
            "episode: 1091   score: 0.0   memory length: 208379   epsilon: 0.7854076000046586    steps: 123    lr: 4e-05     evaluation reward: 2.7\n",
            "episode: 1092   score: 3.0   memory length: 208631   epsilon: 0.7849086400046694    steps: 252    lr: 4e-05     evaluation reward: 2.71\n",
            "episode: 1093   score: 3.0   memory length: 208879   epsilon: 0.7844176000046801    steps: 248    lr: 4e-05     evaluation reward: 2.74\n",
            "episode: 1094   score: 2.0   memory length: 209077   epsilon: 0.7840255600046886    steps: 198    lr: 4e-05     evaluation reward: 2.72\n",
            "episode: 1095   score: 3.0   memory length: 209304   epsilon: 0.7835761000046984    steps: 227    lr: 4e-05     evaluation reward: 2.71\n",
            "episode: 1096   score: 4.0   memory length: 209562   epsilon: 0.7830652600047094    steps: 258    lr: 4e-05     evaluation reward: 2.75\n",
            "episode: 1097   score: 3.0   memory length: 209810   epsilon: 0.7825742200047201    steps: 248    lr: 4e-05     evaluation reward: 2.74\n",
            "episode: 1098   score: 4.0   memory length: 210107   epsilon: 0.7819861600047329    steps: 297    lr: 4e-05     evaluation reward: 2.75\n",
            "episode: 1099   score: 2.0   memory length: 210287   epsilon: 0.7816297600047406    steps: 180    lr: 4e-05     evaluation reward: 2.75\n",
            "episode: 1100   score: 2.0   memory length: 210485   epsilon: 0.7812377200047491    steps: 198    lr: 4e-05     evaluation reward: 2.75\n",
            "episode: 1101   score: 0.0   memory length: 210608   epsilon: 0.7809941800047544    steps: 123    lr: 4e-05     evaluation reward: 2.74\n",
            "episode: 1102   score: 3.0   memory length: 210854   epsilon: 0.780507100004765    steps: 246    lr: 4e-05     evaluation reward: 2.76\n",
            "episode: 1103   score: 3.0   memory length: 211080   epsilon: 0.7800596200047747    steps: 226    lr: 4e-05     evaluation reward: 2.69\n",
            "episode: 1104   score: 1.0   memory length: 211249   epsilon: 0.779725000004782    steps: 169    lr: 4e-05     evaluation reward: 2.69\n",
            "episode: 1105   score: 6.0   memory length: 211626   epsilon: 0.7789785400047982    steps: 377    lr: 4e-05     evaluation reward: 2.73\n",
            "episode: 1106   score: 2.0   memory length: 211824   epsilon: 0.7785865000048067    steps: 198    lr: 4e-05     evaluation reward: 2.66\n",
            "episode: 1107   score: 1.0   memory length: 211994   epsilon: 0.778249900004814    steps: 170    lr: 4e-05     evaluation reward: 2.66\n",
            "episode: 1108   score: 3.0   memory length: 212240   epsilon: 0.7777628200048246    steps: 246    lr: 4e-05     evaluation reward: 2.68\n",
            "episode: 1109   score: 4.0   memory length: 212536   epsilon: 0.7771767400048373    steps: 296    lr: 4e-05     evaluation reward: 2.69\n",
            "episode: 1110   score: 5.0   memory length: 212901   epsilon: 0.776454040004853    steps: 365    lr: 4e-05     evaluation reward: 2.72\n",
            "episode: 1111   score: 3.0   memory length: 213147   epsilon: 0.7759669600048635    steps: 246    lr: 4e-05     evaluation reward: 2.72\n",
            "episode: 1112   score: 4.0   memory length: 213424   epsilon: 0.7754185000048754    steps: 277    lr: 4e-05     evaluation reward: 2.72\n",
            "episode: 1113   score: 2.0   memory length: 213605   epsilon: 0.7750601200048832    steps: 181    lr: 4e-05     evaluation reward: 2.7\n",
            "episode: 1114   score: 2.0   memory length: 213802   epsilon: 0.7746700600048917    steps: 197    lr: 4e-05     evaluation reward: 2.67\n",
            "episode: 1115   score: 3.0   memory length: 214069   epsilon: 0.7741414000049032    steps: 267    lr: 4e-05     evaluation reward: 2.68\n",
            "episode: 1116   score: 2.0   memory length: 214267   epsilon: 0.7737493600049117    steps: 198    lr: 4e-05     evaluation reward: 2.67\n",
            "episode: 1117   score: 5.0   memory length: 214559   epsilon: 0.7731712000049242    steps: 292    lr: 4e-05     evaluation reward: 2.68\n",
            "episode: 1118   score: 4.0   memory length: 214801   epsilon: 0.7726920400049346    steps: 242    lr: 4e-05     evaluation reward: 2.71\n",
            "episode: 1119   score: 4.0   memory length: 215079   epsilon: 0.7721416000049466    steps: 278    lr: 4e-05     evaluation reward: 2.71\n",
            "episode: 1120   score: 4.0   memory length: 215378   epsilon: 0.7715495800049594    steps: 299    lr: 4e-05     evaluation reward: 2.64\n",
            "episode: 1121   score: 6.0   memory length: 215758   epsilon: 0.7707971800049758    steps: 380    lr: 4e-05     evaluation reward: 2.7\n",
            "episode: 1122   score: 3.0   memory length: 216005   epsilon: 0.7703081200049864    steps: 247    lr: 4e-05     evaluation reward: 2.69\n",
            "episode: 1123   score: 1.0   memory length: 216177   epsilon: 0.7699675600049938    steps: 172    lr: 4e-05     evaluation reward: 2.69\n",
            "episode: 1124   score: 1.0   memory length: 216328   epsilon: 0.7696685800050003    steps: 151    lr: 4e-05     evaluation reward: 2.69\n",
            "episode: 1125   score: 7.0   memory length: 216715   epsilon: 0.7689023200050169    steps: 387    lr: 4e-05     evaluation reward: 2.73\n",
            "episode: 1126   score: 1.0   memory length: 216883   epsilon: 0.7685696800050241    steps: 168    lr: 4e-05     evaluation reward: 2.73\n",
            "episode: 1127   score: 6.0   memory length: 217260   epsilon: 0.7678232200050403    steps: 377    lr: 4e-05     evaluation reward: 2.78\n",
            "episode: 1128   score: 2.0   memory length: 217458   epsilon: 0.7674311800050488    steps: 198    lr: 4e-05     evaluation reward: 2.8\n",
            "episode: 1129   score: 7.0   memory length: 217777   epsilon: 0.7667995600050626    steps: 319    lr: 4e-05     evaluation reward: 2.85\n",
            "episode: 1130   score: 3.0   memory length: 218024   epsilon: 0.7663105000050732    steps: 247    lr: 4e-05     evaluation reward: 2.87\n",
            "episode: 1131   score: 9.0   memory length: 218471   epsilon: 0.7654254400050924    steps: 447    lr: 4e-05     evaluation reward: 2.94\n",
            "episode: 1132   score: 4.0   memory length: 218752   epsilon: 0.7648690600051045    steps: 281    lr: 4e-05     evaluation reward: 2.96\n",
            "episode: 1133   score: 3.0   memory length: 218998   epsilon: 0.764381980005115    steps: 246    lr: 4e-05     evaluation reward: 2.97\n",
            "episode: 1134   score: 4.0   memory length: 219274   epsilon: 0.7638355000051269    steps: 276    lr: 4e-05     evaluation reward: 2.96\n",
            "episode: 1135   score: 1.0   memory length: 219425   epsilon: 0.7635365200051334    steps: 151    lr: 4e-05     evaluation reward: 2.96\n",
            "episode: 1136   score: 4.0   memory length: 219667   epsilon: 0.7630573600051438    steps: 242    lr: 4e-05     evaluation reward: 2.97\n",
            "episode: 1137   score: 6.0   memory length: 220021   epsilon: 0.762356440005159    steps: 354    lr: 4e-05     evaluation reward: 2.98\n",
            "episode: 1138   score: 4.0   memory length: 220281   epsilon: 0.7618416400051702    steps: 260    lr: 4e-05     evaluation reward: 2.98\n",
            "episode: 1139   score: 6.0   memory length: 220675   epsilon: 0.7610615200051871    steps: 394    lr: 4e-05     evaluation reward: 3.01\n",
            "episode: 1140   score: 3.0   memory length: 220901   epsilon: 0.7606140400051968    steps: 226    lr: 4e-05     evaluation reward: 3.0\n",
            "episode: 1141   score: 5.0   memory length: 221227   epsilon: 0.7599685600052108    steps: 326    lr: 4e-05     evaluation reward: 3.01\n",
            "episode: 1142   score: 6.0   memory length: 221570   epsilon: 0.7592894200052256    steps: 343    lr: 4e-05     evaluation reward: 3.04\n",
            "episode: 1143   score: 6.0   memory length: 221923   epsilon: 0.7585904800052408    steps: 353    lr: 4e-05     evaluation reward: 3.07\n",
            "episode: 1144   score: 5.0   memory length: 222270   epsilon: 0.7579034200052557    steps: 347    lr: 4e-05     evaluation reward: 3.11\n",
            "episode: 1145   score: 1.0   memory length: 222421   epsilon: 0.7576044400052622    steps: 151    lr: 4e-05     evaluation reward: 3.1\n",
            "episode: 1146   score: 1.0   memory length: 222572   epsilon: 0.7573054600052687    steps: 151    lr: 4e-05     evaluation reward: 3.09\n",
            "episode: 1147   score: 2.0   memory length: 222769   epsilon: 0.7569154000052771    steps: 197    lr: 4e-05     evaluation reward: 3.08\n",
            "episode: 1148   score: 7.0   memory length: 223137   epsilon: 0.756186760005293    steps: 368    lr: 4e-05     evaluation reward: 3.14\n",
            "episode: 1149   score: 1.0   memory length: 223289   epsilon: 0.7558858000052995    steps: 152    lr: 4e-05     evaluation reward: 3.14\n",
            "episode: 1150   score: 5.0   memory length: 223586   epsilon: 0.7552977400053122    steps: 297    lr: 4e-05     evaluation reward: 3.17\n",
            "episode: 1151   score: 3.0   memory length: 223797   epsilon: 0.7548799600053213    steps: 211    lr: 4e-05     evaluation reward: 3.18\n",
            "episode: 1152   score: 3.0   memory length: 224023   epsilon: 0.754432480005331    steps: 226    lr: 4e-05     evaluation reward: 3.2\n",
            "episode: 1153   score: 3.0   memory length: 224270   epsilon: 0.7539434200053416    steps: 247    lr: 4e-05     evaluation reward: 3.21\n",
            "episode: 1154   score: 2.0   memory length: 224470   epsilon: 0.7535474200053502    steps: 200    lr: 4e-05     evaluation reward: 3.21\n",
            "episode: 1155   score: 8.0   memory length: 224927   epsilon: 0.7526425600053699    steps: 457    lr: 4e-05     evaluation reward: 3.26\n",
            "episode: 1156   score: 2.0   memory length: 225125   epsilon: 0.7522505200053784    steps: 198    lr: 4e-05     evaluation reward: 3.24\n",
            "episode: 1157   score: 1.0   memory length: 225277   epsilon: 0.7519495600053849    steps: 152    lr: 4e-05     evaluation reward: 3.24\n",
            "episode: 1158   score: 1.0   memory length: 225446   epsilon: 0.7516149400053922    steps: 169    lr: 4e-05     evaluation reward: 3.21\n",
            "episode: 1159   score: 2.0   memory length: 225628   epsilon: 0.7512545800054    steps: 182    lr: 4e-05     evaluation reward: 3.22\n",
            "episode: 1160   score: 3.0   memory length: 225854   epsilon: 0.7508071000054097    steps: 226    lr: 4e-05     evaluation reward: 3.24\n",
            "episode: 1161   score: 5.0   memory length: 226170   epsilon: 0.7501814200054233    steps: 316    lr: 4e-05     evaluation reward: 3.27\n",
            "episode: 1162   score: 5.0   memory length: 226471   epsilon: 0.7495854400054363    steps: 301    lr: 4e-05     evaluation reward: 3.28\n",
            "episode: 1163   score: 2.0   memory length: 226671   epsilon: 0.7491894400054449    steps: 200    lr: 4e-05     evaluation reward: 3.25\n",
            "episode: 1164   score: 5.0   memory length: 226960   epsilon: 0.7486172200054573    steps: 289    lr: 4e-05     evaluation reward: 3.26\n",
            "episode: 1165   score: 6.0   memory length: 227339   epsilon: 0.7478668000054736    steps: 379    lr: 4e-05     evaluation reward: 3.26\n",
            "episode: 1166   score: 3.0   memory length: 227607   epsilon: 0.7473361600054851    steps: 268    lr: 4e-05     evaluation reward: 3.25\n",
            "episode: 1167   score: 4.0   memory length: 227902   epsilon: 0.7467520600054978    steps: 295    lr: 4e-05     evaluation reward: 3.26\n",
            "episode: 1168   score: 10.0   memory length: 228448   epsilon: 0.7456709800055212    steps: 546    lr: 4e-05     evaluation reward: 3.32\n",
            "episode: 1169   score: 6.0   memory length: 228823   epsilon: 0.7449284800055374    steps: 375    lr: 4e-05     evaluation reward: 3.38\n",
            "episode: 1170   score: 4.0   memory length: 229099   epsilon: 0.7443820000055492    steps: 276    lr: 4e-05     evaluation reward: 3.39\n",
            "episode: 1171   score: 7.0   memory length: 229493   epsilon: 0.7436018800055662    steps: 394    lr: 4e-05     evaluation reward: 3.44\n",
            "episode: 1172   score: 2.0   memory length: 229675   epsilon: 0.743241520005574    steps: 182    lr: 4e-05     evaluation reward: 3.42\n",
            "episode: 1173   score: 5.0   memory length: 229990   epsilon: 0.7426178200055875    steps: 315    lr: 4e-05     evaluation reward: 3.44\n",
            "episode: 1174   score: 2.0   memory length: 230187   epsilon: 0.742227760005596    steps: 197    lr: 4e-05     evaluation reward: 3.45\n",
            "episode: 1175   score: 5.0   memory length: 230510   epsilon: 0.7415882200056099    steps: 323    lr: 4e-05     evaluation reward: 3.49\n",
            "episode: 1176   score: 0.0   memory length: 230633   epsilon: 0.7413446800056152    steps: 123    lr: 4e-05     evaluation reward: 3.46\n",
            "episode: 1177   score: 0.0   memory length: 230755   epsilon: 0.7411031200056204    steps: 122    lr: 4e-05     evaluation reward: 3.42\n",
            "episode: 1178   score: 4.0   memory length: 231055   epsilon: 0.7405091200056333    steps: 300    lr: 4e-05     evaluation reward: 3.45\n",
            "episode: 1179   score: 5.0   memory length: 231355   epsilon: 0.7399151200056462    steps: 300    lr: 4e-05     evaluation reward: 3.48\n",
            "episode: 1180   score: 2.0   memory length: 231571   epsilon: 0.7394874400056555    steps: 216    lr: 4e-05     evaluation reward: 3.47\n",
            "episode: 1181   score: 8.0   memory length: 232024   epsilon: 0.7385905000056749    steps: 453    lr: 4e-05     evaluation reward: 3.51\n",
            "episode: 1182   score: 5.0   memory length: 232352   epsilon: 0.737941060005689    steps: 328    lr: 4e-05     evaluation reward: 3.56\n",
            "episode: 1183   score: 4.0   memory length: 232631   epsilon: 0.737388640005701    steps: 279    lr: 4e-05     evaluation reward: 3.55\n",
            "episode: 1184   score: 2.0   memory length: 232829   epsilon: 0.7369966000057095    steps: 198    lr: 4e-05     evaluation reward: 3.54\n",
            "episode: 1185   score: 1.0   memory length: 232980   epsilon: 0.736697620005716    steps: 151    lr: 4e-05     evaluation reward: 3.52\n",
            "episode: 1186   score: 1.0   memory length: 233130   epsilon: 0.7364006200057225    steps: 150    lr: 4e-05     evaluation reward: 3.49\n",
            "episode: 1187   score: 6.0   memory length: 233464   epsilon: 0.7357393000057368    steps: 334    lr: 4e-05     evaluation reward: 3.54\n",
            "episode: 1188   score: 3.0   memory length: 233712   epsilon: 0.7352482600057475    steps: 248    lr: 4e-05     evaluation reward: 3.55\n",
            "episode: 1189   score: 7.0   memory length: 234151   epsilon: 0.7343790400057664    steps: 439    lr: 4e-05     evaluation reward: 3.58\n",
            "episode: 1190   score: 2.0   memory length: 234333   epsilon: 0.7340186800057742    steps: 182    lr: 4e-05     evaluation reward: 3.56\n",
            "episode: 1191   score: 5.0   memory length: 234678   epsilon: 0.733335580005789    steps: 345    lr: 4e-05     evaluation reward: 3.61\n",
            "episode: 1192   score: 6.0   memory length: 235050   epsilon: 0.732599020005805    steps: 372    lr: 4e-05     evaluation reward: 3.64\n",
            "episode: 1193   score: 2.0   memory length: 235272   epsilon: 0.7321594600058146    steps: 222    lr: 4e-05     evaluation reward: 3.63\n",
            "episode: 1194   score: 3.0   memory length: 235498   epsilon: 0.7317119800058243    steps: 226    lr: 4e-05     evaluation reward: 3.64\n",
            "episode: 1195   score: 3.0   memory length: 235747   epsilon: 0.731218960005835    steps: 249    lr: 4e-05     evaluation reward: 3.64\n",
            "episode: 1196   score: 2.0   memory length: 235947   epsilon: 0.7308229600058436    steps: 200    lr: 4e-05     evaluation reward: 3.62\n",
            "episode: 1197   score: 3.0   memory length: 236173   epsilon: 0.7303754800058533    steps: 226    lr: 4e-05     evaluation reward: 3.62\n",
            "episode: 1198   score: 4.0   memory length: 236432   epsilon: 0.7298626600058644    steps: 259    lr: 4e-05     evaluation reward: 3.62\n",
            "episode: 1199   score: 6.0   memory length: 236770   epsilon: 0.729193420005879    steps: 338    lr: 4e-05     evaluation reward: 3.66\n",
            "episode: 1200   score: 2.0   memory length: 236988   epsilon: 0.7287617800058883    steps: 218    lr: 4e-05     evaluation reward: 3.66\n",
            "episode: 1201   score: 4.0   memory length: 237305   epsilon: 0.7281341200059019    steps: 317    lr: 4e-05     evaluation reward: 3.7\n",
            "episode: 1202   score: 6.0   memory length: 237683   epsilon: 0.7273856800059182    steps: 378    lr: 4e-05     evaluation reward: 3.73\n",
            "episode: 1203   score: 1.0   memory length: 237853   epsilon: 0.7270490800059255    steps: 170    lr: 4e-05     evaluation reward: 3.71\n",
            "episode: 1204   score: 8.0   memory length: 238307   epsilon: 0.726150160005945    steps: 454    lr: 4e-05     evaluation reward: 3.78\n",
            "episode: 1205   score: 5.0   memory length: 238634   epsilon: 0.7255027000059591    steps: 327    lr: 4e-05     evaluation reward: 3.77\n",
            "episode: 1206   score: 3.0   memory length: 238881   epsilon: 0.7250136400059697    steps: 247    lr: 4e-05     evaluation reward: 3.78\n",
            "episode: 1207   score: 3.0   memory length: 239090   epsilon: 0.7245998200059787    steps: 209    lr: 4e-05     evaluation reward: 3.8\n",
            "episode: 1208   score: 5.0   memory length: 239360   epsilon: 0.7240652200059903    steps: 270    lr: 4e-05     evaluation reward: 3.82\n",
            "episode: 1209   score: 6.0   memory length: 239738   epsilon: 0.7233167800060065    steps: 378    lr: 4e-05     evaluation reward: 3.84\n",
            "episode: 1210   score: 6.0   memory length: 240112   epsilon: 0.7225762600060226    steps: 374    lr: 4e-05     evaluation reward: 3.85\n",
            "episode: 1211   score: 0.0   memory length: 240234   epsilon: 0.7223347000060278    steps: 122    lr: 4e-05     evaluation reward: 3.82\n",
            "episode: 1212   score: 4.0   memory length: 240511   epsilon: 0.7217862400060397    steps: 277    lr: 4e-05     evaluation reward: 3.82\n",
            "episode: 1213   score: 2.0   memory length: 240708   epsilon: 0.7213961800060482    steps: 197    lr: 4e-05     evaluation reward: 3.82\n",
            "episode: 1214   score: 3.0   memory length: 240934   epsilon: 0.7209487000060579    steps: 226    lr: 4e-05     evaluation reward: 3.83\n",
            "episode: 1215   score: 4.0   memory length: 241232   epsilon: 0.7203586600060707    steps: 298    lr: 4e-05     evaluation reward: 3.84\n",
            "episode: 1216   score: 0.0   memory length: 241354   epsilon: 0.720117100006076    steps: 122    lr: 4e-05     evaluation reward: 3.82\n",
            "episode: 1217   score: 7.0   memory length: 241756   epsilon: 0.7193211400060933    steps: 402    lr: 4e-05     evaluation reward: 3.84\n",
            "episode: 1218   score: 5.0   memory length: 242049   epsilon: 0.7187410000061059    steps: 293    lr: 4e-05     evaluation reward: 3.85\n",
            "episode: 1219   score: 3.0   memory length: 242262   epsilon: 0.718319260006115    steps: 213    lr: 4e-05     evaluation reward: 3.84\n",
            "episode: 1220   score: 4.0   memory length: 242559   epsilon: 0.7177312000061278    steps: 297    lr: 4e-05     evaluation reward: 3.84\n",
            "episode: 1221   score: 3.0   memory length: 242786   epsilon: 0.7172817400061375    steps: 227    lr: 4e-05     evaluation reward: 3.81\n",
            "episode: 1222   score: 4.0   memory length: 243068   epsilon: 0.7167233800061497    steps: 282    lr: 4e-05     evaluation reward: 3.82\n",
            "episode: 1223   score: 5.0   memory length: 243398   epsilon: 0.7160699800061638    steps: 330    lr: 4e-05     evaluation reward: 3.86\n",
            "episode: 1224   score: 3.0   memory length: 243642   epsilon: 0.7155868600061743    steps: 244    lr: 4e-05     evaluation reward: 3.88\n",
            "episode: 1225   score: 4.0   memory length: 243920   epsilon: 0.7150364200061863    steps: 278    lr: 4e-05     evaluation reward: 3.85\n",
            "episode: 1226   score: 4.0   memory length: 244195   epsilon: 0.7144919200061981    steps: 275    lr: 4e-05     evaluation reward: 3.88\n",
            "episode: 1227   score: 3.0   memory length: 244421   epsilon: 0.7140444400062078    steps: 226    lr: 4e-05     evaluation reward: 3.85\n",
            "episode: 1228   score: 2.0   memory length: 244640   epsilon: 0.7136108200062172    steps: 219    lr: 4e-05     evaluation reward: 3.85\n",
            "episode: 1229   score: 1.0   memory length: 244810   epsilon: 0.7132742200062245    steps: 170    lr: 4e-05     evaluation reward: 3.79\n",
            "episode: 1230   score: 2.0   memory length: 245009   epsilon: 0.7128802000062331    steps: 199    lr: 4e-05     evaluation reward: 3.78\n",
            "episode: 1231   score: 4.0   memory length: 245287   epsilon: 0.712329760006245    steps: 278    lr: 4e-05     evaluation reward: 3.73\n",
            "episode: 1232   score: 5.0   memory length: 245597   epsilon: 0.7117159600062584    steps: 310    lr: 4e-05     evaluation reward: 3.74\n",
            "episode: 1233   score: 4.0   memory length: 245872   epsilon: 0.7111714600062702    steps: 275    lr: 4e-05     evaluation reward: 3.75\n",
            "episode: 1234   score: 3.0   memory length: 246085   epsilon: 0.7107497200062793    steps: 213    lr: 4e-05     evaluation reward: 3.74\n",
            "episode: 1235   score: 4.0   memory length: 246348   epsilon: 0.7102289800062906    steps: 263    lr: 4e-05     evaluation reward: 3.77\n",
            "episode: 1236   score: 3.0   memory length: 246577   epsilon: 0.7097755600063005    steps: 229    lr: 4e-05     evaluation reward: 3.76\n",
            "episode: 1237   score: 1.0   memory length: 246747   epsilon: 0.7094389600063078    steps: 170    lr: 4e-05     evaluation reward: 3.71\n",
            "episode: 1238   score: 8.0   memory length: 247210   epsilon: 0.7085222200063277    steps: 463    lr: 4e-05     evaluation reward: 3.75\n",
            "episode: 1239   score: 1.0   memory length: 247379   epsilon: 0.708187600006335    steps: 169    lr: 4e-05     evaluation reward: 3.7\n",
            "episode: 1240   score: 6.0   memory length: 247752   epsilon: 0.707449060006351    steps: 373    lr: 4e-05     evaluation reward: 3.73\n",
            "episode: 1241   score: 4.0   memory length: 248065   epsilon: 0.7068293200063644    steps: 313    lr: 4e-05     evaluation reward: 3.72\n",
            "episode: 1242   score: 5.0   memory length: 248390   epsilon: 0.7061858200063784    steps: 325    lr: 4e-05     evaluation reward: 3.71\n",
            "episode: 1243   score: 2.0   memory length: 248587   epsilon: 0.7057957600063869    steps: 197    lr: 4e-05     evaluation reward: 3.67\n",
            "episode: 1244   score: 2.0   memory length: 248769   epsilon: 0.7054354000063947    steps: 182    lr: 4e-05     evaluation reward: 3.64\n",
            "episode: 1245   score: 3.0   memory length: 248998   epsilon: 0.7049819800064046    steps: 229    lr: 4e-05     evaluation reward: 3.66\n",
            "episode: 1246   score: 5.0   memory length: 249324   epsilon: 0.7043365000064186    steps: 326    lr: 4e-05     evaluation reward: 3.7\n",
            "episode: 1247   score: 4.0   memory length: 249621   epsilon: 0.7037484400064313    steps: 297    lr: 4e-05     evaluation reward: 3.72\n",
            "episode: 1248   score: 2.0   memory length: 249819   epsilon: 0.7033564000064398    steps: 198    lr: 4e-05     evaluation reward: 3.67\n",
            "episode: 1249   score: 1.0   memory length: 249989   epsilon: 0.7030198000064471    steps: 170    lr: 4e-05     evaluation reward: 3.67\n",
            "episode: 1250   score: 6.0   memory length: 250315   epsilon: 0.7023743200064612    steps: 326    lr: 4e-05     evaluation reward: 3.68\n",
            "episode: 1251   score: 1.0   memory length: 250466   epsilon: 0.7020753400064677    steps: 151    lr: 4e-05     evaluation reward: 3.66\n",
            "episode: 1252   score: 8.0   memory length: 250846   epsilon: 0.701322940006484    steps: 380    lr: 4e-05     evaluation reward: 3.71\n",
            "episode: 1253   score: 2.0   memory length: 251044   epsilon: 0.7009309000064925    steps: 198    lr: 4e-05     evaluation reward: 3.7\n",
            "episode: 1254   score: 7.0   memory length: 251461   epsilon: 0.7001052400065104    steps: 417    lr: 4e-05     evaluation reward: 3.75\n",
            "episode: 1255   score: 3.0   memory length: 251710   epsilon: 0.6996122200065211    steps: 249    lr: 4e-05     evaluation reward: 3.7\n",
            "episode: 1256   score: 7.0   memory length: 252084   epsilon: 0.6988717000065372    steps: 374    lr: 4e-05     evaluation reward: 3.75\n",
            "episode: 1257   score: 5.0   memory length: 252410   epsilon: 0.6982262200065512    steps: 326    lr: 4e-05     evaluation reward: 3.79\n",
            "episode: 1258   score: 3.0   memory length: 252623   epsilon: 0.6978044800065604    steps: 213    lr: 4e-05     evaluation reward: 3.81\n",
            "episode: 1259   score: 4.0   memory length: 252865   epsilon: 0.6973253200065708    steps: 242    lr: 4e-05     evaluation reward: 3.83\n",
            "episode: 1260   score: 4.0   memory length: 253142   epsilon: 0.6967768600065827    steps: 277    lr: 4e-05     evaluation reward: 3.84\n",
            "episode: 1261   score: 5.0   memory length: 253467   epsilon: 0.6961333600065966    steps: 325    lr: 4e-05     evaluation reward: 3.84\n",
            "episode: 1262   score: 3.0   memory length: 253699   epsilon: 0.6956740000066066    steps: 232    lr: 4e-05     evaluation reward: 3.82\n",
            "episode: 1263   score: 5.0   memory length: 254038   epsilon: 0.6950027800066212    steps: 339    lr: 4e-05     evaluation reward: 3.85\n",
            "episode: 1264   score: 1.0   memory length: 254207   epsilon: 0.6946681600066285    steps: 169    lr: 4e-05     evaluation reward: 3.81\n",
            "episode: 1265   score: 5.0   memory length: 254531   epsilon: 0.6940266400066424    steps: 324    lr: 4e-05     evaluation reward: 3.8\n",
            "episode: 1266   score: 4.0   memory length: 254790   epsilon: 0.6935138200066535    steps: 259    lr: 4e-05     evaluation reward: 3.81\n",
            "episode: 1267   score: 2.0   memory length: 254988   epsilon: 0.693121780006662    steps: 198    lr: 4e-05     evaluation reward: 3.79\n",
            "episode: 1268   score: 5.0   memory length: 255332   epsilon: 0.6924406600066768    steps: 344    lr: 4e-05     evaluation reward: 3.74\n",
            "episode: 1269   score: 2.0   memory length: 255549   epsilon: 0.6920110000066861    steps: 217    lr: 4e-05     evaluation reward: 3.7\n",
            "episode: 1270   score: 13.0   memory length: 256152   epsilon: 0.6908170600067121    steps: 603    lr: 4e-05     evaluation reward: 3.79\n",
            "episode: 1271   score: 2.0   memory length: 256350   epsilon: 0.6904250200067206    steps: 198    lr: 4e-05     evaluation reward: 3.74\n",
            "episode: 1272   score: 7.0   memory length: 256774   epsilon: 0.6895855000067388    steps: 424    lr: 4e-05     evaluation reward: 3.79\n",
            "episode: 1273   score: 3.0   memory length: 256982   epsilon: 0.6891736600067477    steps: 208    lr: 4e-05     evaluation reward: 3.77\n",
            "episode: 1274   score: 2.0   memory length: 257162   epsilon: 0.6888172600067555    steps: 180    lr: 4e-05     evaluation reward: 3.77\n",
            "episode: 1275   score: 4.0   memory length: 257404   epsilon: 0.6883381000067659    steps: 242    lr: 4e-05     evaluation reward: 3.76\n",
            "episode: 1276   score: 6.0   memory length: 257735   epsilon: 0.6876827200067801    steps: 331    lr: 4e-05     evaluation reward: 3.82\n",
            "episode: 1277   score: 4.0   memory length: 257990   epsilon: 0.6871778200067911    steps: 255    lr: 4e-05     evaluation reward: 3.86\n",
            "episode: 1278   score: 1.0   memory length: 258140   epsilon: 0.6868808200067975    steps: 150    lr: 4e-05     evaluation reward: 3.83\n",
            "episode: 1279   score: 9.0   memory length: 258502   epsilon: 0.6861640600068131    steps: 362    lr: 4e-05     evaluation reward: 3.87\n",
            "episode: 1280   score: 3.0   memory length: 258733   epsilon: 0.685706680006823    steps: 231    lr: 4e-05     evaluation reward: 3.88\n",
            "episode: 1281   score: 5.0   memory length: 259057   epsilon: 0.6850651600068369    steps: 324    lr: 4e-05     evaluation reward: 3.85\n",
            "episode: 1282   score: 8.0   memory length: 259518   epsilon: 0.6841523800068567    steps: 461    lr: 4e-05     evaluation reward: 3.88\n",
            "episode: 1283   score: 2.0   memory length: 259737   epsilon: 0.6837187600068662    steps: 219    lr: 4e-05     evaluation reward: 3.86\n",
            "episode: 1284   score: 5.0   memory length: 260023   epsilon: 0.6831524800068784    steps: 286    lr: 4e-05     evaluation reward: 3.89\n",
            "episode: 1285   score: 11.0   memory length: 260464   epsilon: 0.6822793000068974    steps: 441    lr: 4e-05     evaluation reward: 3.99\n",
            "episode: 1286   score: 5.0   memory length: 260760   epsilon: 0.6816932200069101    steps: 296    lr: 4e-05     evaluation reward: 4.03\n",
            "episode: 1287   score: 6.0   memory length: 261104   epsilon: 0.6810121000069249    steps: 344    lr: 4e-05     evaluation reward: 4.03\n",
            "episode: 1288   score: 6.0   memory length: 261476   epsilon: 0.6802755400069409    steps: 372    lr: 4e-05     evaluation reward: 4.06\n",
            "episode: 1289   score: 1.0   memory length: 261627   epsilon: 0.6799765600069474    steps: 151    lr: 4e-05     evaluation reward: 4.0\n",
            "episode: 1290   score: 8.0   memory length: 262056   epsilon: 0.6791271400069658    steps: 429    lr: 4e-05     evaluation reward: 4.06\n",
            "episode: 1291   score: 6.0   memory length: 262404   epsilon: 0.6784381000069808    steps: 348    lr: 4e-05     evaluation reward: 4.07\n",
            "episode: 1292   score: 2.0   memory length: 262602   epsilon: 0.6780460600069893    steps: 198    lr: 4e-05     evaluation reward: 4.03\n",
            "episode: 1293   score: 8.0   memory length: 263093   epsilon: 0.6770738800070104    steps: 491    lr: 4e-05     evaluation reward: 4.09\n",
            "episode: 1294   score: 7.0   memory length: 263458   epsilon: 0.6763511800070261    steps: 365    lr: 4e-05     evaluation reward: 4.13\n",
            "episode: 1295   score: 3.0   memory length: 263666   epsilon: 0.675939340007035    steps: 208    lr: 4e-05     evaluation reward: 4.13\n",
            "episode: 1296   score: 3.0   memory length: 263895   epsilon: 0.6754859200070449    steps: 229    lr: 4e-05     evaluation reward: 4.14\n",
            "episode: 1297   score: 3.0   memory length: 264121   epsilon: 0.6750384400070546    steps: 226    lr: 4e-05     evaluation reward: 4.14\n",
            "episode: 1298   score: 2.0   memory length: 264322   epsilon: 0.6746404600070632    steps: 201    lr: 4e-05     evaluation reward: 4.12\n",
            "episode: 1299   score: 6.0   memory length: 264651   epsilon: 0.6739890400070774    steps: 329    lr: 4e-05     evaluation reward: 4.12\n",
            "episode: 1300   score: 5.0   memory length: 264993   epsilon: 0.6733118800070921    steps: 342    lr: 4e-05     evaluation reward: 4.15\n",
            "episode: 1301   score: 3.0   memory length: 265206   epsilon: 0.6728901400071012    steps: 213    lr: 4e-05     evaluation reward: 4.14\n",
            "episode: 1302   score: 2.0   memory length: 265405   epsilon: 0.6724961200071098    steps: 199    lr: 4e-05     evaluation reward: 4.1\n",
            "episode: 1303   score: 3.0   memory length: 265652   epsilon: 0.6720070600071204    steps: 247    lr: 4e-05     evaluation reward: 4.12\n",
            "episode: 1304   score: 4.0   memory length: 265948   epsilon: 0.6714209800071331    steps: 296    lr: 4e-05     evaluation reward: 4.08\n",
            "episode: 1305   score: 7.0   memory length: 266373   epsilon: 0.6705794800071514    steps: 425    lr: 4e-05     evaluation reward: 4.1\n",
            "episode: 1306   score: 9.0   memory length: 266796   epsilon: 0.6697419400071696    steps: 423    lr: 4e-05     evaluation reward: 4.16\n",
            "episode: 1307   score: 2.0   memory length: 267012   epsilon: 0.6693142600071789    steps: 216    lr: 4e-05     evaluation reward: 4.15\n",
            "episode: 1308   score: 5.0   memory length: 267336   epsilon: 0.6686727400071928    steps: 324    lr: 4e-05     evaluation reward: 4.15\n",
            "episode: 1309   score: 3.0   memory length: 267600   epsilon: 0.6681500200072041    steps: 264    lr: 4e-05     evaluation reward: 4.12\n",
            "episode: 1310   score: 5.0   memory length: 267971   epsilon: 0.6674154400072201    steps: 371    lr: 4e-05     evaluation reward: 4.11\n",
            "episode: 1311   score: 5.0   memory length: 268260   epsilon: 0.6668432200072325    steps: 289    lr: 4e-05     evaluation reward: 4.16\n",
            "episode: 1312   score: 6.0   memory length: 268618   epsilon: 0.6661343800072479    steps: 358    lr: 4e-05     evaluation reward: 4.18\n",
            "episode: 1313   score: 3.0   memory length: 268844   epsilon: 0.6656869000072576    steps: 226    lr: 4e-05     evaluation reward: 4.19\n",
            "episode: 1314   score: 3.0   memory length: 269093   epsilon: 0.6651938800072683    steps: 249    lr: 4e-05     evaluation reward: 4.19\n",
            "episode: 1315   score: 5.0   memory length: 269436   epsilon: 0.6645147400072831    steps: 343    lr: 4e-05     evaluation reward: 4.2\n",
            "episode: 1316   score: 9.0   memory length: 269916   epsilon: 0.6635643400073037    steps: 480    lr: 4e-05     evaluation reward: 4.29\n",
            "episode: 1317   score: 4.0   memory length: 270191   epsilon: 0.6630198400073155    steps: 275    lr: 4e-05     evaluation reward: 4.26\n",
            "episode: 1318   score: 5.0   memory length: 270517   epsilon: 0.6623743600073295    steps: 326    lr: 4e-05     evaluation reward: 4.26\n",
            "episode: 1319   score: 6.0   memory length: 270858   epsilon: 0.6616991800073442    steps: 341    lr: 4e-05     evaluation reward: 4.29\n",
            "episode: 1320   score: 6.0   memory length: 271206   epsilon: 0.6610101400073591    steps: 348    lr: 4e-05     evaluation reward: 4.31\n",
            "episode: 1321   score: 5.0   memory length: 271481   epsilon: 0.660465640007371    steps: 275    lr: 4e-05     evaluation reward: 4.33\n",
            "episode: 1322   score: 7.0   memory length: 271856   epsilon: 0.6597231400073871    steps: 375    lr: 4e-05     evaluation reward: 4.36\n",
            "episode: 1323   score: 3.0   memory length: 272085   epsilon: 0.6592697200073969    steps: 229    lr: 4e-05     evaluation reward: 4.34\n",
            "episode: 1324   score: 4.0   memory length: 272360   epsilon: 0.6587252200074087    steps: 275    lr: 4e-05     evaluation reward: 4.35\n",
            "episode: 1325   score: 8.0   memory length: 272758   epsilon: 0.6579371800074258    steps: 398    lr: 4e-05     evaluation reward: 4.39\n",
            "episode: 1326   score: 7.0   memory length: 273099   epsilon: 0.6572620000074405    steps: 341    lr: 4e-05     evaluation reward: 4.42\n",
            "episode: 1327   score: 4.0   memory length: 273413   epsilon: 0.656640280007454    steps: 314    lr: 4e-05     evaluation reward: 4.43\n",
            "episode: 1328   score: 3.0   memory length: 273639   epsilon: 0.6561928000074637    steps: 226    lr: 4e-05     evaluation reward: 4.44\n",
            "episode: 1329   score: 3.0   memory length: 273852   epsilon: 0.6557710600074729    steps: 213    lr: 4e-05     evaluation reward: 4.46\n",
            "episode: 1330   score: 8.0   memory length: 274147   epsilon: 0.6551869600074856    steps: 295    lr: 4e-05     evaluation reward: 4.52\n",
            "episode: 1331   score: 5.0   memory length: 274471   epsilon: 0.6545454400074995    steps: 324    lr: 4e-05     evaluation reward: 4.53\n",
            "episode: 1332   score: 6.0   memory length: 274810   epsilon: 0.653874220007514    steps: 339    lr: 4e-05     evaluation reward: 4.54\n",
            "episode: 1333   score: 7.0   memory length: 275222   epsilon: 0.6530584600075318    steps: 412    lr: 4e-05     evaluation reward: 4.57\n",
            "episode: 1334   score: 5.0   memory length: 275529   epsilon: 0.652450600007545    steps: 307    lr: 4e-05     evaluation reward: 4.59\n",
            "episode: 1335   score: 3.0   memory length: 275740   epsilon: 0.652032820007554    steps: 211    lr: 4e-05     evaluation reward: 4.58\n",
            "episode: 1336   score: 5.0   memory length: 276043   epsilon: 0.651432880007567    steps: 303    lr: 4e-05     evaluation reward: 4.6\n",
            "episode: 1337   score: 3.0   memory length: 276271   epsilon: 0.6509814400075769    steps: 228    lr: 4e-05     evaluation reward: 4.62\n",
            "episode: 1338   score: 3.0   memory length: 276519   epsilon: 0.6504904000075875    steps: 248    lr: 4e-05     evaluation reward: 4.57\n",
            "episode: 1339   score: 7.0   memory length: 276867   epsilon: 0.6498013600076025    steps: 348    lr: 4e-05     evaluation reward: 4.63\n",
            "episode: 1340   score: 4.0   memory length: 277163   epsilon: 0.6492152800076152    steps: 296    lr: 4e-05     evaluation reward: 4.61\n",
            "episode: 1341   score: 6.0   memory length: 277507   epsilon: 0.64853416000763    steps: 344    lr: 4e-05     evaluation reward: 4.63\n",
            "episode: 1342   score: 2.0   memory length: 277705   epsilon: 0.6481421200076385    steps: 198    lr: 4e-05     evaluation reward: 4.6\n",
            "episode: 1343   score: 4.0   memory length: 277947   epsilon: 0.6476629600076489    steps: 242    lr: 4e-05     evaluation reward: 4.62\n",
            "episode: 1344   score: 7.0   memory length: 278390   epsilon: 0.6467858200076679    steps: 443    lr: 4e-05     evaluation reward: 4.67\n",
            "episode: 1345   score: 8.0   memory length: 278835   epsilon: 0.6459047200076871    steps: 445    lr: 4e-05     evaluation reward: 4.72\n",
            "episode: 1346   score: 4.0   memory length: 279151   epsilon: 0.6452790400077006    steps: 316    lr: 4e-05     evaluation reward: 4.71\n",
            "episode: 1347   score: 8.0   memory length: 279552   epsilon: 0.6444850600077179    steps: 401    lr: 4e-05     evaluation reward: 4.75\n",
            "episode: 1348   score: 3.0   memory length: 279781   epsilon: 0.6440316400077277    steps: 229    lr: 4e-05     evaluation reward: 4.76\n",
            "episode: 1349   score: 6.0   memory length: 280120   epsilon: 0.6433604200077423    steps: 339    lr: 4e-05     evaluation reward: 4.81\n",
            "episode: 1350   score: 3.0   memory length: 280389   epsilon: 0.6428278000077539    steps: 269    lr: 4e-05     evaluation reward: 4.78\n",
            "episode: 1351   score: 3.0   memory length: 280618   epsilon: 0.6423743800077637    steps: 229    lr: 4e-05     evaluation reward: 4.8\n",
            "episode: 1352   score: 4.0   memory length: 280884   epsilon: 0.6418477000077751    steps: 266    lr: 4e-05     evaluation reward: 4.76\n",
            "episode: 1353   score: 4.0   memory length: 281141   epsilon: 0.6413388400077862    steps: 257    lr: 4e-05     evaluation reward: 4.78\n",
            "episode: 1354   score: 2.0   memory length: 281323   epsilon: 0.640978480007794    steps: 182    lr: 4e-05     evaluation reward: 4.73\n",
            "episode: 1355   score: 6.0   memory length: 281678   epsilon: 0.6402755800078093    steps: 355    lr: 4e-05     evaluation reward: 4.76\n",
            "episode: 1356   score: 5.0   memory length: 282022   epsilon: 0.639594460007824    steps: 344    lr: 4e-05     evaluation reward: 4.74\n",
            "episode: 1357   score: 2.0   memory length: 282243   epsilon: 0.6391568800078335    steps: 221    lr: 4e-05     evaluation reward: 4.71\n",
            "episode: 1358   score: 3.0   memory length: 282489   epsilon: 0.6386698000078441    steps: 246    lr: 4e-05     evaluation reward: 4.71\n",
            "episode: 1359   score: 5.0   memory length: 282814   epsilon: 0.6380263000078581    steps: 325    lr: 4e-05     evaluation reward: 4.72\n",
            "episode: 1360   score: 6.0   memory length: 283166   epsilon: 0.6373293400078732    steps: 352    lr: 4e-05     evaluation reward: 4.74\n",
            "episode: 1361   score: 12.0   memory length: 283644   epsilon: 0.6363829000078938    steps: 478    lr: 4e-05     evaluation reward: 4.81\n",
            "episode: 1362   score: 2.0   memory length: 283825   epsilon: 0.6360245200079016    steps: 181    lr: 4e-05     evaluation reward: 4.8\n",
            "episode: 1363   score: 3.0   memory length: 284057   epsilon: 0.6355651600079115    steps: 232    lr: 4e-05     evaluation reward: 4.78\n",
            "episode: 1364   score: 5.0   memory length: 284366   epsilon: 0.6349533400079248    steps: 309    lr: 4e-05     evaluation reward: 4.82\n",
            "episode: 1365   score: 5.0   memory length: 284673   epsilon: 0.634345480007938    steps: 307    lr: 4e-05     evaluation reward: 4.82\n",
            "episode: 1366   score: 4.0   memory length: 284948   epsilon: 0.6338009800079498    steps: 275    lr: 4e-05     evaluation reward: 4.82\n",
            "episode: 1367   score: 9.0   memory length: 285396   epsilon: 0.6329139400079691    steps: 448    lr: 4e-05     evaluation reward: 4.89\n",
            "episode: 1368   score: 8.0   memory length: 285822   epsilon: 0.6320704600079874    steps: 426    lr: 4e-05     evaluation reward: 4.92\n",
            "episode: 1369   score: 4.0   memory length: 286098   epsilon: 0.6315239800079993    steps: 276    lr: 4e-05     evaluation reward: 4.94\n",
            "episode: 1370   score: 9.0   memory length: 286571   epsilon: 0.6305874400080196    steps: 473    lr: 4e-05     evaluation reward: 4.9\n",
            "episode: 1371   score: 6.0   memory length: 286912   epsilon: 0.6299122600080342    steps: 341    lr: 4e-05     evaluation reward: 4.94\n",
            "episode: 1372   score: 3.0   memory length: 287138   epsilon: 0.629464780008044    steps: 226    lr: 4e-05     evaluation reward: 4.9\n",
            "episode: 1373   score: 4.0   memory length: 287415   epsilon: 0.6289163200080559    steps: 277    lr: 4e-05     evaluation reward: 4.91\n",
            "episode: 1374   score: 7.0   memory length: 287663   epsilon: 0.6284252800080665    steps: 248    lr: 4e-05     evaluation reward: 4.96\n",
            "episode: 1375   score: 1.0   memory length: 287813   epsilon: 0.628128280008073    steps: 150    lr: 4e-05     evaluation reward: 4.93\n",
            "episode: 1376   score: 1.0   memory length: 287963   epsilon: 0.6278312800080794    steps: 150    lr: 4e-05     evaluation reward: 4.88\n",
            "episode: 1377   score: 6.0   memory length: 288301   epsilon: 0.627162040008094    steps: 338    lr: 4e-05     evaluation reward: 4.9\n",
            "episode: 1378   score: 3.0   memory length: 288531   epsilon: 0.6267066400081038    steps: 230    lr: 4e-05     evaluation reward: 4.92\n",
            "episode: 1379   score: 5.0   memory length: 288846   epsilon: 0.6260829400081174    steps: 315    lr: 4e-05     evaluation reward: 4.88\n",
            "episode: 1380   score: 3.0   memory length: 289057   epsilon: 0.6256651600081264    steps: 211    lr: 4e-05     evaluation reward: 4.88\n",
            "episode: 1381   score: 8.0   memory length: 289496   epsilon: 0.6247959400081453    steps: 439    lr: 4e-05     evaluation reward: 4.91\n",
            "episode: 1382   score: 1.0   memory length: 289668   epsilon: 0.6244553800081527    steps: 172    lr: 4e-05     evaluation reward: 4.84\n",
            "episode: 1383   score: 3.0   memory length: 289898   epsilon: 0.6239999800081626    steps: 230    lr: 4e-05     evaluation reward: 4.85\n",
            "episode: 1384   score: 6.0   memory length: 290255   epsilon: 0.6232931200081779    steps: 357    lr: 4e-05     evaluation reward: 4.86\n",
            "episode: 1385   score: 6.0   memory length: 290608   epsilon: 0.6225941800081931    steps: 353    lr: 4e-05     evaluation reward: 4.81\n",
            "episode: 1386   score: 5.0   memory length: 290917   epsilon: 0.6219823600082064    steps: 309    lr: 4e-05     evaluation reward: 4.81\n",
            "episode: 1387   score: 4.0   memory length: 291192   epsilon: 0.6214378600082182    steps: 275    lr: 4e-05     evaluation reward: 4.79\n",
            "episode: 1388   score: 1.0   memory length: 291361   epsilon: 0.6211032400082255    steps: 169    lr: 4e-05     evaluation reward: 4.74\n",
            "episode: 1389   score: 5.0   memory length: 291690   epsilon: 0.6204518200082396    steps: 329    lr: 4e-05     evaluation reward: 4.78\n",
            "episode: 1390   score: 3.0   memory length: 291955   epsilon: 0.619927120008251    steps: 265    lr: 4e-05     evaluation reward: 4.73\n",
            "episode: 1391   score: 8.0   memory length: 292381   epsilon: 0.6190836400082693    steps: 426    lr: 4e-05     evaluation reward: 4.75\n",
            "episode: 1392   score: 9.0   memory length: 292860   epsilon: 0.6181352200082899    steps: 479    lr: 4e-05     evaluation reward: 4.82\n",
            "episode: 1393   score: 5.0   memory length: 293183   epsilon: 0.6174956800083038    steps: 323    lr: 4e-05     evaluation reward: 4.79\n",
            "episode: 1394   score: 7.0   memory length: 293572   epsilon: 0.6167254600083205    steps: 389    lr: 4e-05     evaluation reward: 4.79\n",
            "episode: 1395   score: 5.0   memory length: 293901   epsilon: 0.6160740400083347    steps: 329    lr: 4e-05     evaluation reward: 4.81\n",
            "episode: 1396   score: 7.0   memory length: 294309   epsilon: 0.6152662000083522    steps: 408    lr: 4e-05     evaluation reward: 4.85\n",
            "episode: 1397   score: 8.0   memory length: 294761   epsilon: 0.6143712400083716    steps: 452    lr: 4e-05     evaluation reward: 4.9\n",
            "episode: 1398   score: 5.0   memory length: 295059   epsilon: 0.6137812000083844    steps: 298    lr: 4e-05     evaluation reward: 4.93\n",
            "episode: 1399   score: 5.0   memory length: 295386   epsilon: 0.6131337400083985    steps: 327    lr: 4e-05     evaluation reward: 4.92\n",
            "episode: 1400   score: 7.0   memory length: 295790   epsilon: 0.6123338200084159    steps: 404    lr: 4e-05     evaluation reward: 4.94\n",
            "episode: 1401   score: 9.0   memory length: 296282   epsilon: 0.611359660008437    steps: 492    lr: 4e-05     evaluation reward: 5.0\n",
            "episode: 1402   score: 2.0   memory length: 296498   epsilon: 0.6109319800084463    steps: 216    lr: 4e-05     evaluation reward: 5.0\n",
            "episode: 1403   score: 5.0   memory length: 296788   epsilon: 0.6103577800084588    steps: 290    lr: 4e-05     evaluation reward: 5.02\n",
            "episode: 1404   score: 4.0   memory length: 297065   epsilon: 0.6098093200084707    steps: 277    lr: 4e-05     evaluation reward: 5.02\n",
            "episode: 1405   score: 5.0   memory length: 297389   epsilon: 0.6091678000084846    steps: 324    lr: 4e-05     evaluation reward: 5.0\n",
            "episode: 1406   score: 4.0   memory length: 297664   epsilon: 0.6086233000084964    steps: 275    lr: 4e-05     evaluation reward: 4.95\n",
            "episode: 1407   score: 3.0   memory length: 297911   epsilon: 0.608134240008507    steps: 247    lr: 4e-05     evaluation reward: 4.96\n",
            "episode: 1408   score: 5.0   memory length: 298202   epsilon: 0.6075580600085195    steps: 291    lr: 4e-05     evaluation reward: 4.96\n",
            "episode: 1409   score: 5.0   memory length: 298549   epsilon: 0.6068710000085344    steps: 347    lr: 4e-05     evaluation reward: 4.98\n",
            "episode: 1410   score: 4.0   memory length: 298822   epsilon: 0.6063304600085462    steps: 273    lr: 4e-05     evaluation reward: 4.97\n",
            "episode: 1411   score: 3.0   memory length: 299047   epsilon: 0.6058849600085559    steps: 225    lr: 4e-05     evaluation reward: 4.95\n",
            "episode: 1412   score: 3.0   memory length: 299276   epsilon: 0.6054315400085657    steps: 229    lr: 4e-05     evaluation reward: 4.92\n",
            "episode: 1413   score: 4.0   memory length: 299568   epsilon: 0.6048533800085782    steps: 292    lr: 4e-05     evaluation reward: 4.93\n",
            "episode: 1414   score: 5.0   memory length: 299892   epsilon: 0.6042118600085922    steps: 324    lr: 4e-05     evaluation reward: 4.95\n",
            "episode: 1415   score: 1.0   memory length: 300043   epsilon: 0.6039128800085987    steps: 151    lr: 1.6000000000000003e-05     evaluation reward: 4.91\n",
            "episode: 1416   score: 3.0   memory length: 300254   epsilon: 0.6034951000086077    steps: 211    lr: 1.6000000000000003e-05     evaluation reward: 4.85\n",
            "episode: 1417   score: 12.0   memory length: 300909   epsilon: 0.6021982000086359    steps: 655    lr: 1.6000000000000003e-05     evaluation reward: 4.93\n",
            "episode: 1418   score: 6.0   memory length: 301261   epsilon: 0.601501240008651    steps: 352    lr: 1.6000000000000003e-05     evaluation reward: 4.94\n",
            "episode: 1419   score: 3.0   memory length: 301487   epsilon: 0.6010537600086607    steps: 226    lr: 1.6000000000000003e-05     evaluation reward: 4.91\n",
            "episode: 1420   score: 5.0   memory length: 301810   epsilon: 0.6004142200086746    steps: 323    lr: 1.6000000000000003e-05     evaluation reward: 4.9\n",
            "episode: 1421   score: 4.0   memory length: 302051   epsilon: 0.599937040008685    steps: 241    lr: 1.6000000000000003e-05     evaluation reward: 4.89\n",
            "episode: 1422   score: 3.0   memory length: 302297   epsilon: 0.5994499600086955    steps: 246    lr: 1.6000000000000003e-05     evaluation reward: 4.85\n",
            "episode: 1423   score: 7.0   memory length: 302699   epsilon: 0.5986540000087128    steps: 402    lr: 1.6000000000000003e-05     evaluation reward: 4.89\n",
            "episode: 1424   score: 7.0   memory length: 303063   epsilon: 0.5979332800087285    steps: 364    lr: 1.6000000000000003e-05     evaluation reward: 4.92\n",
            "episode: 1425   score: 6.0   memory length: 303401   epsilon: 0.597264040008743    steps: 338    lr: 1.6000000000000003e-05     evaluation reward: 4.9\n",
            "episode: 1426   score: 6.0   memory length: 303758   epsilon: 0.5965571800087583    steps: 357    lr: 1.6000000000000003e-05     evaluation reward: 4.89\n",
            "episode: 1427   score: 8.0   memory length: 304163   epsilon: 0.5957552800087758    steps: 405    lr: 1.6000000000000003e-05     evaluation reward: 4.93\n",
            "episode: 1428   score: 6.0   memory length: 304539   epsilon: 0.5950108000087919    steps: 376    lr: 1.6000000000000003e-05     evaluation reward: 4.96\n",
            "episode: 1429   score: 3.0   memory length: 304769   epsilon: 0.5945554000088018    steps: 230    lr: 1.6000000000000003e-05     evaluation reward: 4.96\n",
            "episode: 1430   score: 3.0   memory length: 304982   epsilon: 0.594133660008811    steps: 213    lr: 1.6000000000000003e-05     evaluation reward: 4.91\n",
            "episode: 1431   score: 7.0   memory length: 305358   epsilon: 0.5933891800088271    steps: 376    lr: 1.6000000000000003e-05     evaluation reward: 4.93\n",
            "episode: 1432   score: 6.0   memory length: 305714   epsilon: 0.5926843000088424    steps: 356    lr: 1.6000000000000003e-05     evaluation reward: 4.93\n",
            "episode: 1433   score: 10.0   memory length: 306173   epsilon: 0.5917754800088622    steps: 459    lr: 1.6000000000000003e-05     evaluation reward: 4.96\n",
            "episode: 1434   score: 4.0   memory length: 306469   epsilon: 0.5911894000088749    steps: 296    lr: 1.6000000000000003e-05     evaluation reward: 4.95\n",
            "episode: 1435   score: 9.0   memory length: 306927   epsilon: 0.5902825600088946    steps: 458    lr: 1.6000000000000003e-05     evaluation reward: 5.01\n",
            "episode: 1436   score: 6.0   memory length: 307299   epsilon: 0.5895460000089106    steps: 372    lr: 1.6000000000000003e-05     evaluation reward: 5.02\n",
            "episode: 1437   score: 4.0   memory length: 307573   epsilon: 0.5890034800089223    steps: 274    lr: 1.6000000000000003e-05     evaluation reward: 5.03\n",
            "episode: 1438   score: 5.0   memory length: 307882   epsilon: 0.5883916600089356    steps: 309    lr: 1.6000000000000003e-05     evaluation reward: 5.05\n",
            "episode: 1439   score: 14.0   memory length: 308392   epsilon: 0.5873818600089575    steps: 510    lr: 1.6000000000000003e-05     evaluation reward: 5.12\n",
            "episode: 1440   score: 8.0   memory length: 308814   epsilon: 0.5865463000089757    steps: 422    lr: 1.6000000000000003e-05     evaluation reward: 5.16\n",
            "episode: 1441   score: 4.0   memory length: 309074   epsilon: 0.5860315000089868    steps: 260    lr: 1.6000000000000003e-05     evaluation reward: 5.14\n",
            "episode: 1442   score: 6.0   memory length: 309413   epsilon: 0.5853602800090014    steps: 339    lr: 1.6000000000000003e-05     evaluation reward: 5.18\n",
            "episode: 1443   score: 4.0   memory length: 309693   epsilon: 0.5848058800090135    steps: 280    lr: 1.6000000000000003e-05     evaluation reward: 5.18\n",
            "episode: 1444   score: 5.0   memory length: 310033   epsilon: 0.5841326800090281    steps: 340    lr: 1.6000000000000003e-05     evaluation reward: 5.16\n",
            "episode: 1445   score: 7.0   memory length: 310387   epsilon: 0.5834317600090433    steps: 354    lr: 1.6000000000000003e-05     evaluation reward: 5.15\n",
            "episode: 1446   score: 5.0   memory length: 310691   epsilon: 0.5828298400090564    steps: 304    lr: 1.6000000000000003e-05     evaluation reward: 5.16\n",
            "episode: 1447   score: 12.0   memory length: 311167   epsilon: 0.5818873600090768    steps: 476    lr: 1.6000000000000003e-05     evaluation reward: 5.2\n",
            "episode: 1448   score: 11.0   memory length: 311570   epsilon: 0.5810894200090941    steps: 403    lr: 1.6000000000000003e-05     evaluation reward: 5.28\n",
            "episode: 1449   score: 5.0   memory length: 311875   epsilon: 0.5804855200091072    steps: 305    lr: 1.6000000000000003e-05     evaluation reward: 5.27\n",
            "episode: 1450   score: 2.0   memory length: 312057   epsilon: 0.5801251600091151    steps: 182    lr: 1.6000000000000003e-05     evaluation reward: 5.26\n",
            "episode: 1451   score: 3.0   memory length: 312268   epsilon: 0.5797073800091241    steps: 211    lr: 1.6000000000000003e-05     evaluation reward: 5.26\n",
            "episode: 1452   score: 4.0   memory length: 312543   epsilon: 0.579162880009136    steps: 275    lr: 1.6000000000000003e-05     evaluation reward: 5.26\n",
            "episode: 1453   score: 3.0   memory length: 312756   epsilon: 0.5787411400091451    steps: 213    lr: 1.6000000000000003e-05     evaluation reward: 5.25\n",
            "episode: 1454   score: 6.0   memory length: 313116   epsilon: 0.5780283400091606    steps: 360    lr: 1.6000000000000003e-05     evaluation reward: 5.29\n",
            "episode: 1455   score: 6.0   memory length: 313470   epsilon: 0.5773274200091758    steps: 354    lr: 1.6000000000000003e-05     evaluation reward: 5.29\n",
            "episode: 1456   score: 6.0   memory length: 313828   epsilon: 0.5766185800091912    steps: 358    lr: 1.6000000000000003e-05     evaluation reward: 5.3\n",
            "episode: 1457   score: 7.0   memory length: 314204   epsilon: 0.5758741000092074    steps: 376    lr: 1.6000000000000003e-05     evaluation reward: 5.35\n",
            "episode: 1458   score: 12.0   memory length: 314654   epsilon: 0.5749831000092267    steps: 450    lr: 1.6000000000000003e-05     evaluation reward: 5.44\n",
            "episode: 1459   score: 5.0   memory length: 314942   epsilon: 0.5744128600092391    steps: 288    lr: 1.6000000000000003e-05     evaluation reward: 5.44\n",
            "episode: 1460   score: 7.0   memory length: 315346   epsilon: 0.5736129400092564    steps: 404    lr: 1.6000000000000003e-05     evaluation reward: 5.45\n",
            "episode: 1461   score: 7.0   memory length: 315731   epsilon: 0.572850640009273    steps: 385    lr: 1.6000000000000003e-05     evaluation reward: 5.4\n",
            "episode: 1462   score: 15.0   memory length: 316361   epsilon: 0.5716032400093001    steps: 630    lr: 1.6000000000000003e-05     evaluation reward: 5.53\n",
            "episode: 1463   score: 6.0   memory length: 316716   epsilon: 0.5709003400093153    steps: 355    lr: 1.6000000000000003e-05     evaluation reward: 5.56\n",
            "episode: 1464   score: 5.0   memory length: 317041   epsilon: 0.5702568400093293    steps: 325    lr: 1.6000000000000003e-05     evaluation reward: 5.56\n",
            "episode: 1465   score: 9.0   memory length: 317495   epsilon: 0.5693579200093488    steps: 454    lr: 1.6000000000000003e-05     evaluation reward: 5.6\n",
            "episode: 1466   score: 4.0   memory length: 317756   epsilon: 0.56884114000936    steps: 261    lr: 1.6000000000000003e-05     evaluation reward: 5.6\n",
            "episode: 1467   score: 5.0   memory length: 318079   epsilon: 0.5682016000093739    steps: 323    lr: 1.6000000000000003e-05     evaluation reward: 5.56\n",
            "episode: 1468   score: 12.0   memory length: 318565   epsilon: 0.5672393200093948    steps: 486    lr: 1.6000000000000003e-05     evaluation reward: 5.6\n",
            "episode: 1469   score: 8.0   memory length: 319013   epsilon: 0.5663522800094141    steps: 448    lr: 1.6000000000000003e-05     evaluation reward: 5.64\n",
            "episode: 1470   score: 5.0   memory length: 319322   epsilon: 0.5657404600094273    steps: 309    lr: 1.6000000000000003e-05     evaluation reward: 5.6\n",
            "episode: 1471   score: 10.0   memory length: 319845   epsilon: 0.5647049200094498    steps: 523    lr: 1.6000000000000003e-05     evaluation reward: 5.64\n",
            "episode: 1472   score: 4.0   memory length: 320120   epsilon: 0.5641604200094616    steps: 275    lr: 1.6000000000000003e-05     evaluation reward: 5.65\n",
            "episode: 1473   score: 10.0   memory length: 320670   epsilon: 0.5630714200094853    steps: 550    lr: 1.6000000000000003e-05     evaluation reward: 5.71\n",
            "episode: 1474   score: 7.0   memory length: 321045   epsilon: 0.5623289200095014    steps: 375    lr: 1.6000000000000003e-05     evaluation reward: 5.71\n",
            "episode: 1475   score: 7.0   memory length: 321413   epsilon: 0.5616002800095172    steps: 368    lr: 1.6000000000000003e-05     evaluation reward: 5.77\n",
            "episode: 1476   score: 6.0   memory length: 321770   epsilon: 0.5608934200095326    steps: 357    lr: 1.6000000000000003e-05     evaluation reward: 5.82\n",
            "episode: 1477   score: 3.0   memory length: 321981   epsilon: 0.5604756400095416    steps: 211    lr: 1.6000000000000003e-05     evaluation reward: 5.79\n",
            "episode: 1478   score: 12.0   memory length: 322553   epsilon: 0.5593430800095662    steps: 572    lr: 1.6000000000000003e-05     evaluation reward: 5.88\n",
            "episode: 1479   score: 5.0   memory length: 322878   epsilon: 0.5586995800095802    steps: 325    lr: 1.6000000000000003e-05     evaluation reward: 5.88\n",
            "episode: 1480   score: 5.0   memory length: 323201   epsilon: 0.5580600400095941    steps: 323    lr: 1.6000000000000003e-05     evaluation reward: 5.9\n",
            "episode: 1481   score: 13.0   memory length: 323773   epsilon: 0.5569274800096187    steps: 572    lr: 1.6000000000000003e-05     evaluation reward: 5.95\n",
            "episode: 1482   score: 3.0   memory length: 324020   epsilon: 0.5564384200096293    steps: 247    lr: 1.6000000000000003e-05     evaluation reward: 5.97\n",
            "episode: 1483   score: 0.0   memory length: 324143   epsilon: 0.5561948800096346    steps: 123    lr: 1.6000000000000003e-05     evaluation reward: 5.94\n",
            "episode: 1484   score: 6.0   memory length: 324495   epsilon: 0.5554979200096497    steps: 352    lr: 1.6000000000000003e-05     evaluation reward: 5.94\n",
            "episode: 1485   score: 5.0   memory length: 324784   epsilon: 0.5549257000096621    steps: 289    lr: 1.6000000000000003e-05     evaluation reward: 5.93\n",
            "episode: 1486   score: 18.0   memory length: 325321   epsilon: 0.5538624400096852    steps: 537    lr: 1.6000000000000003e-05     evaluation reward: 6.06\n",
            "episode: 1487   score: 6.0   memory length: 325694   epsilon: 0.5531239000097012    steps: 373    lr: 1.6000000000000003e-05     evaluation reward: 6.08\n",
            "episode: 1488   score: 7.0   memory length: 326051   epsilon: 0.5524170400097166    steps: 357    lr: 1.6000000000000003e-05     evaluation reward: 6.14\n",
            "episode: 1489   score: 9.0   memory length: 326508   epsilon: 0.5515121800097362    steps: 457    lr: 1.6000000000000003e-05     evaluation reward: 6.18\n",
            "episode: 1490   score: 4.0   memory length: 326806   epsilon: 0.550922140009749    steps: 298    lr: 1.6000000000000003e-05     evaluation reward: 6.19\n",
            "episode: 1491   score: 8.0   memory length: 327211   epsilon: 0.5501202400097664    steps: 405    lr: 1.6000000000000003e-05     evaluation reward: 6.19\n",
            "episode: 1492   score: 5.0   memory length: 327541   epsilon: 0.5494668400097806    steps: 330    lr: 1.6000000000000003e-05     evaluation reward: 6.15\n",
            "episode: 1493   score: 4.0   memory length: 327842   epsilon: 0.5488708600097936    steps: 301    lr: 1.6000000000000003e-05     evaluation reward: 6.14\n",
            "episode: 1494   score: 5.0   memory length: 328151   epsilon: 0.5482590400098069    steps: 309    lr: 1.6000000000000003e-05     evaluation reward: 6.12\n",
            "episode: 1495   score: 8.0   memory length: 328585   epsilon: 0.5473997200098255    steps: 434    lr: 1.6000000000000003e-05     evaluation reward: 6.15\n",
            "episode: 1496   score: 5.0   memory length: 328873   epsilon: 0.5468294800098379    steps: 288    lr: 1.6000000000000003e-05     evaluation reward: 6.13\n",
            "episode: 1497   score: 7.0   memory length: 329276   epsilon: 0.5460315400098552    steps: 403    lr: 1.6000000000000003e-05     evaluation reward: 6.12\n",
            "episode: 1498   score: 8.0   memory length: 329719   epsilon: 0.5451544000098743    steps: 443    lr: 1.6000000000000003e-05     evaluation reward: 6.15\n",
            "episode: 1499   score: 7.0   memory length: 330108   epsilon: 0.544384180009891    steps: 389    lr: 1.6000000000000003e-05     evaluation reward: 6.17\n",
            "episode: 1500   score: 9.0   memory length: 330578   epsilon: 0.5434535800099112    steps: 470    lr: 1.6000000000000003e-05     evaluation reward: 6.19\n",
            "episode: 1501   score: 8.0   memory length: 331054   epsilon: 0.5425111000099316    steps: 476    lr: 1.6000000000000003e-05     evaluation reward: 6.18\n",
            "episode: 1502   score: 7.0   memory length: 331443   epsilon: 0.5417408800099484    steps: 389    lr: 1.6000000000000003e-05     evaluation reward: 6.23\n",
            "episode: 1503   score: 8.0   memory length: 331886   epsilon: 0.5408637400099674    steps: 443    lr: 1.6000000000000003e-05     evaluation reward: 6.26\n",
            "episode: 1504   score: 6.0   memory length: 332241   epsilon: 0.5401608400099827    steps: 355    lr: 1.6000000000000003e-05     evaluation reward: 6.28\n",
            "episode: 1505   score: 8.0   memory length: 332519   epsilon: 0.5396104000099946    steps: 278    lr: 1.6000000000000003e-05     evaluation reward: 6.31\n",
            "episode: 1506   score: 4.0   memory length: 332794   epsilon: 0.5390659000100064    steps: 275    lr: 1.6000000000000003e-05     evaluation reward: 6.31\n",
            "episode: 1507   score: 9.0   memory length: 333279   epsilon: 0.5381056000100273    steps: 485    lr: 1.6000000000000003e-05     evaluation reward: 6.37\n",
            "episode: 1508   score: 6.0   memory length: 333617   epsilon: 0.5374363600100418    steps: 338    lr: 1.6000000000000003e-05     evaluation reward: 6.38\n",
            "episode: 1509   score: 7.0   memory length: 334029   epsilon: 0.5366206000100595    steps: 412    lr: 1.6000000000000003e-05     evaluation reward: 6.4\n",
            "episode: 1510   score: 9.0   memory length: 334460   epsilon: 0.535767220010078    steps: 431    lr: 1.6000000000000003e-05     evaluation reward: 6.45\n",
            "episode: 1511   score: 6.0   memory length: 334810   epsilon: 0.5350742200100931    steps: 350    lr: 1.6000000000000003e-05     evaluation reward: 6.48\n",
            "episode: 1512   score: 5.0   memory length: 335100   epsilon: 0.5345000200101055    steps: 290    lr: 1.6000000000000003e-05     evaluation reward: 6.5\n",
            "episode: 1513   score: 6.0   memory length: 335477   epsilon: 0.5337535600101218    steps: 377    lr: 1.6000000000000003e-05     evaluation reward: 6.52\n",
            "episode: 1514   score: 10.0   memory length: 335936   epsilon: 0.5328447400101415    steps: 459    lr: 1.6000000000000003e-05     evaluation reward: 6.57\n",
            "episode: 1515   score: 6.0   memory length: 336290   epsilon: 0.5321438200101567    steps: 354    lr: 1.6000000000000003e-05     evaluation reward: 6.62\n",
            "episode: 1516   score: 5.0   memory length: 336597   epsilon: 0.5315359600101699    steps: 307    lr: 1.6000000000000003e-05     evaluation reward: 6.64\n",
            "episode: 1517   score: 6.0   memory length: 336955   epsilon: 0.5308271200101853    steps: 358    lr: 1.6000000000000003e-05     evaluation reward: 6.58\n",
            "episode: 1518   score: 4.0   memory length: 337249   epsilon: 0.5302450000101979    steps: 294    lr: 1.6000000000000003e-05     evaluation reward: 6.56\n",
            "episode: 1519   score: 7.0   memory length: 337636   epsilon: 0.5294787400102146    steps: 387    lr: 1.6000000000000003e-05     evaluation reward: 6.6\n",
            "episode: 1520   score: 5.0   memory length: 337928   epsilon: 0.5289005800102271    steps: 292    lr: 1.6000000000000003e-05     evaluation reward: 6.6\n",
            "episode: 1521   score: 7.0   memory length: 338322   epsilon: 0.528120460010244    steps: 394    lr: 1.6000000000000003e-05     evaluation reward: 6.63\n",
            "episode: 1522   score: 5.0   memory length: 338615   epsilon: 0.5275403200102566    steps: 293    lr: 1.6000000000000003e-05     evaluation reward: 6.65\n",
            "episode: 1523   score: 8.0   memory length: 339055   epsilon: 0.5266691200102755    steps: 440    lr: 1.6000000000000003e-05     evaluation reward: 6.66\n",
            "episode: 1524   score: 6.0   memory length: 339426   epsilon: 0.5259345400102915    steps: 371    lr: 1.6000000000000003e-05     evaluation reward: 6.65\n",
            "episode: 1525   score: 7.0   memory length: 339814   epsilon: 0.5251663000103082    steps: 388    lr: 1.6000000000000003e-05     evaluation reward: 6.66\n",
            "episode: 1526   score: 5.0   memory length: 340123   epsilon: 0.5245544800103215    steps: 309    lr: 1.6000000000000003e-05     evaluation reward: 6.65\n",
            "episode: 1527   score: 4.0   memory length: 340402   epsilon: 0.5240020600103334    steps: 279    lr: 1.6000000000000003e-05     evaluation reward: 6.61\n",
            "episode: 1528   score: 4.0   memory length: 340658   epsilon: 0.5234951800103445    steps: 256    lr: 1.6000000000000003e-05     evaluation reward: 6.59\n",
            "episode: 1529   score: 3.0   memory length: 340906   epsilon: 0.5230041400103551    steps: 248    lr: 1.6000000000000003e-05     evaluation reward: 6.59\n",
            "episode: 1530   score: 6.0   memory length: 341250   epsilon: 0.5223230200103699    steps: 344    lr: 1.6000000000000003e-05     evaluation reward: 6.62\n",
            "episode: 1531   score: 13.0   memory length: 341766   epsilon: 0.5213013400103921    steps: 516    lr: 1.6000000000000003e-05     evaluation reward: 6.68\n",
            "episode: 1532   score: 8.0   memory length: 342207   epsilon: 0.520428160010411    steps: 441    lr: 1.6000000000000003e-05     evaluation reward: 6.7\n",
            "episode: 1533   score: 8.0   memory length: 342630   epsilon: 0.5195906200104292    steps: 423    lr: 1.6000000000000003e-05     evaluation reward: 6.68\n",
            "episode: 1534   score: 11.0   memory length: 343208   epsilon: 0.5184461800104541    steps: 578    lr: 1.6000000000000003e-05     evaluation reward: 6.75\n",
            "episode: 1535   score: 7.0   memory length: 343584   epsilon: 0.5177017000104702    steps: 376    lr: 1.6000000000000003e-05     evaluation reward: 6.73\n",
            "episode: 1536   score: 6.0   memory length: 343939   epsilon: 0.5169988000104855    steps: 355    lr: 1.6000000000000003e-05     evaluation reward: 6.73\n",
            "episode: 1537   score: 5.0   memory length: 344262   epsilon: 0.5163592600104994    steps: 323    lr: 1.6000000000000003e-05     evaluation reward: 6.74\n",
            "episode: 1538   score: 4.0   memory length: 344520   epsilon: 0.5158484200105105    steps: 258    lr: 1.6000000000000003e-05     evaluation reward: 6.73\n",
            "episode: 1539   score: 10.0   memory length: 345051   epsilon: 0.5147970400105333    steps: 531    lr: 1.6000000000000003e-05     evaluation reward: 6.69\n",
            "episode: 1540   score: 5.0   memory length: 345399   epsilon: 0.5141080000105482    steps: 348    lr: 1.6000000000000003e-05     evaluation reward: 6.66\n",
            "episode: 1541   score: 11.0   memory length: 345815   epsilon: 0.5132843200105661    steps: 416    lr: 1.6000000000000003e-05     evaluation reward: 6.73\n",
            "episode: 1542   score: 7.0   memory length: 346222   epsilon: 0.5124784600105836    steps: 407    lr: 1.6000000000000003e-05     evaluation reward: 6.74\n",
            "episode: 1543   score: 5.0   memory length: 346526   epsilon: 0.5118765400105967    steps: 304    lr: 1.6000000000000003e-05     evaluation reward: 6.75\n",
            "episode: 1544   score: 5.0   memory length: 346875   epsilon: 0.5111855200106117    steps: 349    lr: 1.6000000000000003e-05     evaluation reward: 6.75\n",
            "episode: 1545   score: 5.0   memory length: 347198   epsilon: 0.5105459800106256    steps: 323    lr: 1.6000000000000003e-05     evaluation reward: 6.73\n",
            "episode: 1546   score: 5.0   memory length: 347523   epsilon: 0.5099024800106395    steps: 325    lr: 1.6000000000000003e-05     evaluation reward: 6.73\n",
            "episode: 1547   score: 6.0   memory length: 347899   epsilon: 0.5091580000106557    steps: 376    lr: 1.6000000000000003e-05     evaluation reward: 6.67\n",
            "episode: 1548   score: 5.0   memory length: 348228   epsilon: 0.5085065800106698    steps: 329    lr: 1.6000000000000003e-05     evaluation reward: 6.61\n",
            "episode: 1549   score: 7.0   memory length: 348620   epsilon: 0.5077304200106867    steps: 392    lr: 1.6000000000000003e-05     evaluation reward: 6.63\n",
            "episode: 1550   score: 13.0   memory length: 349102   epsilon: 0.5067760600107074    steps: 482    lr: 1.6000000000000003e-05     evaluation reward: 6.74\n",
            "episode: 1551   score: 6.0   memory length: 349440   epsilon: 0.5061068200107219    steps: 338    lr: 1.6000000000000003e-05     evaluation reward: 6.77\n",
            "episode: 1552   score: 15.0   memory length: 349980   epsilon: 0.5050376200107451    steps: 540    lr: 1.6000000000000003e-05     evaluation reward: 6.88\n",
            "episode: 1553   score: 5.0   memory length: 350292   epsilon: 0.5044198600107586    steps: 312    lr: 1.6000000000000003e-05     evaluation reward: 6.9\n",
            "episode: 1554   score: 4.0   memory length: 350588   epsilon: 0.5038337800107713    steps: 296    lr: 1.6000000000000003e-05     evaluation reward: 6.88\n",
            "episode: 1555   score: 7.0   memory length: 350998   epsilon: 0.5030219800107889    steps: 410    lr: 1.6000000000000003e-05     evaluation reward: 6.89\n",
            "episode: 1556   score: 5.0   memory length: 351319   epsilon: 0.5023864000108027    steps: 321    lr: 1.6000000000000003e-05     evaluation reward: 6.88\n",
            "episode: 1557   score: 4.0   memory length: 351581   epsilon: 0.501867640010814    steps: 262    lr: 1.6000000000000003e-05     evaluation reward: 6.85\n",
            "episode: 1558   score: 3.0   memory length: 351809   epsilon: 0.5014162000108238    steps: 228    lr: 1.6000000000000003e-05     evaluation reward: 6.76\n",
            "episode: 1559   score: 5.0   memory length: 352139   epsilon: 0.500762800010838    steps: 330    lr: 1.6000000000000003e-05     evaluation reward: 6.76\n",
            "episode: 1560   score: 5.0   memory length: 352482   epsilon: 0.5000836600108527    steps: 343    lr: 1.6000000000000003e-05     evaluation reward: 6.74\n",
            "episode: 1561   score: 8.0   memory length: 352920   epsilon: 0.49921642001084954    steps: 438    lr: 1.6000000000000003e-05     evaluation reward: 6.75\n",
            "episode: 1562   score: 4.0   memory length: 353180   epsilon: 0.4987016200108463    steps: 260    lr: 1.6000000000000003e-05     evaluation reward: 6.64\n",
            "episode: 1563   score: 6.0   memory length: 353514   epsilon: 0.4980403000108421    steps: 334    lr: 1.6000000000000003e-05     evaluation reward: 6.64\n",
            "episode: 1564   score: 7.0   memory length: 353890   epsilon: 0.4972958200108374    steps: 376    lr: 1.6000000000000003e-05     evaluation reward: 6.66\n",
            "episode: 1565   score: 8.0   memory length: 354312   epsilon: 0.4964602600108321    steps: 422    lr: 1.6000000000000003e-05     evaluation reward: 6.65\n",
            "episode: 1566   score: 7.0   memory length: 354699   epsilon: 0.49569400001082725    steps: 387    lr: 1.6000000000000003e-05     evaluation reward: 6.68\n",
            "episode: 1567   score: 9.0   memory length: 355205   epsilon: 0.4946921200108209    steps: 506    lr: 1.6000000000000003e-05     evaluation reward: 6.72\n",
            "episode: 1568   score: 4.0   memory length: 355445   epsilon: 0.4942169200108179    steps: 240    lr: 1.6000000000000003e-05     evaluation reward: 6.64\n",
            "episode: 1569   score: 9.0   memory length: 355891   epsilon: 0.4933338400108123    steps: 446    lr: 1.6000000000000003e-05     evaluation reward: 6.65\n",
            "episode: 1570   score: 7.0   memory length: 356275   epsilon: 0.4925735200108075    steps: 384    lr: 1.6000000000000003e-05     evaluation reward: 6.67\n",
            "episode: 1571   score: 4.0   memory length: 356571   epsilon: 0.4919874400108038    steps: 296    lr: 1.6000000000000003e-05     evaluation reward: 6.61\n",
            "episode: 1572   score: 5.0   memory length: 356880   epsilon: 0.49137562001079993    steps: 309    lr: 1.6000000000000003e-05     evaluation reward: 6.62\n",
            "episode: 1573   score: 7.0   memory length: 357247   epsilon: 0.49064896001079533    steps: 367    lr: 1.6000000000000003e-05     evaluation reward: 6.59\n",
            "episode: 1574   score: 5.0   memory length: 357547   epsilon: 0.4900549600107916    steps: 300    lr: 1.6000000000000003e-05     evaluation reward: 6.57\n",
            "episode: 1575   score: 8.0   memory length: 358002   epsilon: 0.4891540600107859    steps: 455    lr: 1.6000000000000003e-05     evaluation reward: 6.58\n",
            "episode: 1576   score: 4.0   memory length: 358247   epsilon: 0.4886689600107828    steps: 245    lr: 1.6000000000000003e-05     evaluation reward: 6.56\n",
            "episode: 1577   score: 9.0   memory length: 358706   epsilon: 0.48776014001077705    steps: 459    lr: 1.6000000000000003e-05     evaluation reward: 6.62\n",
            "episode: 1578   score: 4.0   memory length: 358981   epsilon: 0.4872156400107736    steps: 275    lr: 1.6000000000000003e-05     evaluation reward: 6.54\n",
            "episode: 1579   score: 7.0   memory length: 359366   epsilon: 0.4864533400107688    steps: 385    lr: 1.6000000000000003e-05     evaluation reward: 6.56\n",
            "episode: 1580   score: 6.0   memory length: 359721   epsilon: 0.48575044001076434    steps: 355    lr: 1.6000000000000003e-05     evaluation reward: 6.57\n",
            "episode: 1581   score: 6.0   memory length: 360082   epsilon: 0.4850356600107598    steps: 361    lr: 1.6000000000000003e-05     evaluation reward: 6.5\n",
            "episode: 1582   score: 9.0   memory length: 360539   epsilon: 0.4841308000107541    steps: 457    lr: 1.6000000000000003e-05     evaluation reward: 6.56\n",
            "episode: 1583   score: 5.0   memory length: 360830   epsilon: 0.48355462001075045    steps: 291    lr: 1.6000000000000003e-05     evaluation reward: 6.61\n",
            "episode: 1584   score: 7.0   memory length: 361207   epsilon: 0.4828081600107457    steps: 377    lr: 1.6000000000000003e-05     evaluation reward: 6.62\n",
            "episode: 1585   score: 4.0   memory length: 361485   epsilon: 0.48225772001074224    steps: 278    lr: 1.6000000000000003e-05     evaluation reward: 6.61\n",
            "episode: 1586   score: 8.0   memory length: 361896   epsilon: 0.4814439400107371    steps: 411    lr: 1.6000000000000003e-05     evaluation reward: 6.51\n",
            "episode: 1587   score: 5.0   memory length: 362168   epsilon: 0.4809053800107337    steps: 272    lr: 1.6000000000000003e-05     evaluation reward: 6.5\n",
            "episode: 1588   score: 5.0   memory length: 362471   epsilon: 0.4803054400107299    steps: 303    lr: 1.6000000000000003e-05     evaluation reward: 6.48\n",
            "episode: 1589   score: 7.0   memory length: 362827   epsilon: 0.47960056001072543    steps: 356    lr: 1.6000000000000003e-05     evaluation reward: 6.46\n",
            "episode: 1590   score: 6.0   memory length: 363147   epsilon: 0.4789669600107214    steps: 320    lr: 1.6000000000000003e-05     evaluation reward: 6.48\n",
            "episode: 1591   score: 4.0   memory length: 363391   epsilon: 0.47848384001071836    steps: 244    lr: 1.6000000000000003e-05     evaluation reward: 6.44\n",
            "episode: 1592   score: 8.0   memory length: 363814   epsilon: 0.47764630001071307    steps: 423    lr: 1.6000000000000003e-05     evaluation reward: 6.47\n",
            "episode: 1593   score: 9.0   memory length: 364309   epsilon: 0.47666620001070686    steps: 495    lr: 1.6000000000000003e-05     evaluation reward: 6.52\n",
            "episode: 1594   score: 7.0   memory length: 364694   epsilon: 0.47590390001070204    steps: 385    lr: 1.6000000000000003e-05     evaluation reward: 6.54\n",
            "episode: 1595   score: 5.0   memory length: 365040   epsilon: 0.4752188200106977    steps: 346    lr: 1.6000000000000003e-05     evaluation reward: 6.51\n",
            "episode: 1596   score: 6.0   memory length: 365395   epsilon: 0.47451592001069326    steps: 355    lr: 1.6000000000000003e-05     evaluation reward: 6.52\n",
            "episode: 1597   score: 7.0   memory length: 365798   epsilon: 0.4737179800106882    steps: 403    lr: 1.6000000000000003e-05     evaluation reward: 6.52\n",
            "episode: 1598   score: 8.0   memory length: 366216   epsilon: 0.472890340010683    steps: 418    lr: 1.6000000000000003e-05     evaluation reward: 6.52\n",
            "episode: 1599   score: 6.0   memory length: 366591   epsilon: 0.4721478400106783    steps: 375    lr: 1.6000000000000003e-05     evaluation reward: 6.51\n",
            "episode: 1600   score: 8.0   memory length: 367024   epsilon: 0.47129050001067285    steps: 433    lr: 1.6000000000000003e-05     evaluation reward: 6.5\n",
            "episode: 1601   score: 9.0   memory length: 367507   epsilon: 0.4703341600106668    steps: 483    lr: 1.6000000000000003e-05     evaluation reward: 6.51\n",
            "episode: 1602   score: 9.0   memory length: 367996   epsilon: 0.4693659400106607    steps: 489    lr: 1.6000000000000003e-05     evaluation reward: 6.53\n",
            "episode: 1603   score: 7.0   memory length: 368373   epsilon: 0.46861948001065595    steps: 377    lr: 1.6000000000000003e-05     evaluation reward: 6.52\n",
            "episode: 1604   score: 4.0   memory length: 368651   epsilon: 0.46806904001065247    steps: 278    lr: 1.6000000000000003e-05     evaluation reward: 6.5\n",
            "episode: 1605   score: 13.0   memory length: 369295   epsilon: 0.4667939200106444    steps: 644    lr: 1.6000000000000003e-05     evaluation reward: 6.55\n",
            "episode: 1606   score: 10.0   memory length: 369767   epsilon: 0.4658593600106385    steps: 472    lr: 1.6000000000000003e-05     evaluation reward: 6.61\n",
            "episode: 1607   score: 10.0   memory length: 370248   epsilon: 0.46490698001063246    steps: 481    lr: 1.6000000000000003e-05     evaluation reward: 6.62\n",
            "episode: 1608   score: 7.0   memory length: 370611   epsilon: 0.4641882400106279    steps: 363    lr: 1.6000000000000003e-05     evaluation reward: 6.63\n",
            "episode: 1609   score: 10.0   memory length: 371109   epsilon: 0.4632022000106217    steps: 498    lr: 1.6000000000000003e-05     evaluation reward: 6.66\n",
            "episode: 1610   score: 4.0   memory length: 371385   epsilon: 0.4626557200106182    steps: 276    lr: 1.6000000000000003e-05     evaluation reward: 6.61\n",
            "episode: 1611   score: 8.0   memory length: 371817   epsilon: 0.4618003600106128    steps: 432    lr: 1.6000000000000003e-05     evaluation reward: 6.63\n",
            "episode: 1612   score: 8.0   memory length: 372096   epsilon: 0.4612479400106093    steps: 279    lr: 1.6000000000000003e-05     evaluation reward: 6.66\n",
            "episode: 1613   score: 8.0   memory length: 372537   epsilon: 0.4603747600106038    steps: 441    lr: 1.6000000000000003e-05     evaluation reward: 6.68\n",
            "episode: 1614   score: 15.0   memory length: 373164   epsilon: 0.45913330001059593    steps: 627    lr: 1.6000000000000003e-05     evaluation reward: 6.73\n",
            "episode: 1615   score: 8.0   memory length: 373636   epsilon: 0.45819874001059    steps: 472    lr: 1.6000000000000003e-05     evaluation reward: 6.75\n",
            "episode: 1616   score: 14.0   memory length: 374189   epsilon: 0.4571038000105831    steps: 553    lr: 1.6000000000000003e-05     evaluation reward: 6.84\n",
            "episode: 1617   score: 6.0   memory length: 374542   epsilon: 0.4564048600105787    steps: 353    lr: 1.6000000000000003e-05     evaluation reward: 6.84\n",
            "episode: 1618   score: 9.0   memory length: 374981   epsilon: 0.4555356400105732    steps: 439    lr: 1.6000000000000003e-05     evaluation reward: 6.89\n",
            "episode: 1619   score: 7.0   memory length: 375389   epsilon: 0.45472780001056806    steps: 408    lr: 1.6000000000000003e-05     evaluation reward: 6.89\n",
            "episode: 1620   score: 8.0   memory length: 375866   epsilon: 0.4537833400105621    steps: 477    lr: 1.6000000000000003e-05     evaluation reward: 6.92\n",
            "episode: 1621   score: 6.0   memory length: 376207   epsilon: 0.4531081600105578    steps: 341    lr: 1.6000000000000003e-05     evaluation reward: 6.91\n",
            "episode: 1622   score: 7.0   memory length: 376605   epsilon: 0.45232012001055283    steps: 398    lr: 1.6000000000000003e-05     evaluation reward: 6.93\n",
            "episode: 1623   score: 8.0   memory length: 377032   epsilon: 0.4514746600105475    steps: 427    lr: 1.6000000000000003e-05     evaluation reward: 6.93\n",
            "episode: 1624   score: 10.0   memory length: 377553   epsilon: 0.45044308001054095    steps: 521    lr: 1.6000000000000003e-05     evaluation reward: 6.97\n",
            "episode: 1625   score: 6.0   memory length: 377888   epsilon: 0.44977978001053676    steps: 335    lr: 1.6000000000000003e-05     evaluation reward: 6.96\n",
            "episode: 1626   score: 11.0   memory length: 378330   epsilon: 0.4489046200105312    steps: 442    lr: 1.6000000000000003e-05     evaluation reward: 7.02\n",
            "episode: 1627   score: 10.0   memory length: 378740   epsilon: 0.4480928200105261    steps: 410    lr: 1.6000000000000003e-05     evaluation reward: 7.08\n",
            "episode: 1628   score: 10.0   memory length: 379216   epsilon: 0.4471503400105201    steps: 476    lr: 1.6000000000000003e-05     evaluation reward: 7.14\n",
            "episode: 1629   score: 7.0   memory length: 379643   epsilon: 0.44630488001051477    steps: 427    lr: 1.6000000000000003e-05     evaluation reward: 7.18\n",
            "episode: 1630   score: 8.0   memory length: 380086   epsilon: 0.4454277400105092    steps: 443    lr: 1.6000000000000003e-05     evaluation reward: 7.2\n",
            "episode: 1631   score: 7.0   memory length: 380463   epsilon: 0.4446812800105045    steps: 377    lr: 1.6000000000000003e-05     evaluation reward: 7.14\n",
            "episode: 1632   score: 8.0   memory length: 380883   epsilon: 0.44384968001049924    steps: 420    lr: 1.6000000000000003e-05     evaluation reward: 7.14\n",
            "episode: 1633   score: 8.0   memory length: 381339   epsilon: 0.4429468000104935    steps: 456    lr: 1.6000000000000003e-05     evaluation reward: 7.14\n",
            "episode: 1634   score: 19.0   memory length: 382039   epsilon: 0.44156080001048476    steps: 700    lr: 1.6000000000000003e-05     evaluation reward: 7.22\n",
            "episode: 1635   score: 9.0   memory length: 382513   epsilon: 0.4406222800104788    steps: 474    lr: 1.6000000000000003e-05     evaluation reward: 7.24\n",
            "episode: 1636   score: 6.0   memory length: 382867   epsilon: 0.4399213600104744    steps: 354    lr: 1.6000000000000003e-05     evaluation reward: 7.24\n",
            "episode: 1637   score: 9.0   memory length: 383320   epsilon: 0.4390244200104687    steps: 453    lr: 1.6000000000000003e-05     evaluation reward: 7.28\n",
            "episode: 1638   score: 6.0   memory length: 383642   epsilon: 0.4383868600104647    steps: 322    lr: 1.6000000000000003e-05     evaluation reward: 7.3\n",
            "episode: 1639   score: 6.0   memory length: 383960   epsilon: 0.4377572200104607    steps: 318    lr: 1.6000000000000003e-05     evaluation reward: 7.26\n",
            "episode: 1640   score: 9.0   memory length: 384396   epsilon: 0.43689394001045523    steps: 436    lr: 1.6000000000000003e-05     evaluation reward: 7.3\n",
            "episode: 1641   score: 8.0   memory length: 384892   epsilon: 0.435911860010449    steps: 496    lr: 1.6000000000000003e-05     evaluation reward: 7.27\n",
            "episode: 1642   score: 5.0   memory length: 385221   epsilon: 0.4352604400104449    steps: 329    lr: 1.6000000000000003e-05     evaluation reward: 7.25\n",
            "episode: 1643   score: 10.0   memory length: 385632   epsilon: 0.43444666001043974    steps: 411    lr: 1.6000000000000003e-05     evaluation reward: 7.3\n",
            "episode: 1644   score: 10.0   memory length: 386121   epsilon: 0.4334784400104336    steps: 489    lr: 1.6000000000000003e-05     evaluation reward: 7.35\n",
            "episode: 1645   score: 5.0   memory length: 386448   epsilon: 0.4328309800104295    steps: 327    lr: 1.6000000000000003e-05     evaluation reward: 7.35\n",
            "episode: 1646   score: 4.0   memory length: 386708   epsilon: 0.43231618001042627    steps: 260    lr: 1.6000000000000003e-05     evaluation reward: 7.34\n",
            "episode: 1647   score: 8.0   memory length: 387116   epsilon: 0.43150834001042115    steps: 408    lr: 1.6000000000000003e-05     evaluation reward: 7.36\n",
            "episode: 1648   score: 11.0   memory length: 387690   epsilon: 0.43037182001041396    steps: 574    lr: 1.6000000000000003e-05     evaluation reward: 7.42\n",
            "episode: 1649   score: 13.0   memory length: 388282   epsilon: 0.42919966001040655    steps: 592    lr: 1.6000000000000003e-05     evaluation reward: 7.48\n",
            "episode: 1650   score: 9.0   memory length: 388752   epsilon: 0.42826906001040066    steps: 470    lr: 1.6000000000000003e-05     evaluation reward: 7.44\n",
            "episode: 1651   score: 11.0   memory length: 389221   epsilon: 0.4273404400103948    steps: 469    lr: 1.6000000000000003e-05     evaluation reward: 7.49\n",
            "episode: 1652   score: 12.0   memory length: 389808   epsilon: 0.42617818001038743    steps: 587    lr: 1.6000000000000003e-05     evaluation reward: 7.46\n",
            "episode: 1653   score: 5.0   memory length: 390133   epsilon: 0.42553468001038336    steps: 325    lr: 1.6000000000000003e-05     evaluation reward: 7.46\n",
            "episode: 1654   score: 6.0   memory length: 390473   epsilon: 0.4248614800103791    steps: 340    lr: 1.6000000000000003e-05     evaluation reward: 7.48\n",
            "episode: 1655   score: 7.0   memory length: 390895   epsilon: 0.4240259200103738    steps: 422    lr: 1.6000000000000003e-05     evaluation reward: 7.48\n",
            "episode: 1656   score: 8.0   memory length: 391332   epsilon: 0.42316066001036834    steps: 437    lr: 1.6000000000000003e-05     evaluation reward: 7.51\n",
            "episode: 1657   score: 5.0   memory length: 391643   epsilon: 0.42254488001036444    steps: 311    lr: 1.6000000000000003e-05     evaluation reward: 7.52\n",
            "episode: 1658   score: 8.0   memory length: 392062   epsilon: 0.4217152600103592    steps: 419    lr: 1.6000000000000003e-05     evaluation reward: 7.57\n",
            "episode: 1659   score: 3.0   memory length: 392272   epsilon: 0.42129946001035656    steps: 210    lr: 1.6000000000000003e-05     evaluation reward: 7.55\n",
            "episode: 1660   score: 8.0   memory length: 392668   epsilon: 0.4205153800103516    steps: 396    lr: 1.6000000000000003e-05     evaluation reward: 7.58\n",
            "episode: 1661   score: 7.0   memory length: 393071   epsilon: 0.41971744001034655    steps: 403    lr: 1.6000000000000003e-05     evaluation reward: 7.57\n",
            "episode: 1662   score: 4.0   memory length: 393330   epsilon: 0.4192046200103433    steps: 259    lr: 1.6000000000000003e-05     evaluation reward: 7.57\n",
            "episode: 1663   score: 10.0   memory length: 393845   epsilon: 0.41818492001033686    steps: 515    lr: 1.6000000000000003e-05     evaluation reward: 7.61\n",
            "episode: 1664   score: 15.0   memory length: 394438   epsilon: 0.41701078001032943    steps: 593    lr: 1.6000000000000003e-05     evaluation reward: 7.69\n",
            "episode: 1665   score: 7.0   memory length: 394824   epsilon: 0.4162465000103246    steps: 386    lr: 1.6000000000000003e-05     evaluation reward: 7.68\n",
            "episode: 1666   score: 8.0   memory length: 395298   epsilon: 0.41530798001031866    steps: 474    lr: 1.6000000000000003e-05     evaluation reward: 7.69\n",
            "episode: 1667   score: 6.0   memory length: 395637   epsilon: 0.4146367600103144    steps: 339    lr: 1.6000000000000003e-05     evaluation reward: 7.66\n",
            "episode: 1668   score: 9.0   memory length: 396122   epsilon: 0.41367646001030833    steps: 485    lr: 1.6000000000000003e-05     evaluation reward: 7.71\n",
            "episode: 1669   score: 8.0   memory length: 396571   epsilon: 0.4127874400103027    steps: 449    lr: 1.6000000000000003e-05     evaluation reward: 7.7\n",
            "episode: 1670   score: 5.0   memory length: 396875   epsilon: 0.4121855200102989    steps: 304    lr: 1.6000000000000003e-05     evaluation reward: 7.68\n",
            "episode: 1671   score: 11.0   memory length: 397387   epsilon: 0.4111717600102925    steps: 512    lr: 1.6000000000000003e-05     evaluation reward: 7.75\n",
            "episode: 1672   score: 8.0   memory length: 397770   epsilon: 0.4104134200102877    steps: 383    lr: 1.6000000000000003e-05     evaluation reward: 7.78\n",
            "episode: 1673   score: 6.0   memory length: 398145   epsilon: 0.409670920010283    steps: 375    lr: 1.6000000000000003e-05     evaluation reward: 7.77\n",
            "episode: 1674   score: 11.0   memory length: 398680   epsilon: 0.4086116200102763    steps: 535    lr: 1.6000000000000003e-05     evaluation reward: 7.83\n",
            "episode: 1675   score: 11.0   memory length: 399170   epsilon: 0.40764142001027015    steps: 490    lr: 1.6000000000000003e-05     evaluation reward: 7.86\n",
            "episode: 1676   score: 4.0   memory length: 399465   epsilon: 0.40705732001026645    steps: 295    lr: 1.6000000000000003e-05     evaluation reward: 7.86\n",
            "episode: 1677   score: 10.0   memory length: 400001   epsilon: 0.40599604001025974    steps: 536    lr: 6.400000000000001e-06     evaluation reward: 7.87\n",
            "episode: 1678   score: 7.0   memory length: 400347   epsilon: 0.4053109600102554    steps: 346    lr: 6.400000000000001e-06     evaluation reward: 7.9\n",
            "episode: 1679   score: 7.0   memory length: 400711   epsilon: 0.40459024001025085    steps: 364    lr: 6.400000000000001e-06     evaluation reward: 7.9\n",
            "episode: 1680   score: 4.0   memory length: 400970   epsilon: 0.4040774200102476    steps: 259    lr: 6.400000000000001e-06     evaluation reward: 7.88\n",
            "episode: 1681   score: 5.0   memory length: 401278   epsilon: 0.40346758001024374    steps: 308    lr: 6.400000000000001e-06     evaluation reward: 7.87\n",
            "episode: 1682   score: 12.0   memory length: 401840   epsilon: 0.4023548200102367    steps: 562    lr: 6.400000000000001e-06     evaluation reward: 7.9\n",
            "episode: 1683   score: 8.0   memory length: 402283   epsilon: 0.40147768001023115    steps: 443    lr: 6.400000000000001e-06     evaluation reward: 7.93\n",
            "episode: 1684   score: 8.0   memory length: 402740   epsilon: 0.4005728200102254    steps: 457    lr: 6.400000000000001e-06     evaluation reward: 7.94\n",
            "episode: 1685   score: 5.0   memory length: 403029   epsilon: 0.4000006000102218    steps: 289    lr: 6.400000000000001e-06     evaluation reward: 7.95\n",
            "episode: 1686   score: 8.0   memory length: 403427   epsilon: 0.3992125600102168    steps: 398    lr: 6.400000000000001e-06     evaluation reward: 7.95\n",
            "episode: 1687   score: 5.0   memory length: 403734   epsilon: 0.398604700010213    steps: 307    lr: 6.400000000000001e-06     evaluation reward: 7.95\n",
            "episode: 1688   score: 5.0   memory length: 404057   epsilon: 0.39796516001020893    steps: 323    lr: 6.400000000000001e-06     evaluation reward: 7.95\n",
            "episode: 1689   score: 3.0   memory length: 404287   epsilon: 0.39750976001020605    steps: 230    lr: 6.400000000000001e-06     evaluation reward: 7.91\n",
            "episode: 1690   score: 5.0   memory length: 404596   epsilon: 0.3968979400102022    steps: 309    lr: 6.400000000000001e-06     evaluation reward: 7.9\n",
            "episode: 1691   score: 9.0   memory length: 405054   epsilon: 0.39599110001019644    steps: 458    lr: 6.400000000000001e-06     evaluation reward: 7.95\n",
            "episode: 1692   score: 11.0   memory length: 405563   epsilon: 0.39498328001019006    steps: 509    lr: 6.400000000000001e-06     evaluation reward: 7.98\n",
            "episode: 1693   score: 7.0   memory length: 405913   epsilon: 0.3942902800101857    steps: 350    lr: 6.400000000000001e-06     evaluation reward: 7.96\n",
            "episode: 1694   score: 22.0   memory length: 406632   epsilon: 0.39286666001017667    steps: 719    lr: 6.400000000000001e-06     evaluation reward: 8.11\n",
            "episode: 1695   score: 10.0   memory length: 407114   epsilon: 0.39191230001017063    steps: 482    lr: 6.400000000000001e-06     evaluation reward: 8.16\n",
            "episode: 1696   score: 13.0   memory length: 407759   epsilon: 0.39063520001016255    steps: 645    lr: 6.400000000000001e-06     evaluation reward: 8.23\n",
            "episode: 1697   score: 5.0   memory length: 408064   epsilon: 0.39003130001015873    steps: 305    lr: 6.400000000000001e-06     evaluation reward: 8.21\n",
            "episode: 1698   score: 6.0   memory length: 408442   epsilon: 0.389282860010154    steps: 378    lr: 6.400000000000001e-06     evaluation reward: 8.19\n",
            "episode: 1699   score: 10.0   memory length: 408917   epsilon: 0.38834236001014805    steps: 475    lr: 6.400000000000001e-06     evaluation reward: 8.23\n",
            "episode: 1700   score: 7.0   memory length: 409309   epsilon: 0.38756620001014314    steps: 392    lr: 6.400000000000001e-06     evaluation reward: 8.22\n",
            "episode: 1701   score: 4.0   memory length: 409571   epsilon: 0.38704744001013985    steps: 262    lr: 6.400000000000001e-06     evaluation reward: 8.17\n",
            "episode: 1702   score: 9.0   memory length: 410036   epsilon: 0.38612674001013403    steps: 465    lr: 6.400000000000001e-06     evaluation reward: 8.17\n",
            "episode: 1703   score: 8.0   memory length: 410432   epsilon: 0.38534266001012907    steps: 396    lr: 6.400000000000001e-06     evaluation reward: 8.18\n",
            "episode: 1704   score: 5.0   memory length: 410721   epsilon: 0.38477044001012545    steps: 289    lr: 6.400000000000001e-06     evaluation reward: 8.19\n",
            "episode: 1705   score: 5.0   memory length: 411014   epsilon: 0.3841903000101218    steps: 293    lr: 6.400000000000001e-06     evaluation reward: 8.11\n",
            "episode: 1706   score: 11.0   memory length: 411425   epsilon: 0.38337652001011663    steps: 411    lr: 6.400000000000001e-06     evaluation reward: 8.12\n",
            "episode: 1707   score: 15.0   memory length: 411952   epsilon: 0.38233306001011    steps: 527    lr: 6.400000000000001e-06     evaluation reward: 8.17\n",
            "episode: 1708   score: 9.0   memory length: 412437   epsilon: 0.38137276001010395    steps: 485    lr: 6.400000000000001e-06     evaluation reward: 8.19\n",
            "episode: 1709   score: 7.0   memory length: 412822   epsilon: 0.3806104600100991    steps: 385    lr: 6.400000000000001e-06     evaluation reward: 8.16\n",
            "episode: 1710   score: 7.0   memory length: 413210   epsilon: 0.37984222001009427    steps: 388    lr: 6.400000000000001e-06     evaluation reward: 8.19\n",
            "episode: 1711   score: 6.0   memory length: 413551   epsilon: 0.37916704001009    steps: 341    lr: 6.400000000000001e-06     evaluation reward: 8.17\n",
            "episode: 1712   score: 9.0   memory length: 413989   epsilon: 0.3782998000100845    steps: 438    lr: 6.400000000000001e-06     evaluation reward: 8.18\n",
            "episode: 1713   score: 7.0   memory length: 414394   epsilon: 0.37749790001007943    steps: 405    lr: 6.400000000000001e-06     evaluation reward: 8.17\n",
            "episode: 1714   score: 5.0   memory length: 414667   epsilon: 0.376957360010076    steps: 273    lr: 6.400000000000001e-06     evaluation reward: 8.07\n",
            "episode: 1715   score: 9.0   memory length: 415123   epsilon: 0.3760544800100703    steps: 456    lr: 6.400000000000001e-06     evaluation reward: 8.08\n",
            "episode: 1716   score: 9.0   memory length: 415615   epsilon: 0.37508032001006414    steps: 492    lr: 6.400000000000001e-06     evaluation reward: 8.03\n",
            "episode: 1717   score: 8.0   memory length: 416019   epsilon: 0.3742804000100591    steps: 404    lr: 6.400000000000001e-06     evaluation reward: 8.05\n",
            "episode: 1718   score: 9.0   memory length: 416429   epsilon: 0.37346860001005394    steps: 410    lr: 6.400000000000001e-06     evaluation reward: 8.05\n",
            "episode: 1719   score: 8.0   memory length: 416863   epsilon: 0.3726092800100485    steps: 434    lr: 6.400000000000001e-06     evaluation reward: 8.06\n",
            "episode: 1720   score: 11.0   memory length: 417303   epsilon: 0.371738080010043    steps: 440    lr: 6.400000000000001e-06     evaluation reward: 8.09\n",
            "episode: 1721   score: 7.0   memory length: 417675   epsilon: 0.37100152001003833    steps: 372    lr: 6.400000000000001e-06     evaluation reward: 8.1\n",
            "episode: 1722   score: 12.0   memory length: 418213   epsilon: 0.3699362800100316    steps: 538    lr: 6.400000000000001e-06     evaluation reward: 8.15\n",
            "episode: 1723   score: 6.0   memory length: 418574   epsilon: 0.36922150001002707    steps: 361    lr: 6.400000000000001e-06     evaluation reward: 8.13\n",
            "episode: 1724   score: 10.0   memory length: 419096   epsilon: 0.36818794001002053    steps: 522    lr: 6.400000000000001e-06     evaluation reward: 8.13\n",
            "episode: 1725   score: 7.0   memory length: 419453   epsilon: 0.36748108001001606    steps: 357    lr: 6.400000000000001e-06     evaluation reward: 8.14\n",
            "episode: 1726   score: 11.0   memory length: 420055   epsilon: 0.3662891200100085    steps: 602    lr: 6.400000000000001e-06     evaluation reward: 8.14\n",
            "episode: 1727   score: 7.0   memory length: 420465   epsilon: 0.3654773200100034    steps: 410    lr: 6.400000000000001e-06     evaluation reward: 8.11\n",
            "episode: 1728   score: 3.0   memory length: 420676   epsilon: 0.36505954001000074    steps: 211    lr: 6.400000000000001e-06     evaluation reward: 8.04\n",
            "episode: 1729   score: 10.0   memory length: 421204   epsilon: 0.3640141000099941    steps: 528    lr: 6.400000000000001e-06     evaluation reward: 8.07\n",
            "episode: 1730   score: 12.0   memory length: 421707   epsilon: 0.3630181600099878    steps: 503    lr: 6.400000000000001e-06     evaluation reward: 8.11\n",
            "episode: 1731   score: 6.0   memory length: 422045   epsilon: 0.3623489200099836    steps: 338    lr: 6.400000000000001e-06     evaluation reward: 8.1\n",
            "episode: 1732   score: 8.0   memory length: 422503   epsilon: 0.36144208000997785    steps: 458    lr: 6.400000000000001e-06     evaluation reward: 8.1\n",
            "episode: 1733   score: 5.0   memory length: 422830   epsilon: 0.36079462000997375    steps: 327    lr: 6.400000000000001e-06     evaluation reward: 8.07\n",
            "episode: 1734   score: 10.0   memory length: 423330   epsilon: 0.3598046200099675    steps: 500    lr: 6.400000000000001e-06     evaluation reward: 7.98\n",
            "episode: 1735   score: 7.0   memory length: 423718   epsilon: 0.35903638000996263    steps: 388    lr: 6.400000000000001e-06     evaluation reward: 7.96\n",
            "episode: 1736   score: 7.0   memory length: 424104   epsilon: 0.3582721000099578    steps: 386    lr: 6.400000000000001e-06     evaluation reward: 7.97\n",
            "episode: 1737   score: 9.0   memory length: 424561   epsilon: 0.35736724000995207    steps: 457    lr: 6.400000000000001e-06     evaluation reward: 7.97\n",
            "episode: 1738   score: 10.0   memory length: 425081   epsilon: 0.35633764000994556    steps: 520    lr: 6.400000000000001e-06     evaluation reward: 8.01\n",
            "episode: 1739   score: 9.0   memory length: 425581   epsilon: 0.3553476400099393    steps: 500    lr: 6.400000000000001e-06     evaluation reward: 8.04\n",
            "episode: 1740   score: 6.0   memory length: 425939   epsilon: 0.3546388000099348    steps: 358    lr: 6.400000000000001e-06     evaluation reward: 8.01\n",
            "episode: 1741   score: 11.0   memory length: 426406   epsilon: 0.35371414000992896    steps: 467    lr: 6.400000000000001e-06     evaluation reward: 8.04\n",
            "episode: 1742   score: 6.0   memory length: 426781   epsilon: 0.35297164000992426    steps: 375    lr: 6.400000000000001e-06     evaluation reward: 8.05\n",
            "episode: 1743   score: 11.0   memory length: 427301   epsilon: 0.35194204000991774    steps: 520    lr: 6.400000000000001e-06     evaluation reward: 8.06\n",
            "episode: 1744   score: 4.0   memory length: 427558   epsilon: 0.3514331800099145    steps: 257    lr: 6.400000000000001e-06     evaluation reward: 8.0\n",
            "episode: 1745   score: 9.0   memory length: 428022   epsilon: 0.3505144600099087    steps: 464    lr: 6.400000000000001e-06     evaluation reward: 8.04\n",
            "episode: 1746   score: 15.0   memory length: 428707   epsilon: 0.34915816000990013    steps: 685    lr: 6.400000000000001e-06     evaluation reward: 8.15\n",
            "episode: 1747   score: 8.0   memory length: 429107   epsilon: 0.3483661600098951    steps: 400    lr: 6.400000000000001e-06     evaluation reward: 8.15\n",
            "episode: 1748   score: 13.0   memory length: 429766   epsilon: 0.34706134000988687    steps: 659    lr: 6.400000000000001e-06     evaluation reward: 8.17\n",
            "episode: 1749   score: 6.0   memory length: 430121   epsilon: 0.3463584400098824    steps: 355    lr: 6.400000000000001e-06     evaluation reward: 8.1\n",
            "episode: 1750   score: 7.0   memory length: 430549   epsilon: 0.34551100000987706    steps: 428    lr: 6.400000000000001e-06     evaluation reward: 8.08\n",
            "episode: 1751   score: 24.0   memory length: 431077   epsilon: 0.34446556000987044    steps: 528    lr: 6.400000000000001e-06     evaluation reward: 8.21\n",
            "episode: 1752   score: 13.0   memory length: 431684   epsilon: 0.34326370000986284    steps: 607    lr: 6.400000000000001e-06     evaluation reward: 8.22\n",
            "episode: 1753   score: 11.0   memory length: 432208   epsilon: 0.3422261800098563    steps: 524    lr: 6.400000000000001e-06     evaluation reward: 8.28\n",
            "episode: 1754   score: 7.0   memory length: 432572   epsilon: 0.3415054600098517    steps: 364    lr: 6.400000000000001e-06     evaluation reward: 8.29\n",
            "episode: 1755   score: 17.0   memory length: 433193   epsilon: 0.34027588000984393    steps: 621    lr: 6.400000000000001e-06     evaluation reward: 8.39\n",
            "episode: 1756   score: 9.0   memory length: 433630   epsilon: 0.33941062000983846    steps: 437    lr: 6.400000000000001e-06     evaluation reward: 8.4\n",
            "episode: 1757   score: 12.0   memory length: 434208   epsilon: 0.3382661800098312    steps: 578    lr: 6.400000000000001e-06     evaluation reward: 8.47\n",
            "episode: 1758   score: 16.0   memory length: 434858   epsilon: 0.3369791800098231    steps: 650    lr: 6.400000000000001e-06     evaluation reward: 8.55\n",
            "episode: 1759   score: 5.0   memory length: 435149   epsilon: 0.33640300000981943    steps: 291    lr: 6.400000000000001e-06     evaluation reward: 8.57\n",
            "episode: 1760   score: 12.0   memory length: 435683   epsilon: 0.33534568000981274    steps: 534    lr: 6.400000000000001e-06     evaluation reward: 8.61\n",
            "episode: 1761   score: 4.0   memory length: 435922   epsilon: 0.33487246000980975    steps: 239    lr: 6.400000000000001e-06     evaluation reward: 8.58\n",
            "episode: 1762   score: 11.0   memory length: 436503   epsilon: 0.33372208000980247    steps: 581    lr: 6.400000000000001e-06     evaluation reward: 8.65\n",
            "episode: 1763   score: 8.0   memory length: 436905   epsilon: 0.33292612000979743    steps: 402    lr: 6.400000000000001e-06     evaluation reward: 8.63\n",
            "episode: 1764   score: 9.0   memory length: 437390   epsilon: 0.33196582000979136    steps: 485    lr: 6.400000000000001e-06     evaluation reward: 8.57\n",
            "episode: 1765   score: 9.0   memory length: 437891   epsilon: 0.3309738400097851    steps: 501    lr: 6.400000000000001e-06     evaluation reward: 8.59\n",
            "episode: 1766   score: 9.0   memory length: 438360   epsilon: 0.3300452200097792    steps: 469    lr: 6.400000000000001e-06     evaluation reward: 8.6\n",
            "episode: 1767   score: 6.0   memory length: 438737   epsilon: 0.3292987600097745    steps: 377    lr: 6.400000000000001e-06     evaluation reward: 8.6\n",
            "episode: 1768   score: 7.0   memory length: 439146   epsilon: 0.32848894000976936    steps: 409    lr: 6.400000000000001e-06     evaluation reward: 8.58\n",
            "episode: 1769   score: 7.0   memory length: 439571   epsilon: 0.32764744000976403    steps: 425    lr: 6.400000000000001e-06     evaluation reward: 8.57\n",
            "episode: 1770   score: 5.0   memory length: 439878   epsilon: 0.3270395800097602    steps: 307    lr: 6.400000000000001e-06     evaluation reward: 8.57\n",
            "episode: 1771   score: 12.0   memory length: 440476   epsilon: 0.3258555400097527    steps: 598    lr: 6.400000000000001e-06     evaluation reward: 8.58\n",
            "episode: 1772   score: 8.0   memory length: 440899   epsilon: 0.3250180000097474    steps: 423    lr: 6.400000000000001e-06     evaluation reward: 8.58\n",
            "episode: 1773   score: 6.0   memory length: 441203   epsilon: 0.3244160800097436    steps: 304    lr: 6.400000000000001e-06     evaluation reward: 8.58\n",
            "episode: 1774   score: 6.0   memory length: 441562   epsilon: 0.3237052600097391    steps: 359    lr: 6.400000000000001e-06     evaluation reward: 8.53\n",
            "episode: 1775   score: 9.0   memory length: 441904   epsilon: 0.3230281000097348    steps: 342    lr: 6.400000000000001e-06     evaluation reward: 8.51\n",
            "episode: 1776   score: 17.0   memory length: 442413   epsilon: 0.32202028000972843    steps: 509    lr: 6.400000000000001e-06     evaluation reward: 8.64\n",
            "episode: 1777   score: 8.0   memory length: 442820   epsilon: 0.32121442000972333    steps: 407    lr: 6.400000000000001e-06     evaluation reward: 8.62\n",
            "episode: 1778   score: 10.0   memory length: 443317   epsilon: 0.3202303600097171    steps: 497    lr: 6.400000000000001e-06     evaluation reward: 8.65\n",
            "episode: 1779   score: 26.0   memory length: 443970   epsilon: 0.3189374200097089    steps: 653    lr: 6.400000000000001e-06     evaluation reward: 8.84\n",
            "episode: 1780   score: 8.0   memory length: 444422   epsilon: 0.31804246000970327    steps: 452    lr: 6.400000000000001e-06     evaluation reward: 8.88\n",
            "episode: 1781   score: 14.0   memory length: 445007   epsilon: 0.31688416000969594    steps: 585    lr: 6.400000000000001e-06     evaluation reward: 8.97\n",
            "episode: 1782   score: 14.0   memory length: 445653   epsilon: 0.31560508000968784    steps: 646    lr: 6.400000000000001e-06     evaluation reward: 8.99\n",
            "episode: 1783   score: 13.0   memory length: 446154   epsilon: 0.31461310000968157    steps: 501    lr: 6.400000000000001e-06     evaluation reward: 9.04\n",
            "episode: 1784   score: 6.0   memory length: 446491   epsilon: 0.31394584000967735    steps: 337    lr: 6.400000000000001e-06     evaluation reward: 9.02\n",
            "episode: 1785   score: 8.0   memory length: 446967   epsilon: 0.3130033600096714    steps: 476    lr: 6.400000000000001e-06     evaluation reward: 9.05\n",
            "episode: 1786   score: 6.0   memory length: 447324   epsilon: 0.3122965000096669    steps: 357    lr: 6.400000000000001e-06     evaluation reward: 9.03\n",
            "episode: 1787   score: 9.0   memory length: 447750   epsilon: 0.3114530200096616    steps: 426    lr: 6.400000000000001e-06     evaluation reward: 9.07\n",
            "episode: 1788   score: 9.0   memory length: 448077   epsilon: 0.3108055600096575    steps: 327    lr: 6.400000000000001e-06     evaluation reward: 9.11\n",
            "episode: 1789   score: 13.0   memory length: 448582   epsilon: 0.30980566000965115    steps: 505    lr: 6.400000000000001e-06     evaluation reward: 9.21\n",
            "episode: 1790   score: 6.0   memory length: 448960   epsilon: 0.3090572200096464    steps: 378    lr: 6.400000000000001e-06     evaluation reward: 9.22\n",
            "episode: 1791   score: 14.0   memory length: 449594   epsilon: 0.3078019000096385    steps: 634    lr: 6.400000000000001e-06     evaluation reward: 9.27\n",
            "episode: 1792   score: 20.0   memory length: 450275   epsilon: 0.30645352000962994    steps: 681    lr: 6.400000000000001e-06     evaluation reward: 9.36\n",
            "episode: 1793   score: 5.0   memory length: 450581   epsilon: 0.3058476400096261    steps: 306    lr: 6.400000000000001e-06     evaluation reward: 9.34\n",
            "episode: 1794   score: 7.0   memory length: 450965   epsilon: 0.3050873200096213    steps: 384    lr: 6.400000000000001e-06     evaluation reward: 9.19\n",
            "episode: 1795   score: 6.0   memory length: 451336   epsilon: 0.30435274000961665    steps: 371    lr: 6.400000000000001e-06     evaluation reward: 9.15\n",
            "episode: 1796   score: 6.0   memory length: 451697   epsilon: 0.30363796000961213    steps: 361    lr: 6.400000000000001e-06     evaluation reward: 9.08\n",
            "episode: 1797   score: 10.0   memory length: 452247   epsilon: 0.30254896000960524    steps: 550    lr: 6.400000000000001e-06     evaluation reward: 9.13\n",
            "episode: 1798   score: 14.0   memory length: 452841   epsilon: 0.3013728400095978    steps: 594    lr: 6.400000000000001e-06     evaluation reward: 9.21\n",
            "episode: 1799   score: 6.0   memory length: 453185   epsilon: 0.3006917200095935    steps: 344    lr: 6.400000000000001e-06     evaluation reward: 9.17\n",
            "episode: 1800   score: 8.0   memory length: 453595   epsilon: 0.29987992000958835    steps: 410    lr: 6.400000000000001e-06     evaluation reward: 9.18\n",
            "episode: 1801   score: 9.0   memory length: 454070   epsilon: 0.2989394200095824    steps: 475    lr: 6.400000000000001e-06     evaluation reward: 9.23\n",
            "episode: 1802   score: 14.0   memory length: 454611   epsilon: 0.2978682400095756    steps: 541    lr: 6.400000000000001e-06     evaluation reward: 9.28\n",
            "episode: 1803   score: 11.0   memory length: 455127   epsilon: 0.29684656000956916    steps: 516    lr: 6.400000000000001e-06     evaluation reward: 9.31\n",
            "episode: 1804   score: 13.0   memory length: 455707   epsilon: 0.2956981600095619    steps: 580    lr: 6.400000000000001e-06     evaluation reward: 9.39\n",
            "episode: 1805   score: 6.0   memory length: 456100   epsilon: 0.29492002000955697    steps: 393    lr: 6.400000000000001e-06     evaluation reward: 9.4\n",
            "episode: 1806   score: 8.0   memory length: 456537   epsilon: 0.2940547600095515    steps: 437    lr: 6.400000000000001e-06     evaluation reward: 9.37\n",
            "episode: 1807   score: 11.0   memory length: 457033   epsilon: 0.2930726800095453    steps: 496    lr: 6.400000000000001e-06     evaluation reward: 9.33\n",
            "episode: 1808   score: 9.0   memory length: 457504   epsilon: 0.2921401000095394    steps: 471    lr: 6.400000000000001e-06     evaluation reward: 9.33\n",
            "episode: 1809   score: 8.0   memory length: 457938   epsilon: 0.29128078000953395    steps: 434    lr: 6.400000000000001e-06     evaluation reward: 9.34\n",
            "episode: 1810   score: 8.0   memory length: 458409   epsilon: 0.29034820000952805    steps: 471    lr: 6.400000000000001e-06     evaluation reward: 9.35\n",
            "episode: 1811   score: 6.0   memory length: 458731   epsilon: 0.289710640009524    steps: 322    lr: 6.400000000000001e-06     evaluation reward: 9.35\n",
            "episode: 1812   score: 9.0   memory length: 459229   epsilon: 0.2887246000095178    steps: 498    lr: 6.400000000000001e-06     evaluation reward: 9.35\n",
            "episode: 1813   score: 4.0   memory length: 459505   epsilon: 0.2881781200095143    steps: 276    lr: 6.400000000000001e-06     evaluation reward: 9.32\n",
            "episode: 1814   score: 5.0   memory length: 459818   epsilon: 0.2875583800095104    steps: 313    lr: 6.400000000000001e-06     evaluation reward: 9.32\n",
            "episode: 1815   score: 13.0   memory length: 460261   epsilon: 0.28668124000950485    steps: 443    lr: 6.400000000000001e-06     evaluation reward: 9.36\n",
            "episode: 1816   score: 7.0   memory length: 460664   epsilon: 0.2858833000094998    steps: 403    lr: 6.400000000000001e-06     evaluation reward: 9.34\n",
            "episode: 1817   score: 13.0   memory length: 461268   epsilon: 0.28468738000949223    steps: 604    lr: 6.400000000000001e-06     evaluation reward: 9.39\n",
            "episode: 1818   score: 11.0   memory length: 461808   epsilon: 0.28361818000948547    steps: 540    lr: 6.400000000000001e-06     evaluation reward: 9.41\n",
            "episode: 1819   score: 7.0   memory length: 462179   epsilon: 0.2828836000094808    steps: 371    lr: 6.400000000000001e-06     evaluation reward: 9.4\n",
            "episode: 1820   score: 9.0   memory length: 462650   epsilon: 0.2819510200094749    steps: 471    lr: 6.400000000000001e-06     evaluation reward: 9.38\n",
            "episode: 1821   score: 5.0   memory length: 462937   epsilon: 0.2813827600094713    steps: 287    lr: 6.400000000000001e-06     evaluation reward: 9.36\n",
            "episode: 1822   score: 8.0   memory length: 463408   epsilon: 0.2804501800094654    steps: 471    lr: 6.400000000000001e-06     evaluation reward: 9.32\n",
            "episode: 1823   score: 5.0   memory length: 463728   epsilon: 0.2798165800094614    steps: 320    lr: 6.400000000000001e-06     evaluation reward: 9.31\n",
            "episode: 1824   score: 8.0   memory length: 464203   epsilon: 0.27887608000945546    steps: 475    lr: 6.400000000000001e-06     evaluation reward: 9.29\n",
            "episode: 1825   score: 13.0   memory length: 464771   epsilon: 0.27775144000944835    steps: 568    lr: 6.400000000000001e-06     evaluation reward: 9.35\n",
            "episode: 1826   score: 9.0   memory length: 465189   epsilon: 0.2769238000094431    steps: 418    lr: 6.400000000000001e-06     evaluation reward: 9.33\n",
            "episode: 1827   score: 6.0   memory length: 465541   epsilon: 0.2762268400094387    steps: 352    lr: 6.400000000000001e-06     evaluation reward: 9.32\n",
            "episode: 1828   score: 11.0   memory length: 466128   epsilon: 0.27506458000943135    steps: 587    lr: 6.400000000000001e-06     evaluation reward: 9.4\n",
            "episode: 1829   score: 12.0   memory length: 466602   epsilon: 0.2741260600094254    steps: 474    lr: 6.400000000000001e-06     evaluation reward: 9.42\n",
            "episode: 1830   score: 5.0   memory length: 466889   epsilon: 0.2735578000094218    steps: 287    lr: 6.400000000000001e-06     evaluation reward: 9.35\n",
            "episode: 1831   score: 9.0   memory length: 467355   epsilon: 0.272635120009416    steps: 466    lr: 6.400000000000001e-06     evaluation reward: 9.38\n",
            "episode: 1832   score: 8.0   memory length: 467788   epsilon: 0.27177778000941055    steps: 433    lr: 6.400000000000001e-06     evaluation reward: 9.38\n",
            "episode: 1833   score: 13.0   memory length: 468296   epsilon: 0.2707719400094042    steps: 508    lr: 6.400000000000001e-06     evaluation reward: 9.46\n",
            "episode: 1834   score: 6.0   memory length: 468618   epsilon: 0.27013438000940015    steps: 322    lr: 6.400000000000001e-06     evaluation reward: 9.42\n",
            "episode: 1835   score: 14.0   memory length: 469174   epsilon: 0.2690335000093932    steps: 556    lr: 6.400000000000001e-06     evaluation reward: 9.49\n",
            "episode: 1836   score: 9.0   memory length: 469657   epsilon: 0.26807716000938714    steps: 483    lr: 6.400000000000001e-06     evaluation reward: 9.51\n",
            "episode: 1837   score: 5.0   memory length: 469967   epsilon: 0.26746336000938326    steps: 310    lr: 6.400000000000001e-06     evaluation reward: 9.47\n",
            "episode: 1838   score: 8.0   memory length: 470410   epsilon: 0.2665862200093777    steps: 443    lr: 6.400000000000001e-06     evaluation reward: 9.45\n",
            "episode: 1839   score: 7.0   memory length: 470840   epsilon: 0.2657348200093723    steps: 430    lr: 6.400000000000001e-06     evaluation reward: 9.43\n",
            "episode: 1840   score: 7.0   memory length: 471225   epsilon: 0.2649725200093675    steps: 385    lr: 6.400000000000001e-06     evaluation reward: 9.44\n",
            "episode: 1841   score: 6.0   memory length: 471541   epsilon: 0.26434684000936354    steps: 316    lr: 6.400000000000001e-06     evaluation reward: 9.39\n",
            "episode: 1842   score: 7.0   memory length: 471875   epsilon: 0.26368552000935935    steps: 334    lr: 6.400000000000001e-06     evaluation reward: 9.4\n",
            "episode: 1843   score: 16.0   memory length: 472518   epsilon: 0.2624123800093513    steps: 643    lr: 6.400000000000001e-06     evaluation reward: 9.45\n",
            "episode: 1844   score: 11.0   memory length: 473081   epsilon: 0.26129764000934425    steps: 563    lr: 6.400000000000001e-06     evaluation reward: 9.52\n",
            "episode: 1845   score: 16.0   memory length: 473700   epsilon: 0.2600720200093365    steps: 619    lr: 6.400000000000001e-06     evaluation reward: 9.59\n",
            "episode: 1846   score: 15.0   memory length: 474384   epsilon: 0.2587177000093279    steps: 684    lr: 6.400000000000001e-06     evaluation reward: 9.59\n",
            "episode: 1847   score: 8.0   memory length: 474808   epsilon: 0.2578781800093226    steps: 424    lr: 6.400000000000001e-06     evaluation reward: 9.59\n",
            "episode: 1848   score: 8.0   memory length: 475245   epsilon: 0.25701292000931714    steps: 437    lr: 6.400000000000001e-06     evaluation reward: 9.54\n",
            "episode: 1849   score: 7.0   memory length: 475620   epsilon: 0.25627042000931244    steps: 375    lr: 6.400000000000001e-06     evaluation reward: 9.55\n",
            "episode: 1850   score: 4.0   memory length: 475895   epsilon: 0.255725920009309    steps: 275    lr: 6.400000000000001e-06     evaluation reward: 9.52\n",
            "episode: 1851   score: 10.0   memory length: 476408   epsilon: 0.25471018000930257    steps: 513    lr: 6.400000000000001e-06     evaluation reward: 9.38\n",
            "episode: 1852   score: 14.0   memory length: 476922   epsilon: 0.25369246000929613    steps: 514    lr: 6.400000000000001e-06     evaluation reward: 9.39\n",
            "episode: 1853   score: 9.0   memory length: 477415   epsilon: 0.25271632000928995    steps: 493    lr: 6.400000000000001e-06     evaluation reward: 9.37\n",
            "episode: 1854   score: 11.0   memory length: 477934   epsilon: 0.25168870000928345    steps: 519    lr: 6.400000000000001e-06     evaluation reward: 9.41\n",
            "episode: 1855   score: 13.0   memory length: 478269   epsilon: 0.25102540000927925    steps: 335    lr: 6.400000000000001e-06     evaluation reward: 9.37\n",
            "episode: 1856   score: 13.0   memory length: 478712   epsilon: 0.2501482600092737    steps: 443    lr: 6.400000000000001e-06     evaluation reward: 9.41\n",
            "episode: 1857   score: 9.0   memory length: 479152   epsilon: 0.2492770600092682    steps: 440    lr: 6.400000000000001e-06     evaluation reward: 9.38\n",
            "episode: 1858   score: 7.0   memory length: 479519   epsilon: 0.2485504000092636    steps: 367    lr: 6.400000000000001e-06     evaluation reward: 9.29\n",
            "episode: 1859   score: 12.0   memory length: 480072   epsilon: 0.24745546000925667    steps: 553    lr: 6.400000000000001e-06     evaluation reward: 9.36\n",
            "episode: 1860   score: 8.0   memory length: 480476   epsilon: 0.2466555400092516    steps: 404    lr: 6.400000000000001e-06     evaluation reward: 9.32\n",
            "episode: 1861   score: 14.0   memory length: 481071   epsilon: 0.24547744000924415    steps: 595    lr: 6.400000000000001e-06     evaluation reward: 9.42\n",
            "episode: 1862   score: 4.0   memory length: 481333   epsilon: 0.24495868000924087    steps: 262    lr: 6.400000000000001e-06     evaluation reward: 9.35\n",
            "episode: 1863   score: 11.0   memory length: 481856   epsilon: 0.24392314000923432    steps: 523    lr: 6.400000000000001e-06     evaluation reward: 9.38\n",
            "episode: 1864   score: 10.0   memory length: 482347   epsilon: 0.24295096000922817    steps: 491    lr: 6.400000000000001e-06     evaluation reward: 9.39\n",
            "episode: 1865   score: 12.0   memory length: 482930   epsilon: 0.24179662000922086    steps: 583    lr: 6.400000000000001e-06     evaluation reward: 9.42\n",
            "episode: 1866   score: 7.0   memory length: 483299   epsilon: 0.24106600000921624    steps: 369    lr: 6.400000000000001e-06     evaluation reward: 9.4\n",
            "episode: 1867   score: 16.0   memory length: 483922   epsilon: 0.23983246000920844    steps: 623    lr: 6.400000000000001e-06     evaluation reward: 9.5\n",
            "episode: 1868   score: 12.0   memory length: 484509   epsilon: 0.23867020000920108    steps: 587    lr: 6.400000000000001e-06     evaluation reward: 9.55\n",
            "episode: 1869   score: 11.0   memory length: 485036   epsilon: 0.23762674000919448    steps: 527    lr: 6.400000000000001e-06     evaluation reward: 9.59\n",
            "episode: 1870   score: 7.0   memory length: 485420   epsilon: 0.23686642000918967    steps: 384    lr: 6.400000000000001e-06     evaluation reward: 9.61\n",
            "episode: 1871   score: 9.0   memory length: 485891   epsilon: 0.23593384000918377    steps: 471    lr: 6.400000000000001e-06     evaluation reward: 9.58\n",
            "episode: 1872   score: 9.0   memory length: 486369   epsilon: 0.23498740000917778    steps: 478    lr: 6.400000000000001e-06     evaluation reward: 9.59\n",
            "episode: 1873   score: 11.0   memory length: 486905   epsilon: 0.23392612000917107    steps: 536    lr: 6.400000000000001e-06     evaluation reward: 9.64\n",
            "episode: 1874   score: 5.0   memory length: 487197   epsilon: 0.2333479600091674    steps: 292    lr: 6.400000000000001e-06     evaluation reward: 9.63\n",
            "episode: 1875   score: 9.0   memory length: 487685   epsilon: 0.2323817200091613    steps: 488    lr: 6.400000000000001e-06     evaluation reward: 9.63\n",
            "episode: 1876   score: 10.0   memory length: 488183   epsilon: 0.23139568000915506    steps: 498    lr: 6.400000000000001e-06     evaluation reward: 9.56\n",
            "episode: 1877   score: 8.0   memory length: 488582   epsilon: 0.23060566000915006    steps: 399    lr: 6.400000000000001e-06     evaluation reward: 9.56\n",
            "episode: 1878   score: 8.0   memory length: 489003   epsilon: 0.22977208000914479    steps: 421    lr: 6.400000000000001e-06     evaluation reward: 9.54\n",
            "episode: 1879   score: 11.0   memory length: 489509   epsilon: 0.22877020000913845    steps: 506    lr: 6.400000000000001e-06     evaluation reward: 9.39\n",
            "episode: 1880   score: 9.0   memory length: 489978   epsilon: 0.22784158000913257    steps: 469    lr: 6.400000000000001e-06     evaluation reward: 9.4\n",
            "episode: 1881   score: 11.0   memory length: 490373   epsilon: 0.22705948000912762    steps: 395    lr: 6.400000000000001e-06     evaluation reward: 9.37\n",
            "episode: 1882   score: 7.0   memory length: 490801   epsilon: 0.22621204000912226    steps: 428    lr: 6.400000000000001e-06     evaluation reward: 9.3\n",
            "episode: 1883   score: 11.0   memory length: 491313   epsilon: 0.22519828000911585    steps: 512    lr: 6.400000000000001e-06     evaluation reward: 9.28\n",
            "episode: 1884   score: 13.0   memory length: 491925   epsilon: 0.22398652000910818    steps: 612    lr: 6.400000000000001e-06     evaluation reward: 9.35\n",
            "episode: 1885   score: 6.0   memory length: 492273   epsilon: 0.22329748000910382    steps: 348    lr: 6.400000000000001e-06     evaluation reward: 9.33\n",
            "episode: 1886   score: 11.0   memory length: 492751   epsilon: 0.22235104000909783    steps: 478    lr: 6.400000000000001e-06     evaluation reward: 9.38\n",
            "episode: 1887   score: 12.0   memory length: 493177   epsilon: 0.2215075600090925    steps: 426    lr: 6.400000000000001e-06     evaluation reward: 9.41\n",
            "episode: 1888   score: 8.0   memory length: 493567   epsilon: 0.2207353600090876    steps: 390    lr: 6.400000000000001e-06     evaluation reward: 9.4\n",
            "episode: 1889   score: 11.0   memory length: 494076   epsilon: 0.21972754000908123    steps: 509    lr: 6.400000000000001e-06     evaluation reward: 9.38\n",
            "episode: 1890   score: 7.0   memory length: 494447   epsilon: 0.2189929600090766    steps: 371    lr: 6.400000000000001e-06     evaluation reward: 9.39\n",
            "episode: 1891   score: 14.0   memory length: 495090   epsilon: 0.21771982000906853    steps: 643    lr: 6.400000000000001e-06     evaluation reward: 9.39\n",
            "episode: 1892   score: 5.0   memory length: 495397   epsilon: 0.2171119600090647    steps: 307    lr: 6.400000000000001e-06     evaluation reward: 9.24\n",
            "episode: 1893   score: 9.0   memory length: 495827   epsilon: 0.2162605600090593    steps: 430    lr: 6.400000000000001e-06     evaluation reward: 9.28\n",
            "episode: 1894   score: 15.0   memory length: 496432   epsilon: 0.21506266000905172    steps: 605    lr: 6.400000000000001e-06     evaluation reward: 9.36\n",
            "episode: 1895   score: 10.0   memory length: 496927   epsilon: 0.21408256000904552    steps: 495    lr: 6.400000000000001e-06     evaluation reward: 9.4\n",
            "episode: 1896   score: 11.0   memory length: 497436   epsilon: 0.21307474000903914    steps: 509    lr: 6.400000000000001e-06     evaluation reward: 9.45\n",
            "episode: 1897   score: 7.0   memory length: 497860   epsilon: 0.21223522000903383    steps: 424    lr: 6.400000000000001e-06     evaluation reward: 9.42\n",
            "episode: 1898   score: 3.0   memory length: 498088   epsilon: 0.21178378000903098    steps: 228    lr: 6.400000000000001e-06     evaluation reward: 9.31\n",
            "episode: 1899   score: 12.0   memory length: 498604   epsilon: 0.2107621000090245    steps: 516    lr: 6.400000000000001e-06     evaluation reward: 9.37\n",
            "episode: 1900   score: 10.0   memory length: 499092   epsilon: 0.2097958600090184    steps: 488    lr: 6.400000000000001e-06     evaluation reward: 9.39\n",
            "episode: 1901   score: 6.0   memory length: 499445   epsilon: 0.20909692000901398    steps: 353    lr: 6.400000000000001e-06     evaluation reward: 9.36\n",
            "episode: 1902   score: 7.0   memory length: 499852   epsilon: 0.20829106000900888    steps: 407    lr: 6.400000000000001e-06     evaluation reward: 9.29\n",
            "episode: 1903   score: 9.0   memory length: 500300   epsilon: 0.20740402000900326    steps: 448    lr: 2.560000000000001e-06     evaluation reward: 9.27\n",
            "episode: 1904   score: 9.0   memory length: 500745   epsilon: 0.2065229200089977    steps: 445    lr: 2.560000000000001e-06     evaluation reward: 9.23\n",
            "episode: 1905   score: 8.0   memory length: 501222   epsilon: 0.20557846000899171    steps: 477    lr: 2.560000000000001e-06     evaluation reward: 9.25\n",
            "episode: 1906   score: 10.0   memory length: 501703   epsilon: 0.2046260800089857    steps: 481    lr: 2.560000000000001e-06     evaluation reward: 9.27\n",
            "episode: 1907   score: 13.0   memory length: 502290   epsilon: 0.20346382000897834    steps: 587    lr: 2.560000000000001e-06     evaluation reward: 9.29\n",
            "episode: 1908   score: 11.0   memory length: 502845   epsilon: 0.20236492000897138    steps: 555    lr: 2.560000000000001e-06     evaluation reward: 9.31\n",
            "episode: 1909   score: 7.0   memory length: 503241   epsilon: 0.20158084000896642    steps: 396    lr: 2.560000000000001e-06     evaluation reward: 9.3\n",
            "episode: 1910   score: 11.0   memory length: 503765   epsilon: 0.20054332000895986    steps: 524    lr: 2.560000000000001e-06     evaluation reward: 9.33\n",
            "episode: 1911   score: 15.0   memory length: 504413   epsilon: 0.19926028000895174    steps: 648    lr: 2.560000000000001e-06     evaluation reward: 9.42\n",
            "episode: 1912   score: 13.0   memory length: 505089   epsilon: 0.19792180000894327    steps: 676    lr: 2.560000000000001e-06     evaluation reward: 9.46\n",
            "episode: 1913   score: 7.0   memory length: 505473   epsilon: 0.19716148000893846    steps: 384    lr: 2.560000000000001e-06     evaluation reward: 9.49\n",
            "episode: 1914   score: 5.0   memory length: 505763   epsilon: 0.19658728000893483    steps: 290    lr: 2.560000000000001e-06     evaluation reward: 9.49\n",
            "episode: 1915   score: 12.0   memory length: 506345   epsilon: 0.19543492000892754    steps: 582    lr: 2.560000000000001e-06     evaluation reward: 9.48\n",
            "episode: 1916   score: 8.0   memory length: 506728   epsilon: 0.19467658000892274    steps: 383    lr: 2.560000000000001e-06     evaluation reward: 9.49\n",
            "episode: 1917   score: 9.0   memory length: 507160   epsilon: 0.19382122000891733    steps: 432    lr: 2.560000000000001e-06     evaluation reward: 9.45\n",
            "episode: 1918   score: 9.0   memory length: 507591   epsilon: 0.19296784000891193    steps: 431    lr: 2.560000000000001e-06     evaluation reward: 9.43\n",
            "episode: 1919   score: 9.0   memory length: 508113   epsilon: 0.1919342800089054    steps: 522    lr: 2.560000000000001e-06     evaluation reward: 9.45\n",
            "episode: 1920   score: 16.0   memory length: 508722   epsilon: 0.19072846000889776    steps: 609    lr: 2.560000000000001e-06     evaluation reward: 9.52\n",
            "episode: 1921   score: 7.0   memory length: 509108   epsilon: 0.18996418000889292    steps: 386    lr: 2.560000000000001e-06     evaluation reward: 9.54\n",
            "episode: 1922   score: 14.0   memory length: 509674   epsilon: 0.18884350000888583    steps: 566    lr: 2.560000000000001e-06     evaluation reward: 9.6\n",
            "episode: 1923   score: 10.0   memory length: 510199   epsilon: 0.18780400000887926    steps: 525    lr: 2.560000000000001e-06     evaluation reward: 9.65\n",
            "episode: 1924   score: 12.0   memory length: 510781   epsilon: 0.18665164000887197    steps: 582    lr: 2.560000000000001e-06     evaluation reward: 9.69\n",
            "episode: 1925   score: 10.0   memory length: 511136   epsilon: 0.18594874000886752    steps: 355    lr: 2.560000000000001e-06     evaluation reward: 9.66\n",
            "episode: 1926   score: 7.0   memory length: 511537   epsilon: 0.1851547600088625    steps: 401    lr: 2.560000000000001e-06     evaluation reward: 9.64\n",
            "episode: 1927   score: 10.0   memory length: 512036   epsilon: 0.18416674000885624    steps: 499    lr: 2.560000000000001e-06     evaluation reward: 9.68\n",
            "episode: 1928   score: 13.0   memory length: 512530   epsilon: 0.18318862000885006    steps: 494    lr: 2.560000000000001e-06     evaluation reward: 9.7\n",
            "episode: 1929   score: 11.0   memory length: 513079   epsilon: 0.18210160000884318    steps: 549    lr: 2.560000000000001e-06     evaluation reward: 9.69\n",
            "episode: 1930   score: 9.0   memory length: 513517   epsilon: 0.1812343600088377    steps: 438    lr: 2.560000000000001e-06     evaluation reward: 9.73\n",
            "episode: 1931   score: 9.0   memory length: 514018   epsilon: 0.18024238000883142    steps: 501    lr: 2.560000000000001e-06     evaluation reward: 9.73\n",
            "episode: 1932   score: 11.0   memory length: 514520   epsilon: 0.17924842000882513    steps: 502    lr: 2.560000000000001e-06     evaluation reward: 9.76\n",
            "episode: 1933   score: 6.0   memory length: 514848   epsilon: 0.17859898000882102    steps: 328    lr: 2.560000000000001e-06     evaluation reward: 9.69\n",
            "episode: 1934   score: 7.0   memory length: 515294   epsilon: 0.17771590000881543    steps: 446    lr: 2.560000000000001e-06     evaluation reward: 9.7\n",
            "episode: 1935   score: 6.0   memory length: 515613   epsilon: 0.17708428000881143    steps: 319    lr: 2.560000000000001e-06     evaluation reward: 9.62\n",
            "episode: 1936   score: 10.0   memory length: 516120   epsilon: 0.17608042000880508    steps: 507    lr: 2.560000000000001e-06     evaluation reward: 9.63\n",
            "episode: 1937   score: 17.0   memory length: 516810   epsilon: 0.17471422000879644    steps: 690    lr: 2.560000000000001e-06     evaluation reward: 9.75\n",
            "episode: 1938   score: 12.0   memory length: 517312   epsilon: 0.17372026000879015    steps: 502    lr: 2.560000000000001e-06     evaluation reward: 9.79\n",
            "episode: 1939   score: 7.0   memory length: 517692   epsilon: 0.1729678600087854    steps: 380    lr: 2.560000000000001e-06     evaluation reward: 9.79\n",
            "episode: 1940   score: 13.0   memory length: 518320   epsilon: 0.17172442000877752    steps: 628    lr: 2.560000000000001e-06     evaluation reward: 9.85\n",
            "episode: 1941   score: 11.0   memory length: 518894   epsilon: 0.17058790000877033    steps: 574    lr: 2.560000000000001e-06     evaluation reward: 9.9\n",
            "episode: 1942   score: 10.0   memory length: 519337   epsilon: 0.16971076000876478    steps: 443    lr: 2.560000000000001e-06     evaluation reward: 9.93\n",
            "episode: 1943   score: 10.0   memory length: 519801   epsilon: 0.16879204000875897    steps: 464    lr: 2.560000000000001e-06     evaluation reward: 9.87\n",
            "episode: 1944   score: 7.0   memory length: 520222   epsilon: 0.1679584600087537    steps: 421    lr: 2.560000000000001e-06     evaluation reward: 9.83\n",
            "episode: 1945   score: 16.0   memory length: 520804   epsilon: 0.1668061000087464    steps: 582    lr: 2.560000000000001e-06     evaluation reward: 9.83\n",
            "episode: 1946   score: 6.0   memory length: 521126   epsilon: 0.16616854000874237    steps: 322    lr: 2.560000000000001e-06     evaluation reward: 9.74\n",
            "episode: 1947   score: 8.0   memory length: 521544   epsilon: 0.16534090000873713    steps: 418    lr: 2.560000000000001e-06     evaluation reward: 9.74\n",
            "episode: 1948   score: 17.0   memory length: 522213   epsilon: 0.16401628000872875    steps: 669    lr: 2.560000000000001e-06     evaluation reward: 9.83\n",
            "episode: 1949   score: 10.0   memory length: 522674   epsilon: 0.16310350000872298    steps: 461    lr: 2.560000000000001e-06     evaluation reward: 9.86\n",
            "episode: 1950   score: 15.0   memory length: 523379   epsilon: 0.16170760000871415    steps: 705    lr: 2.560000000000001e-06     evaluation reward: 9.97\n",
            "episode: 1951   score: 12.0   memory length: 523939   epsilon: 0.16059880000870713    steps: 560    lr: 2.560000000000001e-06     evaluation reward: 9.99\n",
            "episode: 1952   score: 21.0   memory length: 524446   epsilon: 0.15959494000870078    steps: 507    lr: 2.560000000000001e-06     evaluation reward: 10.06\n",
            "episode: 1953   score: 11.0   memory length: 524988   epsilon: 0.158521780008694    steps: 542    lr: 2.560000000000001e-06     evaluation reward: 10.08\n",
            "episode: 1954   score: 3.0   memory length: 525201   epsilon: 0.15810004000869132    steps: 213    lr: 2.560000000000001e-06     evaluation reward: 10.0\n",
            "episode: 1955   score: 8.0   memory length: 525605   epsilon: 0.15730012000868626    steps: 404    lr: 2.560000000000001e-06     evaluation reward: 9.95\n",
            "episode: 1956   score: 15.0   memory length: 526160   epsilon: 0.1562012200086793    steps: 555    lr: 2.560000000000001e-06     evaluation reward: 9.97\n",
            "episode: 1957   score: 12.0   memory length: 526684   epsilon: 0.15516370000867274    steps: 524    lr: 2.560000000000001e-06     evaluation reward: 10.0\n",
            "episode: 1958   score: 9.0   memory length: 527145   epsilon: 0.15425092000866697    steps: 461    lr: 2.560000000000001e-06     evaluation reward: 10.02\n",
            "episode: 1959   score: 12.0   memory length: 527680   epsilon: 0.15319162000866027    steps: 535    lr: 2.560000000000001e-06     evaluation reward: 10.02\n",
            "episode: 1960   score: 16.0   memory length: 528370   epsilon: 0.15182542000865162    steps: 690    lr: 2.560000000000001e-06     evaluation reward: 10.1\n",
            "episode: 1961   score: 16.0   memory length: 528932   epsilon: 0.15071266000864458    steps: 562    lr: 2.560000000000001e-06     evaluation reward: 10.12\n",
            "episode: 1962   score: 8.0   memory length: 529333   epsilon: 0.14991868000863956    steps: 401    lr: 2.560000000000001e-06     evaluation reward: 10.16\n",
            "episode: 1963   score: 8.0   memory length: 529758   epsilon: 0.14907718000863424    steps: 425    lr: 2.560000000000001e-06     evaluation reward: 10.13\n",
            "episode: 1964   score: 8.0   memory length: 530175   epsilon: 0.148251520008629    steps: 417    lr: 2.560000000000001e-06     evaluation reward: 10.11\n",
            "episode: 1965   score: 7.0   memory length: 530553   epsilon: 0.14750308000862428    steps: 378    lr: 2.560000000000001e-06     evaluation reward: 10.06\n",
            "episode: 1966   score: 7.0   memory length: 530955   epsilon: 0.14670712000861924    steps: 402    lr: 2.560000000000001e-06     evaluation reward: 10.06\n",
            "episode: 1967   score: 13.0   memory length: 531543   epsilon: 0.14554288000861187    steps: 588    lr: 2.560000000000001e-06     evaluation reward: 10.03\n",
            "episode: 1968   score: 8.0   memory length: 531997   epsilon: 0.1446439600086062    steps: 454    lr: 2.560000000000001e-06     evaluation reward: 9.99\n",
            "episode: 1969   score: 9.0   memory length: 532481   epsilon: 0.14368564000860012    steps: 484    lr: 2.560000000000001e-06     evaluation reward: 9.97\n",
            "episode: 1970   score: 19.0   memory length: 533106   epsilon: 0.1424481400085923    steps: 625    lr: 2.560000000000001e-06     evaluation reward: 10.09\n",
            "episode: 1971   score: 12.0   memory length: 533639   epsilon: 0.14139280000858562    steps: 533    lr: 2.560000000000001e-06     evaluation reward: 10.12\n",
            "episode: 1972   score: 11.0   memory length: 534243   epsilon: 0.14019688000857805    steps: 604    lr: 2.560000000000001e-06     evaluation reward: 10.14\n",
            "episode: 1973   score: 15.0   memory length: 534934   epsilon: 0.1388287000085694    steps: 691    lr: 2.560000000000001e-06     evaluation reward: 10.18\n",
            "episode: 1974   score: 11.0   memory length: 535479   epsilon: 0.13774960000856257    steps: 545    lr: 2.560000000000001e-06     evaluation reward: 10.24\n",
            "episode: 1975   score: 10.0   memory length: 535979   epsilon: 0.1367596000085563    steps: 500    lr: 2.560000000000001e-06     evaluation reward: 10.25\n",
            "episode: 1976   score: 13.0   memory length: 536486   epsilon: 0.13575574000854995    steps: 507    lr: 2.560000000000001e-06     evaluation reward: 10.28\n",
            "episode: 1977   score: 13.0   memory length: 536972   epsilon: 0.13479346000854386    steps: 486    lr: 2.560000000000001e-06     evaluation reward: 10.33\n",
            "episode: 1978   score: 14.0   memory length: 537545   epsilon: 0.13365892000853669    steps: 573    lr: 2.560000000000001e-06     evaluation reward: 10.39\n",
            "episode: 1979   score: 6.0   memory length: 537924   epsilon: 0.13290850000853194    steps: 379    lr: 2.560000000000001e-06     evaluation reward: 10.34\n",
            "episode: 1980   score: 13.0   memory length: 538537   epsilon: 0.13169476000852426    steps: 613    lr: 2.560000000000001e-06     evaluation reward: 10.38\n",
            "episode: 1981   score: 16.0   memory length: 539145   epsilon: 0.13049092000851664    steps: 608    lr: 2.560000000000001e-06     evaluation reward: 10.43\n",
            "episode: 1982   score: 10.0   memory length: 539646   epsilon: 0.12949894000851037    steps: 501    lr: 2.560000000000001e-06     evaluation reward: 10.46\n",
            "episode: 1983   score: 14.0   memory length: 540234   epsilon: 0.128334700008503    steps: 588    lr: 2.560000000000001e-06     evaluation reward: 10.49\n",
            "episode: 1984   score: 12.0   memory length: 540856   epsilon: 0.1271031400084952    steps: 622    lr: 2.560000000000001e-06     evaluation reward: 10.48\n",
            "episode: 1985   score: 11.0   memory length: 541292   epsilon: 0.12623986000848975    steps: 436    lr: 2.560000000000001e-06     evaluation reward: 10.53\n",
            "episode: 1986   score: 13.0   memory length: 541844   epsilon: 0.12514690000848283    steps: 552    lr: 2.560000000000001e-06     evaluation reward: 10.55\n",
            "episode: 1987   score: 10.0   memory length: 542324   epsilon: 0.12419650000848245    steps: 480    lr: 2.560000000000001e-06     evaluation reward: 10.53\n",
            "episode: 1988   score: 10.0   memory length: 542859   epsilon: 0.12313720000848317    steps: 535    lr: 2.560000000000001e-06     evaluation reward: 10.55\n",
            "episode: 1989   score: 10.0   memory length: 543382   epsilon: 0.12210166000848388    steps: 523    lr: 2.560000000000001e-06     evaluation reward: 10.54\n",
            "episode: 1990   score: 10.0   memory length: 543884   epsilon: 0.12110770000848456    steps: 502    lr: 2.560000000000001e-06     evaluation reward: 10.57\n",
            "episode: 1991   score: 13.0   memory length: 544474   epsilon: 0.11993950000848536    steps: 590    lr: 2.560000000000001e-06     evaluation reward: 10.56\n",
            "episode: 1992   score: 9.0   memory length: 544948   epsilon: 0.119000980008486    steps: 474    lr: 2.560000000000001e-06     evaluation reward: 10.6\n",
            "episode: 1993   score: 12.0   memory length: 545499   epsilon: 0.11791000000848674    steps: 551    lr: 2.560000000000001e-06     evaluation reward: 10.63\n",
            "episode: 1994   score: 12.0   memory length: 545958   epsilon: 0.11700118000848736    steps: 459    lr: 2.560000000000001e-06     evaluation reward: 10.6\n",
            "episode: 1995   score: 12.0   memory length: 546482   epsilon: 0.11596366000848807    steps: 524    lr: 2.560000000000001e-06     evaluation reward: 10.62\n",
            "episode: 1996   score: 12.0   memory length: 546916   epsilon: 0.11510434000848865    steps: 434    lr: 2.560000000000001e-06     evaluation reward: 10.63\n",
            "episode: 1997   score: 15.0   memory length: 547449   epsilon: 0.11404900000848937    steps: 533    lr: 2.560000000000001e-06     evaluation reward: 10.71\n",
            "episode: 1998   score: 10.0   memory length: 547971   epsilon: 0.11301544000849008    steps: 522    lr: 2.560000000000001e-06     evaluation reward: 10.78\n",
            "episode: 1999   score: 10.0   memory length: 548479   epsilon: 0.11200960000849076    steps: 508    lr: 2.560000000000001e-06     evaluation reward: 10.76\n",
            "episode: 2000   score: 12.0   memory length: 549064   epsilon: 0.11085130000849155    steps: 585    lr: 2.560000000000001e-06     evaluation reward: 10.78\n",
            "episode: 2001   score: 8.0   memory length: 549506   epsilon: 0.10997614000849215    steps: 442    lr: 2.560000000000001e-06     evaluation reward: 10.8\n",
            "episode: 2002   score: 12.0   memory length: 550029   epsilon: 0.10894060000849286    steps: 523    lr: 2.560000000000001e-06     evaluation reward: 10.85\n",
            "episode: 2003   score: 13.0   memory length: 550624   epsilon: 0.10776250000849366    steps: 595    lr: 2.560000000000001e-06     evaluation reward: 10.89\n",
            "episode: 2004   score: 3.0   memory length: 550874   epsilon: 0.107267500008494    steps: 250    lr: 2.560000000000001e-06     evaluation reward: 10.83\n",
            "episode: 2005   score: 9.0   memory length: 551364   epsilon: 0.10629730000849466    steps: 490    lr: 2.560000000000001e-06     evaluation reward: 10.84\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-291524992219>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m255.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/My Drive/assignment5_materials/agent_double.py\u001b[0m in \u001b[0;36mget_action\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0;31m# Choose the best action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m               \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/My Drive/assignment5_materials/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    134\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_mean\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrack_running_stats\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_var\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrack_running_stats\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m             self.weight, self.bias, bn_training, exponential_average_factor, self.eps)\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2056\u001b[0m     return torch.batch_norm(\n\u001b[1;32m   2057\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2058\u001b[0;31m         \u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2059\u001b[0m     )\n\u001b[1;32m   2060\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "Error in callback <function flush_figures at 0x7f47a3146b70> (for post_execute):\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/pylab/backend_inline.py\u001b[0m in \u001b[0;36mflush_figures\u001b[0;34m()\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;31m# ignore the tracking, just draw and close all figures\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0;31m# safely show traceback if in IPython, else raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/pylab/backend_inline.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(close, block)\u001b[0m\n\u001b[1;32m     37\u001b[0m             display(\n\u001b[1;32m     38\u001b[0m                 \u001b[0mfigure_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m                 \u001b[0mmetadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_fetch_figure_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigure_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m             )\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/display.py\u001b[0m in \u001b[0;36mdisplay\u001b[0;34m(*objs, **kwargs)\u001b[0m\n\u001b[1;32m    304\u001b[0m             \u001b[0mpublish_display_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 306\u001b[0;31m             \u001b[0mformat_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmd_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minclude\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexclude\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    307\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mformat_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m                 \u001b[0;31m# nothing to display (e.g. _ipython_display_ took over)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36mformat\u001b[0;34m(self, obj, include, exclude)\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0mmd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m             \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m                 \u001b[0;31m# FIXME: log the exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<decorator-gen-9>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36mcatch_format_error\u001b[0;34m(method, self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"show traceback on failed format call\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m         \u001b[0;31m# don't warn on NotImplementedErrors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    332\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mprinter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    335\u001b[0m             \u001b[0;31m# Finally look for special method names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_real_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/pylabtools.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(fig)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'png'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mformats\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m         \u001b[0mpng_formatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFigure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprint_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'png'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'retina'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mformats\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m'png2x'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mformats\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0mpng_formatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFigure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mretina_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/pylabtools.py\u001b[0m in \u001b[0;36mprint_figure\u001b[0;34m(fig, fmt, bbox_inches, **kwargs)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0mbytes_io\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m     \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbytes_io\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytes_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfmt\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'svg'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/backend_bases.py\u001b[0m in \u001b[0;36mprint_figure\u001b[0;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, **kwargs)\u001b[0m\n\u001b[1;32m   2070\u001b[0m         \u001b[0;31m# but this should be fine.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2071\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setattr_cm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmanager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2072\u001b[0;31m                 \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setattr_cm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdpi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdpi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2073\u001b[0m                 \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setattr_cm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_is_saving\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2074\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"generator didn't yield\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/cbook/__init__.py\u001b[0m in \u001b[0;36m_setattr_cm\u001b[0;34m(obj, **kwargs)\u001b[0m\n\u001b[1;32m   1962\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1963\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1964\u001b[0;31m             \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1965\u001b[0m         \u001b[0;32myield\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1966\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/figure.py\u001b[0m in \u001b[0;36m_set_dpi\u001b[0;34m(self, dpi, forward)\u001b[0m\n\u001b[1;32m    474\u001b[0m         \"\"\"\n\u001b[1;32m    475\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dpi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdpi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 476\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdpi_scale_trans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdpi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    477\u001b[0m         \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_size_inches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    478\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_size_inches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/transforms.py\u001b[0m in \u001b[0;36mclear\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1919\u001b[0m         \u001b[0;31m# A bit faster than np.identity(3).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1920\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mtx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mIdentityTransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mtx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1921\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1922\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/transforms.py\u001b[0m in \u001b[0;36minvalidate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_affine\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mINVALID_AFFINE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_invalidate_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minvalidating_node\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_invalidate_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minvalidating_node\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/transforms.py\u001b[0m in \u001b[0;36m_invalidate_internal\u001b[0;34m(self, value, invalidating_node)\u001b[0m\n\u001b[1;32m    159\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mparent\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m                     parent._invalidate_internal(\n\u001b[0;32m--> 161\u001b[0;31m                         value=value, invalidating_node=self)\n\u001b[0m\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_children\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/transforms.py\u001b[0m in \u001b[0;36m_invalidate_internal\u001b[0;34m(self, value, invalidating_node)\u001b[0m\n\u001b[1;32m    159\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mparent\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m                     parent._invalidate_internal(\n\u001b[0;32m--> 161\u001b[0;31m                         value=value, invalidating_node=self)\n\u001b[0m\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_children\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/transforms.py\u001b[0m in \u001b[0;36m_invalidate_internal\u001b[0;34m(self, value, invalidating_node)\u001b[0m\n\u001b[1;32m    159\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mparent\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m                     parent._invalidate_internal(\n\u001b[0;32m--> 161\u001b[0;31m                         value=value, invalidating_node=self)\n\u001b[0m\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_children\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/transforms.py\u001b[0m in \u001b[0;36m_invalidate_internal\u001b[0;34m(self, value, invalidating_node)\u001b[0m\n\u001b[1;32m    159\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mparent\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m                     parent._invalidate_internal(\n\u001b[0;32m--> 161\u001b[0;31m                         value=value, invalidating_node=self)\n\u001b[0m\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_children\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/transforms.py\u001b[0m in \u001b[0;36m_invalidate_internal\u001b[0;34m(self, value, invalidating_node)\u001b[0m\n\u001b[1;32m    159\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mparent\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m                     parent._invalidate_internal(\n\u001b[0;32m--> 161\u001b[0;31m                         value=value, invalidating_node=self)\n\u001b[0m\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_children\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/transforms.py\u001b[0m in \u001b[0;36m_invalidate_internal\u001b[0;34m(self, value, invalidating_node)\u001b[0m\n\u001b[1;32m   2331\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2332\u001b[0m         Transform._invalidate_internal(self, value=value,\n\u001b[0;32m-> 2333\u001b[0;31m                                        invalidating_node=invalidating_node)\n\u001b[0m\u001b[1;32m   2334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2335\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__eq__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/transforms.py\u001b[0m in \u001b[0;36m_invalidate_internal\u001b[0;34m(self, value, invalidating_node)\u001b[0m\n\u001b[1;32m    159\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mparent\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m                     parent._invalidate_internal(\n\u001b[0;32m--> 161\u001b[0;31m                         value=value, invalidating_node=self)\n\u001b[0m\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_children\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/transforms.py\u001b[0m in \u001b[0;36m_invalidate_internal\u001b[0;34m(self, value, invalidating_node)\u001b[0m\n\u001b[1;32m   2331\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2332\u001b[0m         Transform._invalidate_internal(self, value=value,\n\u001b[0;32m-> 2333\u001b[0;31m                                        invalidating_node=invalidating_node)\n\u001b[0m\u001b[1;32m   2334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2335\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__eq__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/transforms.py\u001b[0m in \u001b[0;36m_invalidate_internal\u001b[0;34m(self, value, invalidating_node)\u001b[0m\n\u001b[1;32m    154\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_invalid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mparent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m                 \u001b[0;31m# Dereference the weak reference\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m                 \u001b[0mparent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xm_aiHkHPIDa"
      },
      "source": [
        "# Visualize Agent Performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bs6EljpgPIDa"
      },
      "source": [
        "BE AWARE THIS CODE BELOW MAY CRASH THE KERNEL IF YOU RUN THE SAME CELL TWICE.\n",
        "\n",
        "Please save your model before running this portion of the code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ojZ4VQ3PIDa"
      },
      "source": [
        "torch.save(agent.policy_net, \"./save_model/breakout_dqn_latest.pth\")"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GDEonOstPIDa"
      },
      "source": [
        "from gym.wrappers import Monitor\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "\n",
        "from IPython.display import HTML\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "from pyvirtualdisplay import Display\n",
        "\n",
        "# Displaying the game live\n",
        "def show_state(env, step=0, info=\"\"):\n",
        "    plt.figure(3)\n",
        "    plt.clf()\n",
        "    plt.imshow(env.render(mode='rgb_array'))\n",
        "    plt.title(\"%s | Step: %d %s\" % (\"Agent Playing\",step, info))\n",
        "    plt.axis('off')\n",
        "\n",
        "    ipythondisplay.clear_output(wait=True)\n",
        "    ipythondisplay.display(plt.gcf())\n",
        "    \n",
        "# Recording the game and replaying the game afterwards\n",
        "def show_video():\n",
        "    mp4list = glob.glob('video/*.mp4')\n",
        "    if len(mp4list) > 0:\n",
        "        mp4 = mp4list[0]\n",
        "        video = io.open(mp4, 'r+b').read()\n",
        "        encoded = base64.b64encode(video)\n",
        "        ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "    else: \n",
        "        print(\"Could not find video\")\n",
        "    \n",
        "\n",
        "def wrap_env(env):\n",
        "    env = Monitor(env, './video', force=True)\n",
        "    return env"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tvs128r4PIDa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 438
        },
        "outputId": "ec385899-962f-49c9-e91a-637d0040f020"
      },
      "source": [
        "display = Display(visible=0, size=(300, 200))\n",
        "display.start()\n",
        "\n",
        "# Load agent\n",
        "# agent.load_policy_net(\"./save_model/breakout_dqn.pth\")\n",
        "agent.epsilon = 0.0 # Set agent to only exploit the best action\n",
        "\n",
        "env = gym.make('BreakoutDeterministic-v4')\n",
        "env = wrap_env(env)\n",
        "\n",
        "done = False\n",
        "score = 0\n",
        "step = 0\n",
        "state = env.reset()\n",
        "next_state = state\n",
        "life = number_lives\n",
        "history = np.zeros([5, 84, 84], dtype=np.uint8)\n",
        "get_init_state(history, state)\n",
        "\n",
        "while not done:\n",
        "    \n",
        "    # Render breakout\n",
        "    env.render()\n",
        "#     show_state(env,step) # uncommenting this provides another way to visualize the game\n",
        "\n",
        "    step += 1\n",
        "    frame += 1\n",
        "\n",
        "    # Perform a fire action if ball is no longer on screen\n",
        "    if step > 1 and len(np.unique(next_state[:189] == state[:189])) < 2:\n",
        "        action = 0\n",
        "    else:\n",
        "        action = agent.get_action(np.float32(history[:4, :, :]) / 255.)\n",
        "    state = next_state\n",
        "    \n",
        "    next_state, reward, done, info = env.step(action + 1)\n",
        "        \n",
        "    frame_next_state = get_frame(next_state)\n",
        "    history[4, :, :] = frame_next_state\n",
        "    terminal_state = check_live(life, info['ale.lives'])\n",
        "        \n",
        "    life = info['ale.lives']\n",
        "    r = np.clip(reward, -1, 1) \n",
        "    r = reward\n",
        "\n",
        "    # Store the transition in memory \n",
        "    agent.memory.push(deepcopy(frame_next_state), action, r, terminal_state)\n",
        "    # Start training after random sample generation\n",
        "    score += reward\n",
        "    \n",
        "    history[:4, :, :] = history[1:, :, :]\n",
        "env.close()\n",
        "show_video()\n",
        "display.stop()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<video alt=\"test\" autoplay \n",
              "                loop controls style=\"height: 400px;\">\n",
              "                <source src=\"data:video/mp4;base64,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\" type=\"video/mp4\" />\n",
              "             </video>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyvirtualdisplay.display.Display at 0x7f46e576c080>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mCMcLfB5PIDa"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}